// src/streaming/cache/lru.rs
// --------------------------
use std::{collections::VecDeque, mem::size_of, sync::Arc};
use rustc_hash::FxHashMap as HashMap;

use crate::{config, render::gpu_types::{NodeGpu, NodeRopesGpu}};
use crate::streaming::types::ChunkKey;

#[derive(Clone)]
pub struct CachedChunk {
    pub nodes: Arc<[NodeGpu]>,
    pub ropes: Arc<[NodeRopesGpu]>,
    pub macro_words: Arc<[u32]>,
    pub colinfo_words: Arc<[u32]>,
    pub bytes: usize,
    pub stamp: u64,
}

pub struct ChunkCache {
    map: HashMap<ChunkKey, CachedChunk>,
    lru: VecDeque<(ChunkKey, u64)>,
    stamp: u64,
    bytes: usize,
}

impl ChunkCache {
    pub fn new() -> Self {
        Self { map: HashMap::default(), lru: VecDeque::default(), stamp: 1, bytes: 0 }
    }

    pub fn stats(&self) -> (usize, usize, usize) {
        (self.bytes, self.map.len(), self.lru.len())
    }

    pub fn get(&self, key: &ChunkKey) -> Option<&CachedChunk> { self.map.get(key) }

    pub fn touch(&mut self, key: ChunkKey) {
        if let Some(e) = self.map.get_mut(&key) {
            self.stamp = self.stamp.wrapping_add(1).max(1);
            e.stamp = self.stamp;
            self.lru.push_back((key, e.stamp));
            self.maybe_compact_lru();
        }
    }

    pub fn put(
        &mut self,
        key: ChunkKey,
        nodes: Arc<[NodeGpu]>,
        macro_words: Arc<[u32]>,
        ropes: Arc<[NodeRopesGpu]>,
        colinfo_words: Arc<[u32]>,
    ) {
        if let Some(old) = self.map.remove(&key) {
            self.bytes = self.bytes.saturating_sub(old.bytes);
        }

        self.stamp = self.stamp.wrapping_add(1).max(1);
        let stamp = self.stamp;

        let bytes =
            nodes.len() * size_of::<NodeGpu>()
            + ropes.len() * size_of::<NodeRopesGpu>()
            + macro_words.len() * size_of::<u32>()
            + colinfo_words.len() * size_of::<u32>();

        self.map.insert(
            key,
            CachedChunk { nodes, ropes, macro_words, colinfo_words, bytes, stamp },
        );

        self.bytes = self.bytes.saturating_add(bytes);
        self.lru.push_back((key, stamp));
        self.evict_as_needed();
    }

    pub fn remove(&mut self, key: &ChunkKey) {
        if let Some(old) = self.map.remove(key) {
            self.bytes = self.bytes.saturating_sub(old.bytes);
        }
    }

    fn evict_as_needed(&mut self) {
        let budget = config::CHUNK_CACHE_BUDGET_BYTES;
        while self.bytes > budget {
            let Some((k, stamp)) = self.lru.pop_front() else { break; };

            let should_evict = self.map.get(&k).map(|e| e.stamp == stamp).unwrap_or(false);
            if !should_evict { continue; }

            if let Some(ev) = self.map.remove(&k) {
                self.bytes = self.bytes.saturating_sub(ev.bytes);
            }
        }
    }

    fn maybe_compact_lru(&mut self) {
        let max = self.map.len().saturating_mul(8).max(1024);
        if self.lru.len() <= max { return; }

        let mut new = VecDeque::with_capacity(self.map.len());
        for (k, e) in self.map.iter() {
            new.push_back((*k, e.stamp));
        }
        self.lru = new;
    }
}

// src/streaming/cache/mod.rs
// --------------------------
mod lru;

pub use lru::{CachedChunk, ChunkCache};

// src/streaming/manager/ground.rs
// -------------------------------
use crate::streaming::types::*;
use crate::{config, world::WorldGen};
use super::{ChunkManager};
use super::keep;

#[inline]
fn compute_ground_cy_at_column(world: &WorldGen, cx: i32, cz: i32) -> i32 {
    let cs = config::CHUNK_SIZE as i32;
    let half = cs / 2;
    let wx = cx * cs + half;
    let wz = cz * cs + half;
    let ground_y_vox = world.ground_height(wx, wz);
    ground_y_vox.div_euclid(cs)
}

pub fn ensure_column_cache(mgr: &mut ChunkManager, world: &WorldGen, center: ChunkKey) {
    let new_origin = keep::keep_origin_for(center);
    let origin_changed = mgr.ground.col_ground_cy.is_empty() || new_origin != mgr.grid.grid_origin_chunk;

    if !origin_changed {
        return;
    }

    let old_origin = mgr.grid.grid_origin_chunk;

    // publish new origin first (so indexing uses it)
    mgr.grid.grid_origin_chunk = new_origin;

    update_column_ground_cache(mgr, world, old_origin, new_origin);
    mgr.grid.grid_dirty = true;
}

pub fn ground_cy_for_column(mgr: &ChunkManager, cx: i32, cz: i32) -> Option<i32> {
    let [ox, _, oz] = mgr.grid.grid_origin_chunk;

    let nx = (2 * config::KEEP_RADIUS + 1) as i32;
    let nz = nx;

    let ix = cx - ox;
    let iz = cz - oz;
    if ix < 0 || iz < 0 || ix >= nx || iz >= nz { return None; }

    let idx = (iz * nx + ix) as usize;
    mgr.ground.col_ground_cy.get(idx).copied()
}

fn update_column_ground_cache(
    mgr: &mut ChunkManager,
    world: &WorldGen,
    old_origin: [i32; 3],
    new_origin: [i32; 3],
) {
    let nx = (2 * config::KEEP_RADIUS + 1) as i32;
    let nz = nx;
    let len = (nx * nz) as usize;

    // first time / resize => full rebuild
    if mgr.ground.col_ground_cy.len() != len || mgr.ground.col_ground_cy.is_empty() {
        mgr.ground.col_ground_cy.resize(len, 0);
        let ox = new_origin[0];
        let oz = new_origin[2];
        for dz in 0..nz {
            for dx in 0..nx {
                let cx = ox + dx;
                let cz = oz + dz;
                mgr.ground.col_ground_cy[(dz * nx + dx) as usize] =
                    compute_ground_cy_at_column(world, cx, cz);
            }
        }
        return;
    }

    let dx_chunks = new_origin[0] - old_origin[0];
    let dz_chunks = new_origin[2] - old_origin[2];

    if dx_chunks == 0 && dz_chunks == 0 {
        return;
    }

    // teleport => rebuild
    if dx_chunks.abs() >= nx || dz_chunks.abs() >= nz {
        let ox = new_origin[0];
        let oz = new_origin[2];
        for dz in 0..nz {
            for dx in 0..nx {
                let cx = ox + dx;
                let cz = oz + dz;
                mgr.ground.col_ground_cy[(dz * nx + dx) as usize] =
                    compute_ground_cy_at_column(world, cx, cz);
            }
        }
        return;
    }

    let old = std::mem::take(&mut mgr.ground.col_ground_cy);
    let mut newv = vec![0i32; len];

    let ox_new = new_origin[0];
    let oz_new = new_origin[2];

    for iz in 0..nz {
        for ix in 0..nx {
            let sx = ix - dx_chunks;
            let sz = iz - dz_chunks;

            let dst_idx = (iz * nx + ix) as usize;

            if sx >= 0 && sx < nx && sz >= 0 && sz < nz {
                let src_idx = (sz * nx + sx) as usize;
                newv[dst_idx] = old[src_idx];
            } else {
                let cx = ox_new + ix;
                let cz = oz_new + iz;
                newv[dst_idx] = compute_ground_cy_at_column(world, cx, cz);
            }
        }
    }

    mgr.ground.col_ground_cy = newv;
}

// src/streaming/manager/keep.rs
// -----------------------------
use glam::Vec3;
use crate::{config, world::WorldGen};
use crate::streaming::types::*;

use super::ChunkManager;

#[inline]
pub fn build_offsets(radius: i32) -> Vec<(i32, i32, i32)> {
    let mut v = Vec::new();
    v.reserve((GRID_Y_COUNT as usize) * ((2 * radius + 1) as usize) * ((2 * radius + 1) as usize));

    for dy in GRID_Y_MIN_DY..=(GRID_Y_MIN_DY + GRID_Y_COUNT as i32 - 1) {
        for dz in -radius..=radius {
            for dx in -radius..=radius {
                v.push((dx, dy, dz));
            }
        }
    }
    v
}

#[inline]
pub fn compute_center(world: &WorldGen, cam_pos_m: Vec3) -> ChunkKey {
    let cam_vx = (cam_pos_m.x / config::VOXEL_SIZE_M_F32).floor() as i32;
    let cam_vz = (cam_pos_m.z / config::VOXEL_SIZE_M_F32).floor() as i32;

    let cs = config::CHUNK_SIZE as i32;
    let half = cs / 2;

    let ccx = cam_vx.div_euclid(cs);
    let ccz = cam_vz.div_euclid(cs);

    let wx = ccx * cs + half;
    let wz = ccz * cs + half;
    let ground_y_vox = world.ground_height(wx, wz);
    let ground_cy = ground_y_vox.div_euclid(cs);

    ChunkKey { x: ccx, y: ground_cy, z: ccz }
}

#[inline]
pub fn keep_origin_for(center: ChunkKey) -> [i32; 3] {
    let ox = center.x - config::KEEP_RADIUS;
    let oz = center.z - config::KEEP_RADIUS;
    let oy = center.y + GRID_Y_MIN_DY;
    [ox, oy, oz]
}

#[inline(always)]
pub fn in_active_xz(center: ChunkKey, k: ChunkKey) -> bool {
    let dx = (k.x - center.x).abs();
    let dz = (k.z - center.z).abs();
    dx <= config::ACTIVE_RADIUS.max(PRIORITY_RADIUS) && dz <= config::ACTIVE_RADIUS.max(PRIORITY_RADIUS)
}

#[inline(always)]
pub fn in_priority_xz(center: ChunkKey, k: ChunkKey) -> bool {
    let dx = (k.x - center.x).abs();
    let dz = (k.z - center.z).abs();
    dx <= PRIORITY_RADIUS && dz <= PRIORITY_RADIUS
}

/// publish new center; if changed, rebucket uploads.
pub fn publish_center_and_rebucket(mgr: &mut ChunkManager, center: ChunkKey) -> bool {
    let changed = mgr.build.last_center.map_or(true, |c| c != center);
    if changed {
        mgr.build.last_center = Some(center);
        super::uploads::rebucket_for_center(mgr, center);
    }
    changed
}

// src/streaming/manager/mod.rs
// ----------------------------

mod ground;
mod keep;
mod uploads;

use std::{
    collections::VecDeque,
    mem::size_of,
    sync::{
        atomic::{AtomicBool, Ordering},
        Arc,
    },
};

use crossbeam_channel::{bounded, Receiver, Sender, TrySendError};
use glam::Vec3;
use rustc_hash::{FxHashMap as HashMap, FxHashSet as HashSet};

use crate::{
    config,
    render::gpu_types::{ChunkMetaGpu, NodeGpu, NodeRopesGpu},
    world::WorldGen,
    streaming::types::StreamStats,
};

use crate::streaming::{
    NodeArena,
    cache::ChunkCache,
    priority::sort_queue_near_first,
    types::*,
    workers::spawn_workers,
};

/// Build-related state bucket.
pub(crate) struct BuildState {
    pub chunks: HashMap<ChunkKey, ChunkState>,

    pub build_queue: VecDeque<ChunkKey>,
    pub queued_set: HashSet<ChunkKey>,
    pub cancels: HashMap<ChunkKey, Arc<AtomicBool>>,

    pub tx_job: Sender<BuildJob>,
    pub rx_done: Receiver<BuildDone>,
    pub in_flight: usize,

    pub last_center: Option<ChunkKey>,
    pub to_unload: Vec<ChunkKey>,
}

/// Slot-residency bucket.
pub(crate) struct SlotState {
    pub slot_to_key: Vec<ChunkKey>,
    pub chunk_meta: Vec<ChunkMetaGpu>,
    pub slot_macro: Vec<Arc<[u32]>>,
    pub slot_colinfo: Vec<Arc<[u32]>>,
    pub resident_slots: usize,
}

/// Upload bucket.
pub(crate) struct UploadState {
    pub uploads_rewrite: VecDeque<ChunkUpload>,
    pub uploads_active:  VecDeque<ChunkUpload>,
    pub uploads_other:   VecDeque<ChunkUpload>,
}

/// Grid bucket.
pub(crate) struct GridState {
    pub grid_dirty: bool,
    pub grid_origin_chunk: [i32; 3],
    pub grid_dims: [u32; 3],
    pub chunk_grid: Vec<u32>,
}

/// Column ground cache bucket.
pub(crate) struct GroundState {
    pub col_ground_cy: Vec<i32>,
}

/// Offset precomputes bucket.
pub(crate) struct Offsets {
    pub active_offsets: Vec<(i32, i32, i32)>,
    pub priority_offsets: Vec<(i32, i32, i32)>,
}

pub struct ChunkManager {
    pub(crate) build: BuildState,
    pub(crate) slots: SlotState,
    pub(crate) uploads: UploadState,
    pub(crate) grid: GridState,
    pub(crate) ground: GroundState,
    pub(crate) offsets: Offsets,

    pub(crate) arena: NodeArena,
    pub(crate) cache: ChunkCache,
}

impl ChunkManager {
    pub fn new(gen: Arc<WorldGen>) -> Self {
        let cap = (config::MAX_IN_FLIGHT * 2).max(8);

        let (tx_job, rx_job) = bounded::<BuildJob>(cap);
        let (tx_done, rx_done) = bounded::<BuildDone>(cap);

        spawn_workers(gen, rx_job, tx_done);

        let node_capacity = (config::NODE_BUDGET_BYTES / size_of::<NodeGpu>()) as u32;

        let nx = (2 * config::KEEP_RADIUS + 1) as u32;
        let nz = nx;
        let ny = GRID_Y_COUNT;

        let grid_len = (nx * ny * nz) as usize;

        let active_offsets = keep::build_offsets(config::ACTIVE_RADIUS);
        let priority_offsets = keep::build_offsets(PRIORITY_RADIUS);

        Self {
            build: BuildState {
                chunks: HashMap::default(),
                build_queue: VecDeque::new(),
                queued_set: HashSet::default(),
                cancels: HashMap::default(),
                tx_job,
                rx_done,
                in_flight: 0,
                last_center: None,
                to_unload: Vec::new(),
            },
            slots: SlotState {
                slot_to_key: Vec::new(),
                chunk_meta: Vec::new(),
                slot_macro: Vec::new(),
                slot_colinfo: Vec::new(),
                resident_slots: 0,
            },
            uploads: UploadState {
                uploads_rewrite: VecDeque::new(),
                uploads_active:  VecDeque::new(),
                uploads_other:   VecDeque::new(),
            },
            grid: GridState {
                grid_dirty: true,
                grid_origin_chunk: [0, 0, 0],
                grid_dims: [nx, ny, nz],
                chunk_grid: vec![INVALID_U32; grid_len],
            },
            ground: GroundState { col_ground_cy: Vec::new() },
            offsets: Offsets { active_offsets, priority_offsets },

            arena: NodeArena::new(node_capacity),
            cache: ChunkCache::new(),
        }
    }

    /// Main frame update (same logic as before; now calls into submodules).
    pub fn update(&mut self, world: &Arc<WorldGen>, cam_pos_m: Vec3, cam_fwd: Vec3) -> bool {
        // 1) compute center
        let center = keep::compute_center(world.as_ref(), cam_pos_m);

        // 2) ensure ground cache + publish origin
        ground::ensure_column_cache(self, world.as_ref(), center);

        // 3) publish center (rebucket uploads on center change)
        if keep::publish_center_and_rebucket(self, center) {
            // keep::publish_center_and_rebucket already rebuckets uploads
        }

        // 4) ensure priority box builds/promotions
        build::ensure_priority_box(self, center);

        // 5) enqueue active offsets
        build::enqueue_active_ring(self, center);

        // 6) unload outside keep
        build::unload_outside_keep(self, center);

        // 7) handle center-change cleanup/sort
        build::on_center_change_resort(self, center, cam_fwd);

        // 8) dispatch builds
        build::dispatch_builds(self, center);

        // 9) harvest completions
        build::harvest_done_builds(self, center);

        // 10) rebuild grid if dirty
        grid::rebuild_if_dirty(self, center)
    }

    // --- Public API (same signatures; implemented in submodules via impl blocks) ---

    pub fn chunk_count(&self) -> u32 { self.slots.resident_slots as u32 }
    pub fn grid_origin(&self) -> [i32; 3] { self.grid.grid_origin_chunk }
    pub fn grid_dims(&self) -> [u32; 3] { self.grid.grid_dims }
    pub fn chunk_grid(&self) -> &[u32] { &self.grid.chunk_grid }

    pub fn take_uploads(&mut self) -> Vec<ChunkUpload> {
        uploads::take_all(self)
    }

    pub fn take_uploads_budgeted(&mut self) -> Vec<ChunkUpload> {
        uploads::take_budgeted(self)
    }

    pub fn commit_uploads_applied(&mut self, applied: &[ChunkUpload]) -> bool {
        slots::commit_uploads_applied(self, applied)
    }

    pub fn stats(&self) -> Option<StreamStats> {
        stats::stats(self)
    }
}

// src/streaming/manager/uploads.rs
// --------------------------------
use std::{collections::VecDeque, mem::size_of, sync::Arc};
use crate::{config, render::gpu_types::{ChunkMetaGpu, NodeGpu, NodeRopesGpu}};
use crate::streaming::types::*;
use super::{ChunkManager};
use super::keep;

#[inline]
fn upload_bytes(u: &ChunkUpload) -> usize {
    let mut b = size_of::<ChunkMetaGpu>();
    b += u.nodes.len() * size_of::<NodeGpu>();
    b += u.macro_words.len() * size_of::<u32>();
    b += u.ropes.len() * size_of::<NodeRopesGpu>();
    b += u.colinfo_words.len() * size_of::<u32>();
    b
}

#[inline(always)]
pub fn upload_dist_score(k: ChunkKey, c: ChunkKey) -> f32 {
    let dx = (k.x - c.x) as f32;
    let dz = (k.z - c.z) as f32;
    let dy = (k.y - c.y) as f32;
    dx.abs() + dz.abs() + 2.0 * dy.abs()
}

pub fn enqueue(mgr: &mut ChunkManager, u: ChunkUpload) {
    if !u.completes_residency {
        mgr.uploads.uploads_rewrite.push_front(u);
        return;
    }

    let Some(center) = mgr.build.last_center else {
        mgr.uploads.uploads_other.push_back(u);
        return;
    };

    if keep::in_active_xz(center, u.key) {
        insert_sorted_by_center(&mut mgr.uploads.uploads_active, u, center);
    } else {
        mgr.uploads.uploads_other.push_back(u);
    }
}

pub fn rebucket_for_center(mgr: &mut ChunkManager, center: ChunkKey) {
    let mut new_active = VecDeque::with_capacity(mgr.uploads.uploads_active.len());
    let mut new_other  = VecDeque::with_capacity(mgr.uploads.uploads_other.len());

    let ar = config::ACTIVE_RADIUS.max(PRIORITY_RADIUS);

    let is_active = |k: ChunkKey| {
        let dx = (k.x - center.x).abs();
        let dz = (k.z - center.z).abs();
        dx <= ar && dz <= ar
    };

    for u in mgr.uploads.uploads_active.drain(..) {
        if is_active(u.key) { new_active.push_back(u); }
        else { new_other.push_back(u); }
    }

    for u in mgr.uploads.uploads_other.drain(..) {
        if is_active(u.key) { new_active.push_back(u); }
        else { new_other.push_back(u); }
    }

    mgr.uploads.uploads_active = new_active;
    mgr.uploads.uploads_other  = new_other;
}

#[inline]
fn insert_sorted_by_center(q: &mut VecDeque<ChunkUpload>, u: ChunkUpload, center: ChunkKey) {
    let us = upload_dist_score(u.key, center);
    let pos = q.iter()
        .position(|e| upload_dist_score(e.key, center) > us)
        .unwrap_or(q.len());
    q.insert(pos, u);
}

#[inline]
fn uploads_len_total(mgr: &ChunkManager) -> usize {
    mgr.uploads.uploads_rewrite.len() + mgr.uploads.uploads_active.len() + mgr.uploads.uploads_other.len()
}

pub fn take_all(mgr: &mut ChunkManager) -> Vec<ChunkUpload> {
    let mut out = Vec::new();
    out.extend(mgr.uploads.uploads_rewrite.drain(..));
    out.extend(mgr.uploads.uploads_active.drain(..));
    out.extend(mgr.uploads.uploads_other.drain(..));
    out
}

pub fn take_budgeted(mgr: &mut ChunkManager) -> Vec<ChunkUpload> {
    let backlog = uploads_len_total(mgr);

    let max_uploads = (MAX_UPLOADS_PER_FRAME + backlog / 4).clamp(MAX_UPLOADS_PER_FRAME, 32);
    let max_bytes   = (MAX_UPLOAD_BYTES_PER_FRAME + backlog * (256 << 10)).clamp(MAX_UPLOAD_BYTES_PER_FRAME, 16 << 20);

    let mut out = Vec::new();
    let mut bytes = 0usize;

    let mut rewrites_taken = 0usize;
    let rewrite_cap = 4;

    let mut pop_next = |mgr: &mut ChunkManager| -> Option<(u8, ChunkUpload)> {
        if rewrites_taken < rewrite_cap {
            if let Some(u) = mgr.uploads.uploads_rewrite.pop_front() {
                rewrites_taken += 1;
                return Some((0, u));
            }
        }
        if let Some(u) = mgr.uploads.uploads_active.pop_front() { return Some((1, u)); }
        if let Some(u) = mgr.uploads.uploads_other.pop_front()  { return Some((2, u)); }
        None
    };

    let mut push_front_same = |mgr: &mut ChunkManager, which: u8, u: ChunkUpload| {
        match which {
            0 => mgr.uploads.uploads_rewrite.push_front(u),
            1 => mgr.uploads.uploads_active.push_front(u),
            _ => mgr.uploads.uploads_other.push_front(u),
        }
    };

    while let Some((which, mut u)) = pop_next(mgr) {
        if out.len() >= max_uploads {
            push_front_same(mgr, which, u);
            break;
        }

        // priority gate (same as before)
        if u.completes_residency {
            if let Some(center) = mgr.build.last_center {
                if !super::slots::priority_box_ready(mgr, center) && !super::slots::in_priority_box(mgr, center, u.key) {
                    push_front_same(mgr, which, u);
                    break;
                }
            }
        }

        // validate slot + update bases
        let slot = match mgr.build.chunks.get(&u.key) {
            Some(ChunkState::Resident(r)) => r.slot,
            Some(ChunkState::Uploading(up)) => up.slot,
            _ => continue,
        };

        u.slot = slot;
        u.meta.macro_base = slot * MACRO_WORDS_PER_CHUNK;
        u.meta.colinfo_base = slot * COLINFO_WORDS_PER_CHUNK;

        let ub = upload_bytes(&u);

        if bytes + ub > max_bytes && !out.is_empty() {
            push_front_same(mgr, which, u);
            break;
        }

        bytes += ub;
        out.push(u);
    }

    out
}

// src/streaming/mod.rs
// --------------------
pub mod types;
pub mod node_arena;
pub mod priority;
pub mod workers;

pub mod cache;
pub mod manager;

pub use manager::ChunkManager;
pub use types::{StreamStats, ChunkUpload};
pub use node_arena::NodeArena;

// src/streaming/node_arena.rs
// ---------------------------
// src/streaming/node_arena.rs
//
// Very simple free-list arena for node ranges (in units of NodeGpu elements).
// Improvements:
// - free() now fully coalesces adjacent ranges (fixes long-run fragmentation).
// - alloc() uses best-fit (smallest range that fits) to reduce fragmentation further.

#[derive(Clone, Copy, Debug)]
struct Range {
    start: u32,
    len: u32,
}

pub struct NodeArena {
    free: Vec<Range>, // kept sorted by start
}

impl NodeArena {
    pub fn new(capacity: u32) -> Self {
        Self {
            free: vec![Range {
                start: 0,
                len: capacity,
            }],
        }
    }

    /// Allocate a contiguous range of `len` elements.
    /// Returns the start index in the arena, or None if no free range fits.
    pub fn alloc(&mut self, len: u32) -> Option<u32> {
        if len == 0 {
            return Some(0);
        }

        // Best-fit: choose the smallest free range that still fits.
        let mut best_i: Option<usize> = None;
        let mut best_len: u32 = u32::MAX;

        for (i, r) in self.free.iter().enumerate() {
            if r.len >= len && r.len < best_len {
                best_len = r.len;
                best_i = Some(i);
                if r.len == len {
                    break; // perfect fit
                }
            }
        }

        let i = best_i?;
        let r = self.free[i];
        let start = r.start;

        if r.len == len {
            self.free.remove(i);
        } else {
            self.free[i] = Range {
                start: r.start + len,
                len: r.len - len,
            };
        }

        Some(start)
    }

    /// Free a previously allocated range.
    pub fn free(&mut self, start: u32, len: u32) {
        if len == 0 {
            return;
        }

        // Insert sorted by start (log n search).
        let idx = self
            .free
            .binary_search_by_key(&start, |r| r.start)
            .unwrap_or_else(|i| i);

        self.free.insert(idx, Range { start, len });

        // Fully coalesce with neighbors (both directions).
        self.coalesce_at(idx);
    }

    fn coalesce_at(&mut self, mut i: usize) {
        // Merge backward as long as possible.
        while i > 0 {
            let a = self.free[i - 1];
            let b = self.free[i];
            if a.start + a.len == b.start {
                self.free[i - 1] = Range {
                    start: a.start,
                    len: a.len + b.len,
                };
                self.free.remove(i);
                i -= 1;
            } else {
                break;
            }
        }

        // Merge forward as long as possible.
        while i + 1 < self.free.len() {
            let a = self.free[i];
            let b = self.free[i + 1];
            if a.start + a.len == b.start {
                self.free[i] = Range {
                    start: a.start,
                    len: a.len + b.len,
                };
                self.free.remove(i + 1);
            } else {
                break;
            }
        }
    }
}

// src/streaming/priority.rs
// -------------------------
// src/streaming/priority.rs
use std::collections::VecDeque;
use glam::{Vec2, Vec3};

use super::types::ChunkKey;

pub fn sort_queue_near_first(queue: &mut VecDeque<ChunkKey>, center: ChunkKey, cam_fwd: Vec3) {
    let mut v: Vec<ChunkKey> = queue.drain(..).collect();

    let mut f = Vec2::new(cam_fwd.x, cam_fwd.z);
    if f.length_squared() > 1e-6 {
        f = f.normalize();
    } else {
        f = Vec2::ZERO;
    }

    v.sort_by(|a, b| {
        let sa = chunk_priority_score(*a, center, f);
        let sb = chunk_priority_score(*b, center, f);
        sa.partial_cmp(&sb).unwrap_or(std::cmp::Ordering::Equal)
    });

    queue.extend(v);
}

fn chunk_priority_score(k: ChunkKey, c: ChunkKey, fwd_xz: Vec2) -> f32 {
    let dx = (k.x - c.x) as f32;
    let dz = (k.z - c.z) as f32;
    let dy = (k.y - c.y) as f32;

    let base = dx.abs() + dz.abs() + 2.0 * dy.abs();
    let dir = dx * fwd_xz.x + dz * fwd_xz.y;

    let front_bonus = 0.75;
    let behind_penalty = 0.25;

    let bias = if dir >= 0.0 {
        -front_bonus * dir
    } else {
        -behind_penalty * dir
    };

    base + bias
}

// src/streaming/types.rs
// ----------------------
// src/streaming/types.rs
use std::sync::{Arc, atomic::AtomicBool};

use crate::render::gpu_types::{ChunkMetaGpu, NodeGpu, NodeRopesGpu};

pub const INVALID_U32: u32 = 0xFFFF_FFFF;

// Vertical band dy in [-1..=2]
pub const GRID_Y_MIN_DY: i32 = -2;
pub const GRID_Y_COUNT: u32 = 5;

pub const EVICT_ATTEMPTS: usize = 8;

// 8^3 bits = 512 bits = 16 u32
pub const MACRO_WORDS_PER_CHUNK: u32 = 16;
pub const MACRO_WORDS_PER_CHUNK_USIZE: usize = 16;

// 64x64 columns, packed 2x u16 per u32 => 2048 u32 per chunk
pub const COLINFO_WORDS_PER_CHUNK: u32 = 2048;
pub const COLINFO_WORDS_PER_CHUNK_USIZE: usize = 2048;

pub const MAX_UPLOADS_PER_FRAME: usize = 24;            // start 6–12
pub const MAX_UPLOAD_BYTES_PER_FRAME: usize = 16 << 20; // start 2–8 MB

pub const PRIORITY_RADIUS: i32 = 2; // => 5x5 in XZ, and with GRID_Y_* => 5 in Y


#[derive(Clone, Copy, Hash, PartialEq, Eq, Debug)]
pub struct ChunkKey {
    pub x: i32,
    pub y: i32,
    pub z: i32,
}

pub enum ChunkState {
    Queued,
    Building,
    Uploading(Uploading),
    Resident(Resident),
}

#[derive(Clone, Debug)]
pub struct Uploading {
    pub slot: u32,
    pub node_base: u32,
    pub node_count: u32,
    pub uploaded: bool,
}

#[derive(Clone, Debug)]
pub struct Resident {
    pub slot: u32,
    pub node_base: u32,
    pub node_count: u32,
}

#[derive(Clone, Debug)]
pub struct BuildJob {
    pub key: ChunkKey,
    pub cancel: Arc<AtomicBool>,
}

pub struct BuildDone {
    pub key: ChunkKey,
    pub cancel: Arc<AtomicBool>,
    pub canceled: bool,
    pub nodes: Vec<NodeGpu>,
    pub macro_words: Vec<u32>,
    pub ropes: Vec<NodeRopesGpu>,
    pub colinfo_words: Vec<u32>,
}

pub struct ChunkUpload {
    pub key: ChunkKey,
    pub slot: u32,
    pub meta: ChunkMetaGpu,

    pub node_base: u32,
    pub nodes: Arc<[NodeGpu]>,

    pub macro_words: Arc<[u32]>,

    pub ropes: Arc<[NodeRopesGpu]>,

    pub colinfo_words: Arc<[u32]>,

    pub completes_residency: bool,
}

#[inline(always)]
pub fn y_band_min() -> i32 {
    GRID_Y_MIN_DY
}

#[inline(always)]
pub fn y_band_max() -> i32 {
    GRID_Y_MIN_DY + GRID_Y_COUNT as i32 - 1
}

#[derive(Clone, Copy, Debug, Default)]
pub struct StreamStats {
    pub center: (i32, i32, i32),

    pub resident_slots: u32,
    pub total_slots: u32,
    pub chunks_map: u32,

    pub st_queued: u32,
    pub st_building: u32,
    pub st_uploading: u32,
    pub st_resident: u32,

    pub in_flight: u32,
    pub done_backlog: u32,

    pub up_rewrite: u32,
    pub up_active: u32,
    pub up_other: u32,

    pub cache_bytes: u64,
    pub cache_entries: u32,
    pub cache_lru: u32,

    // NEW
    pub build_queue_len: u32,
    pub queued_set_len: u32,
    pub cancels_len: u32,

    pub orphan_queued: u32,
}

// src/streaming/workers.rs
// ------------------------
// src/streaming/workers.rs
use std::sync::{Arc, atomic::Ordering};

use crossbeam_channel::{Receiver, Sender};

use crate::{
    config,
    render::gpu_types::{NodeGpu, NodeRopesGpu},
    svo::{build_chunk_svo_sparse_cancelable_with_scratch, BuildScratch},
    world::WorldGen,
};

use super::types::{BuildDone, BuildJob};

pub fn spawn_workers(gen: Arc<WorldGen>, rx_job: Receiver<BuildJob>, tx_done: Sender<BuildDone>) {
    for _ in 0..config::WORKER_THREADS {
        let gen = gen.clone();
        let rx_job = rx_job.clone();
        let tx_done = tx_done.clone();

        std::thread::spawn(move || {
            let mut scratch = BuildScratch::new();

            while let Ok(job) = rx_job.recv() {
                let k = job.key;

                if job.cancel.load(Ordering::Relaxed) {
                    let _ = tx_done.send(BuildDone {
                        key: k,
                        cancel: job.cancel,
                        canceled: true,
                        nodes: Vec::new(),
                        macro_words: Vec::new(),
                        ropes: Vec::new(),
                        colinfo_words: Vec::new(),
                    });
                    continue;
                }

                let origin = [
                    k.x * config::CHUNK_SIZE as i32,
                    k.y * config::CHUNK_SIZE as i32,
                    k.z * config::CHUNK_SIZE as i32,
                    0,
                ];

                let (nodes, macro_words, ropes, colinfo_words): (Vec<NodeGpu>, Vec<u32>, Vec<NodeRopesGpu>, Vec<u32>) =
                    build_chunk_svo_sparse_cancelable_with_scratch(
                        &gen,
                        [origin[0], origin[1], origin[2]],
                        config::CHUNK_SIZE,
                        job.cancel.as_ref(),
                        &mut scratch,
                    );

                let canceled = job.cancel.load(Ordering::Relaxed);
                let (nodes, macro_words, ropes) = if canceled {
                    (Vec::new(), Vec::new(), Vec::new())
                } else {
                    (nodes, macro_words, ropes)
                };

                let _ = tx_done.send(BuildDone {
                    key: k,
                    cancel: job.cancel,
                    canceled,
                    nodes,
                    macro_words,
                    ropes,
                    colinfo_words,
                });
            }
        });
    }
}

