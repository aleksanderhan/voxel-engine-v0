Im building a ratraced svo engine in rust. I want to render stuff outside the loaded svo chunks wihout loading them.


2D clipmap height/normal texture (CPU-updated), shader intersects that

A clipmap is a set of nested rings of textures around the camera (like a mipmapped donut) updated incrementally as the camera moves.

You stream just 2D data:

height (and optionally material id)

maybe a normal map derived from height

Then the shader, when outside SVO, intersects ray with that heightfield and shades it.

Tradeoffs

far terrain matches CPU worldgen exactly (because CPU writes heights)

cheap memory and bandwidth (2D, not SVO nodes)

you have to implement clipmap update logic (still simpler than SVO streaming)

Here is the codebase:


// --------------
// src/app/mod.rs
//
// High-level application glue:
// - Owns the window + wgpu surface configuration (swapchain-ish state).
// - Owns input + camera state.
// - Owns world/chunk streaming state.
// - Delegates all GPU resource ownership and rendering work to `Renderer`.
//
// The core loop is driven by winit events. We run with `ControlFlow::Poll`,
// so we continually render (and handle input) as fast as the system allows.

use std::sync::Arc;
use std::time::Instant;

use winit::{
    event::*,
    event_loop::{ControlFlow, EventLoop},
    window::Window,
};

use crate::{
    camera::Camera,
    config,
    input::InputState,
    render::{CameraGpu, OverlayGpu, Renderer},
    streaming::ChunkManager,
    world::WorldGen,
};

/// Entrypoint called by main:
/// - Builds the `App` (async, because wgpu adapter/device acquisition is async).
/// - Starts the winit event loop and forwards events into the app.
pub async fn run(event_loop: EventLoop<()>, window: Arc<Window>) {
    let mut app = App::new(window).await;

    // winit's `run` never returns in normal operation (it exits the process/event loop).
    event_loop.run(move |event, elwt| {
        elwt.set_control_flow(ControlFlow::Wait);

        match &event {
            Event::AboutToWait => {
                // ask for one redraw; OS will pace this (vsync / compositor)
                app.window.request_redraw();
            }
            Event::WindowEvent { event: WindowEvent::RedrawRequested, .. } => {
                app.frame(elwt);
            }
            _ => {
                app.handle_event(event, elwt);
            }
        }
    }).unwrap();

}

/// Application state that lives for the duration of the event loop.
///
/// Ownership split:
/// - `App` owns the *presentation surface* and surface config (format/size/present mode).
/// - `Renderer` owns the wgpu `Device` and `Queue` and all GPU resources/pipelines.
///   This keeps GPU ownership centralized and makes `App` mostly orchestration.
pub struct App {
    /// Shared window handle (Arc keeps it alive as long as needed).
    window: Arc<Window>,

    /// Time origin for animations / time-based shader params.
    start_time: Instant,

    // --- WGPU "presentation" state (kept here, not in Renderer) ---
    //
    // NOTE: The underscore-prefixed fields are intentionally retained so their
    // lifetimes match expectations and to avoid "never read" warnings.
    //
    // - Instance: top-level wgpu context (backend selection, surface creation).
    // - Surface: platform swapchain surface tied to the window.
    // - Adapter: selected physical GPU.
    // - Surface format: chosen swapchain pixel format.
    // - Surface configuration: width/height/present mode/usage, etc.
    _instance: wgpu::Instance,
    surface: wgpu::Surface<'static>,
    _adapter: wgpu::Adapter,
    _surface_format: wgpu::TextureFormat,
    config: wgpu::SurfaceConfiguration,

    /// All actual GPU pipelines/resources and per-frame encoding helpers.
    renderer: Renderer,

    // --- World + streaming state ---
    /// Procedural world generator (shared, so streaming can keep refs cheaply).
    world: Arc<WorldGen>,
    /// Chunk streaming / residency manager (decides what to load/unload/upload).
    chunks: ChunkManager,

    // --- Interaction state ---
    /// Aggregated input state updated from winit events.
    input: InputState,
    /// Camera pose/orientation and input integration.
    camera: Camera,

    // --- FPS overlay bookkeeping ---
    /// Most recent FPS value (rounded).
    fps_value: u32,
    /// Frames counted since last FPS update.
    fps_frames: u32,
    /// Timestamp of last FPS update sample window.
    fps_last: Instant,
}

impl App {
    /// Build a new `App`.
    ///
    /// Steps:
    /// 1) Create wgpu instance + surface from the window.
    /// 2) Pick adapter (GPU).
    /// 3) Choose surface format + configure surface for the initial window size.
    /// 4) Create `Renderer` (device/queue + pipelines).
    /// 5) Create world generator, chunk manager, camera, and input.
    pub async fn new(window: Arc<Window>) -> Self {
        let start_time = Instant::now();

        // Initial size (may be zero on some platforms during resize/minimize, so we clamp later).
        let size = window.inner_size();

        // Create the wgpu instance (selects backend internally).
        let instance = wgpu::Instance::default();

        // Surface must outlive the event loop; Arc<Window> makes this easy.
        // The 'static surface lifetime comes from the fact the window is kept alive.
        let surface = instance.create_surface(window.clone()).unwrap();

        // Ask wgpu to pick an adapter (GPU) that can present to this surface.
        let adapter = instance
            .request_adapter(&wgpu::RequestAdapterOptions {
                compatible_surface: Some(&surface),
                // Prefer a discrete / high perf GPU if available.
                power_preference: wgpu::PowerPreference::HighPerformance,
                // Don't force a fallback adapter (software/low-feature) unless needed.
                force_fallback_adapter: false,
            })
            .await
            .unwrap();

        // Query what the surface supports with this adapter (formats, modes, etc.).
        let surface_caps = surface.get_capabilities(&adapter);

        // Pick the first advertised format.
        // (Often this is a sensible SRGB-ish format, but you could choose based on preference.)
        let surface_format = surface_caps.formats[0];

        // Create the surface configuration ("swapchain config"):
        // - usage: we render into it as a render attachment.
        // - width/height: clamp to >= 1 to avoid invalid zero-sized surfaces.
        // - present_mode/alpha_mode: choose first supported.
        // - desired_maximum_frame_latency: reduce queued frames (helps latency).
        let config_sc = wgpu::SurfaceConfiguration {
            usage: wgpu::TextureUsages::RENDER_ATTACHMENT,
            format: surface_format,
            width: size.width.max(1),
            height: size.height.max(1),
            present_mode: surface_caps.present_modes[0],
            alpha_mode: surface_caps.alpha_modes[0],
            view_formats: vec![],
            desired_maximum_frame_latency: 2,
        };

        // Renderer owns the real device/queue; App owns surface/config.
        // Renderer is async because it requests/creates the wgpu Device/Queue.
        let renderer =
            Renderer::new(&adapter, surface_format, config_sc.width, config_sc.height).await;

        // Configure once, with the real device we will render with.
        surface.configure(renderer.device(), &config_sc);

        // Create world generation & chunk streaming controller.
        let world = Arc::new(WorldGen::new(12345));
        let chunks = ChunkManager::new(world.clone());

        // Camera starts with an initial aspect ratio derived from the surface size.
        let camera = Camera::new(config_sc.width as f32 / config_sc.height as f32);

        // Input begins empty (no keys pressed, no mouse delta, etc.).
        let input = InputState::default();

        Self {
            window,
            start_time,
            _instance: instance,
            surface,
            _adapter: adapter,
            _surface_format: surface_format,
            config: config_sc,
            renderer,
            world,
            chunks,
            input,
            camera,
            fps_value: 0,
            fps_frames: 0,
            fps_last: Instant::now(),
        }
    }

    /// Central event dispatcher called by the event loop.
    ///
    /// We route:
    /// - `DeviceEvent` into raw input (mouse motion, etc.).
    /// - `WindowEvent` into window-related input and resize/close handling.
    /// - `AboutToWait` as a "tick" to render a frame.
    pub fn handle_event(
        &mut self,
        event: Event<()>,
        elwt: &winit::event_loop::EventLoopWindowTarget<()>,
    ) {
        match event {
            // DeviceEvent fires for raw input independent of focus/window coords
            // (e.g. mouse delta from high precision devices).
            Event::DeviceEvent { event, .. } => {
                self.input.on_device_event(&event);
            }

            // WindowEvent includes keyboard, mouse buttons, focus, resize, etc.
            Event::WindowEvent { event, .. } => {
                // Let input layer consume/track events first (focus changes, key state, etc.).
                // Return value is ignored here, but could indicate "consumed".
                let _ = self.input.on_window_event(&event, &self.window);

                match event {
                    // OS requested the window close (Alt+F4, close button, etc.).
                    WindowEvent::CloseRequested => elwt.exit(),

                    // Window was resized; update surface config and renderer output.
                    WindowEvent::Resized(new_size) => {
                        // Clamp to avoid 0-sized surfaces when minimized.
                        self.config.width = new_size.width.max(1);
                        self.config.height = new_size.height.max(1);

                        // Reconfigure the surface swapchain and notify renderer
                        // so any size-dependent textures can be resized.
                        self.surface.configure(self.renderer.device(), &self.config);
                        self.renderer
                            .resize_output(self.config.width, self.config.height);
                    }

                    _ => {}
                }
            }

            // `AboutToWait` is emitted once winit has processed all pending events
            // and is about to sleep. Under Poll control flow, this is effectively
            // our per-frame callback.
            Event::AboutToWait => self.frame(elwt),

            _ => {}
        }
    }

    /// Render/update one frame.
    ///
    /// Pipeline:
    /// 1) Integrate input into camera state.
    /// 2) Update chunk streaming decisions based on camera position/forward.
    /// 3) Build GPU camera parameters (inverse matrices, chunk grid info, etc.).
    /// 4) Update FPS overlay values and write overlay uniforms.
    /// 5) Upload any newly-streamed chunk data to GPU.
    /// 6) Acquire swapchain image, encode compute + blit passes, submit, present.
    fn frame(&mut self, elwt: &winit::event_loop::EventLoopWindowTarget<()>) {
        // 1) camera integrate
        //
        // Convert accumulated input state (keys/mouse deltas) into updated camera pose.
        self.camera.integrate_input(&mut self.input);

        // 2) streaming update
        //
        // Use camera pose to decide which chunks should be present/resident.
        // Then write the chunk grid metadata (addresses/ids) to the renderer.
        let cam_pos = self.camera.position();
        let cam_fwd = self.camera.forward();
        let grid_changed = self.chunks.update(&self.world, cam_pos, cam_fwd);
        if grid_changed {
            self.renderer.write_chunk_grid(self.chunks.chunk_grid());
        }


        // 3) camera matrices -> CameraGpu
        //
        // Compute view/projection matrices and pack their inverses for shader usage.
        // Inverse matrices let shaders go from screen-space rays back into world-space.
        let aspect = self.config.width as f32 / self.config.height as f32;
        let cf = self.camera.frame_matrices(aspect);

        // Elapsed time since startup (typically used for animation/noise jitter/etc.).
        let t = self.start_time.elapsed().as_secs_f32();

        // Raymarch/trace step limit: derived from chunk size but clamped to a sane band.
        // (Avoids tiny chunk sizes producing too few steps, and huge sizes producing too many.)
        let max_steps = (config::CHUNK_SIZE * 2).clamp(48, 96);

        // Camera uniform/SSBO payload for GPU.
        let cam_gpu = CameraGpu {
            view_inv: cf.view_inv.to_cols_array_2d(),
            proj_inv: cf.proj_inv.to_cols_array_2d(),
            cam_pos: [cf.pos.x, cf.pos.y, cf.pos.z, 1.0],

            // Chunking parameters used by shaders to interpret the streamed grid.
            chunk_size: config::CHUNK_SIZE,
            chunk_count: self.chunks.chunk_count(),
            max_steps,
            _pad0: 0,

            // Misc voxel/shader params:
            // [voxel_size_in_meters, time, ?, ?] (the last two likely tune lighting/density).
            voxel_params: [config::VOXEL_SIZE_M_F32, t, 2.0, 0.002],

            // Chunk grid origin and dimensions in chunk coordinates.
            // Packed as ivec-ish arrays with a trailing padding element.
            grid_origin_chunk: [
                self.chunks.grid_origin()[0],
                self.chunks.grid_origin()[1],
                self.chunks.grid_origin()[2],
                0,
            ],
            grid_dims: [
                self.chunks.grid_dims()[0],
                self.chunks.grid_dims()[1],
                self.chunks.grid_dims()[2],
                0,
            ],
        };

        // Upload camera params to GPU.
        self.renderer.write_camera(&cam_gpu);

        // 4) fps overlay
        //
        // We update FPS roughly 4 times per second (every 0.25s) to smooth noise.
        self.fps_frames += 1;
        let dt = self.fps_last.elapsed().as_secs_f32();
        if dt >= 0.25 {
            let fps = (self.fps_frames as f32) / dt;
            self.fps_value = fps.round() as u32;
            self.fps_frames = 0;
            self.fps_last = Instant::now();
        }

        // Overlay uniform payload for the GPU overlay pass.
        let overlay = OverlayGpu {
            fps: self.fps_value,
            width: self.config.width,
            height: self.config.height,
            _pad0: 0,
        };
        self.renderer.write_overlay(&overlay);

        // 5) update scene buffers if changed
        //
        // If the chunk manager has produced new/updated chunk data (meshes/voxels/etc),
        // apply those uploads to GPU resources before encoding this frame.
        self.renderer.apply_chunk_uploads(self.chunks.take_uploads());

        // 6) acquire frame + encode passes
        //
        // Acquire the next drawable surface texture. Handle common surface errors.
        let frame = match self.surface.get_current_texture() {
            Ok(f) => f,

            // Surface got invalidated (resize, display mode change, etc.) -> reconfigure.
            Err(wgpu::SurfaceError::Lost | wgpu::SurfaceError::Outdated) => {
                self.surface.configure(self.renderer.device(), &self.config);
                return;
            }

            // Temporary issue: skip this frame.
            Err(wgpu::SurfaceError::Timeout) => return,

            // Fatal-ish: GPU memory exhaustion -> exit.
            Err(wgpu::SurfaceError::OutOfMemory) => {
                elwt.exit();
                return;
            }
        };

        // View into the swapchain image used as render target in the blit pass.
        let frame_view = frame.texture.create_view(&Default::default());

        // Command encoder collects GPU commands for this frame into a single submission.
        let mut encoder = self
            .renderer
            .device()
            .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("encoder"),
            });

        // First: run compute (likely raymarching / voxel traversal) into an offscreen target.
        self.renderer
            .encode_compute(&mut encoder, self.config.width, self.config.height);

        // Then: blit (copy/compose) the offscreen output into the swapchain image.
        self.renderer.encode_blit(&mut encoder, &frame_view);

        // Submit GPU work and present the swapchain image.
        self.renderer.queue().submit(Some(encoder.finish()));
        frame.present();
    }
}


// src/camera.rs
use glam::{Mat4, Vec3};

use crate::{config, input::InputState};

pub struct Camera {
    pos: Vec3,
    yaw: f32,
    pitch: f32,
    fovy_rad: f32,
    z_near: f32,
    z_far: f32,
    // movement tuning
    speed_per_frame: f32,
    mouse_sens: f32,
}

pub struct CameraFrame {
    pub view_inv: Mat4,
    pub proj_inv: Mat4,
    pub pos: Vec3,
}

impl Camera {
    pub fn new(aspect: f32) -> Self {
        let _ = aspect; // kept for future (if you want aspect-dependent params)
        Self {
            pos: Vec3::new((config::CHUNK_SIZE as f32 * config::VOXEL_SIZE_M_F32) * 0.5, 20.0, -20.0),
            yaw: 0.0,
            pitch: 0.15,
            fovy_rad: 60.0_f32.to_radians(),
            z_near: 0.1,
            z_far: 1000.0,
            speed_per_frame: 0.35,
            mouse_sens: 0.0025,
        }
    }

    pub fn position(&self) -> Vec3 {
        self.pos
    }

    pub fn forward(&self) -> glam::Vec3 {
        let (yaw, pitch) = (self.yaw, self.pitch);
        glam::Vec3::new(
            yaw.sin() * pitch.cos(),
            pitch.sin(),
            yaw.cos() * pitch.cos(),
        )
        .normalize()
    }

    pub fn integrate_input(&mut self, input: &mut InputState) {
        // mouse look
        if input.focused {
            let (dx, dy) = input.take_mouse_delta();
            self.yaw -= dx * self.mouse_sens;
            self.pitch = (self.pitch - dy * self.mouse_sens).clamp(-1.55, 1.55);
        } else {
            // still clear deltas
            let _ = input.take_mouse_delta();
        }

        // basis
        let forward = Vec3::new(
            self.yaw.sin() * self.pitch.cos(),
            self.pitch.sin(),
            self.yaw.cos() * self.pitch.cos(),
        )
        .normalize();

        let right = forward.cross(Vec3::Y).normalize();
        let up = right.cross(forward).normalize();

        // movement (per-frame like your original)
        let k = input.keys;
        let mut vel = Vec3::ZERO;
        if k.w { vel += forward; }
        if k.s { vel -= forward; }
        if k.d { vel += right; }
        if k.a { vel -= right; }
        if k.space { vel += up; }
        if k.alt { vel -= up; }

        if vel.length_squared() > 0.0 {
            self.pos += vel.normalize() * self.speed_per_frame;
        }
    }

    pub fn frame_matrices(&self, aspect: f32) -> CameraFrame {
        let forward = Vec3::new(
            self.yaw.sin() * self.pitch.cos(),
            self.pitch.sin(),
            self.yaw.cos() * self.pitch.cos(),
        )
        .normalize();

        let view = Mat4::look_at_rh(self.pos, self.pos + forward, Vec3::Y);
        let proj = Mat4::perspective_rh(self.fovy_rad, aspect, self.z_near, self.z_far);

        CameraFrame {
            view_inv: view.inverse(),
            proj_inv: proj.inverse(),
            pos: self.pos,
        }
    }
}


// src/config.rs
// -------------
// Global config knobs for the voxel/SVO renderer + streaming.

pub const CHUNK_SIZE: u32 = 128;

pub const ACTIVE_RADIUS: i32 = 4;
pub const KEEP_RADIUS: i32 = ACTIVE_RADIUS + 13;

pub const VOXEL_SIZE_M_F32: f32 = 0.10;
pub const VOXEL_SIZE_M_F64: f64 = 0.10;

// Keep this explicit so you can change voxel size later without hunting constants.
pub const VOXELS_PER_METER: i32 = 10; // 1.0 / 0.10

pub const WORKER_THREADS: usize = 4;
pub const MAX_IN_FLIGHT: usize = 8;

// GPU node arena budget (storage buffer capacity).
pub const NODE_BUDGET_BYTES: usize = 1024 * 1024 * 1024; // 1 GB

// CPU chunk cache budget (SVO nodes stored on CPU so we don't rebuild chunks).
// This is the *total* bytes of cached NodeGpu arrays across all cached chunks.
pub const CHUNK_CACHE_BUDGET_BYTES: usize = 512 * 1024 * 1024; // 512 MB


// src/main.rs
mod app;
mod camera;
mod config;
mod input;
mod render;
mod streaming;
mod svo;
mod world;

use std::sync::Arc;
use winit::{event_loop::EventLoop, window::{Fullscreen, WindowBuilder}, dpi::PhysicalSize};

fn main() {
    let event_loop = EventLoop::new().unwrap();

    let window = Arc::new(
        WindowBuilder::new()
            .with_title("SVO MVP")
            .with_inner_size(PhysicalSize::new(1280, 720))
            .build(&event_loop)
            .unwrap(),
    );
    window.set_fullscreen(Some(Fullscreen::Borderless(None)));

    pollster::block_on(app::run(event_loop, window));
}


// src/render/gpu_types.rs
//
// GPU-facing data layouts shared between Rust and WGSL.
//
// These structs are written into uniform/storage buffers and read by shaders.
// Requirements for correctness:
// - `#[repr(C)]` fixes a predictable field order/layout.
// - `Pod` + `Zeroable` (bytemuck) guarantee the types can be safely cast to bytes.
// - Fields are sized/aligned to match WGSL expectations (notably 16-byte alignment rules).
//
// Practical note about padding:
// WGSL has strict alignment for uniform/storage layouts (e.g. vec3/vec4 alignment),
// so we often include explicit `_pad*` fields to keep everything 16-byte aligned.

use bytemuck::{Pod, Zeroable};

/// Node in the GPU-side node arena (likely an SVO / tree / BVH-like structure).
///
/// Expected usage:
/// - Stored in a STORAGE buffer (read-only from shaders in this codebase).
/// - Addressed by index; child pointers are expressed as a base index + mask.
///
/// Field ideas (based on naming):
/// - child_base: base index of this node's children in the global node arena.
/// - child_mask: bitmask indicating which child slots are present/valid.
/// - material: per-node material/voxel payload (id/flags/packed params).
/// - _pad: explicit padding to keep 16-byte stride (nice for storage access patterns).
#[repr(C)]
#[derive(Clone, Copy, Pod, Zeroable, Debug)]
pub struct NodeGpu {
    pub child_base: u32,
    pub child_mask: u32,
    pub material: u32,
    pub _pad: u32,
}

/// Per-chunk metadata stored in a persistent STORAGE buffer.
///
/// Shaders use this to locate the chunk in world space and map the chunk to its nodes
/// within the global node arena.
///
/// - origin: chunk origin in chunk coordinates (xyz) with a spare lane for alignment.
/// - node_base/node_count: range into the node arena that belongs to this chunk.
/// - _pad*: explicit padding to preserve alignment / struct stride.
#[repr(C)]
#[derive(Clone, Copy, Pod, Zeroable)]
pub struct ChunkMetaGpu {
    /// Chunk origin expressed as i32 for easy chunk-space arithmetic in shaders.
    /// origin[3] is unused padding (keeps 16B alignment for the next fields).
    pub origin: [i32; 4],

    /// First node index in the node arena for this chunk.
    pub node_base: u32,

    /// Number of nodes belonging to this chunk.
    pub node_count: u32,

    pub _pad0: u32,
    pub _pad1: u32,
}

/// Camera + frame parameters packed for GPU consumption.
///
/// Stored as a UNIFORM buffer, updated once per frame.
///
/// Contents are tailored for screen-space ray generation / ray marching:
/// - Inverse matrices let the shader reconstruct world-space rays from pixel coords.
/// - cam_pos is vec4-aligned for uniform layout friendliness.
/// - Chunk/grid parameters tell the shader how to interpret streamed chunk data.
#[repr(C)]
#[derive(Clone, Copy, Pod, Zeroable)]
pub struct CameraGpu {
    /// Inverse view matrix (camera -> world transform).
    pub view_inv: [[f32; 4]; 4],

    /// Inverse projection matrix (clip -> view transform).
    pub proj_inv: [[f32; 4]; 4],

    /// Camera position in world space. Fourth component is padding (or could be 1.0).
    pub cam_pos: [f32; 4],

    /// Voxel chunk edge length (in voxels) for index math in shaders.
    pub chunk_size: u32,

    /// Number of chunk slots currently considered resident/valid.
    pub chunk_count: u32,

    /// Max raymarch/trace steps for the primary shader.
    pub max_steps: u32,

    pub _pad0: u32,

    /// Misc per-frame voxel/shader knobs.
    /// (Meaning is shader-defined; typical uses: voxel size, time, density, jitter, etc.)
    pub voxel_params: [f32; 4],

    /// Chunk grid origin in chunk coordinates (cx0, cy0, cz0, unused).
    /// This is the chunk-space coordinate of grid cell (0,0,0).
    pub grid_origin_chunk: [i32; 4],

    /// Chunk grid dimensions (nx, ny, nz, unused).
    /// Used to bounds-check and map 3D grid coords -> linear index into chunk_grid buffer.
    pub grid_dims: [u32; 4],
}

/// Tiny overlay uniform block for the final blit / UI overlay.
///
/// Stored as a UNIFORM buffer, updated periodically (FPS) and on resize (width/height).
#[repr(C)]
#[derive(Clone, Copy, Pod, Zeroable)]
pub struct OverlayGpu {
    /// FPS value displayed in the overlay.
    pub fps: u32,

    /// Output width in pixels.
    pub width: u32,

    /// Output height in pixels.
    pub height: u32,

    pub _pad0: u32,
}


// src/render/mod.rs

pub mod gpu_types;
pub mod resources;
pub mod shaders;
pub mod state;

pub use gpu_types::*;
pub use state::Renderer;


// src/render/resources.rs
//
// Small GPU resource helpers that don't fit cleanly into the renderer "state" modules.
//
// Right now this file provides the final full-resolution output texture:
// - written as a STORAGE texture by the composite compute pass
// - sampled as a regular texture by the final blit render pass
//
// Keeping this as a tiny helper makes the main texture set code a bit cleaner.

/// Wrapper for the renderer's final output texture view.
///
/// The renderer stores only the TextureView; the view keeps the underlying texture alive
/// for as long as it exists (wgpu uses ref-counted internal ownership).
pub struct OutputTex {
    /// Texture view bound in bind groups (storage write in compute, sampled in blit).
    pub view: wgpu::TextureView,
}

/// Create the final output texture (full resolution).
///
/// Properties:
/// - Format: RGBA16F (high dynamic range, good for post-processing)
/// - Usage:
///   - STORAGE_BINDING: composite pass writes into it as a storage texture
///   - TEXTURE_BINDING: blit pass samples it in the fragment shader
///
/// Notes:
/// - wgpu forbids zero-sized textures, so we clamp `w`/`h` to at least 1.
pub fn create_output_texture(device: &wgpu::Device, w: u32, h: u32) -> OutputTex {
    // Avoid creating zero-sized textures (can happen during minimize/resizes).
    let w = w.max(1);
    let h = h.max(1);

    // Allocate the GPU texture backing store.
    let tex = device.create_texture(&wgpu::TextureDescriptor {
        label: Some("output_tex"),
        size: wgpu::Extent3d {
            width: w,
            height: h,
            depth_or_array_layers: 1,
        },
        mip_level_count: 1,
        sample_count: 1,
        dimension: wgpu::TextureDimension::D2,
        format: wgpu::TextureFormat::Rgba16Float,
        // Must support both compute writes and render sampling.
        usage: wgpu::TextureUsages::STORAGE_BINDING | wgpu::TextureUsages::TEXTURE_BINDING,
        view_formats: &[],
    });

    // Default view covers the whole texture.
    let view = tex.create_view(&Default::default());

    OutputTex { view }
}


// src/render/shaders.rs
//
// Centralized shader sources. WGSL has no native include mechanism in wgpu,
// so we concatenate multiple WGSL files into a single source string.

pub const RAY_CS_WGSL: &str = concat!(
    include_str!("../shaders/common.wgsl"),
    "\n",
    include_str!("../shaders/ray_core.wgsl"),
    "\n",
    include_str!("../shaders/ray_main.wgsl"),
    "\n",
);

pub const BLIT_WGSL: &str = include_str!("../shaders/blit.wgsl");

// Optional function wrappers (keeps call sites like shaders::ray_cs_wgsl()).
#[inline]
pub fn ray_cs_wgsl() -> &'static str {
    RAY_CS_WGSL
}

#[inline]
pub fn blit_wgsl() -> &'static str {
    BLIT_WGSL
}

// src/render/state/bindgroups.rs
//
// Bind group creation.
//
// Bind groups are the concrete "argument packs" you bind before dispatching a
// compute pass or drawing a render pass. Each bind group must match a specific
// BindGroupLayout (the pipeline's expectation).
//
// This file is split out so resize() only needs to recreate textures and then
// rebuild bind groups (since bind groups reference texture views).

use super::{buffers::Buffers, layout::Layouts, textures::TextureSet};

/// All bind groups used by the renderer.
///
/// Naming convention:
/// - primary/scene are group(0) alternatives for different pipelines.
/// - godray/composite are additional groups for post processing.
/// - empty is a placeholder when a pipeline layout expects groups that a pass
///   doesn't logically use.
/// - blit is the final render pass bind group (sample output to swapchain).
pub struct BindGroups {
    /// group(0) for the primary compute pass.
    /// Includes storage outputs (color/depth) so the compute shader can write to them.
    pub primary: wgpu::BindGroup,

    /// group(0) for the godray compute pass.
    /// Excludes storage outputs to avoid binding the same textures as both storage + sampled.
    pub scene: wgpu::BindGroup,

    /// group(1) for godray ping-pong history:
    /// index selects which "direction" we are doing this frame (A->B or B->A).
    pub godray: [wgpu::BindGroup; 2],

    /// group(2) for composite:
    /// index selects which godray texture (A or B) we read when composing into `output`.
    pub composite: [wgpu::BindGroup; 2],

    /// Empty bind group used as a placeholder for group(0/1) in the composite pass.
    /// Some pipeline layouts are fixed to expect groups that a pass doesn't use.
    pub empty: wgpu::BindGroup,

    /// Bind group for the final blit render pass (texture + sampler + overlay uniform).
    pub blit: wgpu::BindGroup,
}

/// Create bind group for the primary compute pass.
///
/// Layout expectations_toggle (by binding index):
/// 0: camera uniform/storage buffer
/// 1: chunk buffer
/// 2: node buffer
/// 3: chunk grid buffer
/// 4: color texture view (storage texture the compute shader writes into)
/// 5: depth texture view (storage texture the compute shader writes into)
fn make_primary_bg(
    device: &wgpu::Device,
    layout: &wgpu::BindGroupLayout,
    buffers: &Buffers,
    textures: &TextureSet,
) -> wgpu::BindGroup {
    device.create_bind_group(&wgpu::BindGroupDescriptor {
        label: Some("primary_bg"),
        layout,
        entries: &[
            // Camera parameters (matrices, position, etc.)
            wgpu::BindGroupEntry {
                binding: 0,
                resource: buffers.camera.as_entire_binding(),
            },
            // Chunk metadata buffer (streamed voxel/chunk data indexing)
            wgpu::BindGroupEntry {
                binding: 1,
                resource: buffers.chunk.as_entire_binding(),
            },
            // Node/acceleration structure data (whatever "node" represents in your scene)
            wgpu::BindGroupEntry {
                binding: 2,
                resource: buffers.node.as_entire_binding(),
            },
            // Grid mapping from world chunk coords -> buffer indices / residency info
            wgpu::BindGroupEntry {
                binding: 3,
                resource: buffers.chunk_grid.as_entire_binding(),
            },
            // Primary color output (storage texture view)
            wgpu::BindGroupEntry {
                binding: 4,
                resource: wgpu::BindingResource::TextureView(&textures.color.view),
            },
            // Primary depth output (storage texture view)
            wgpu::BindGroupEntry {
                binding: 5,
                resource: wgpu::BindingResource::TextureView(&textures.depth.view),
            },
        ],
    })
}

/// Create bind group for "scene" inputs that are common across passes.
///
/// This intentionally *does not* include storage outputs, so it can be reused in
/// compute passes that only read scene data and sample textures elsewhere.
fn make_scene_bg(
    device: &wgpu::Device,
    layout: &wgpu::BindGroupLayout,
    buffers: &Buffers,
) -> wgpu::BindGroup {
    device.create_bind_group(&wgpu::BindGroupDescriptor {
        label: Some("scene_bg"),
        layout,
        entries: &[
            wgpu::BindGroupEntry {
                binding: 0,
                resource: buffers.camera.as_entire_binding(),
            },
            wgpu::BindGroupEntry {
                binding: 1,
                resource: buffers.chunk.as_entire_binding(),
            },
            wgpu::BindGroupEntry {
                binding: 2,
                resource: buffers.node.as_entire_binding(),
            },
            wgpu::BindGroupEntry {
                binding: 3,
                resource: buffers.chunk_grid.as_entire_binding(),
            },
        ],
    })
}

/// Create bind group for the godray compute pass.
///
/// Bindings (by index):
/// 0: depth (read-only) - used for occlusion / ray marching limits.
/// 1: history texture (read-only) - previous godray accumulation.
/// 2: output texture (write) - current frame godray result.
///
/// The ping-pong scheme swaps which texture is "history" and which is "out"
/// each frame to avoid read/write hazards on the same texture.
fn make_godray_bg(
    device: &wgpu::Device,
    layout: &wgpu::BindGroupLayout,
    depth_view: &wgpu::TextureView,
    hist_view: &wgpu::TextureView,
    out_view: &wgpu::TextureView,
    label: &str,
) -> wgpu::BindGroup {
    device.create_bind_group(&wgpu::BindGroupDescriptor {
        label: Some(label),
        layout,
        entries: &[
            wgpu::BindGroupEntry {
                binding: 0,
                resource: wgpu::BindingResource::TextureView(depth_view),
            },
            wgpu::BindGroupEntry {
                binding: 1,
                resource: wgpu::BindingResource::TextureView(hist_view),
            },
            wgpu::BindGroupEntry {
                binding: 2,
                resource: wgpu::BindingResource::TextureView(out_view),
            },
        ],
    })
}

/// Create bind group for the composite compute pass.
///
/// Bindings (by index):
/// 0: base color texture (read-only) - the main scene color.
/// 1: godray texture (read-only) - chosen ping-pong buffer (A or B).
/// 2: output texture (write) - final composited frame texture.
fn make_composite_bg(
    device: &wgpu::Device,
    layout: &wgpu::BindGroupLayout,
    color_view: &wgpu::TextureView,
    godray_view: &wgpu::TextureView,
    output_view: &wgpu::TextureView,
    label: &str,
) -> wgpu::BindGroup {
    device.create_bind_group(&wgpu::BindGroupDescriptor {
        label: Some(label),
        layout,
        entries: &[
            wgpu::BindGroupEntry {
                binding: 0,
                resource: wgpu::BindingResource::TextureView(color_view),
            },
            wgpu::BindGroupEntry {
                binding: 1,
                resource: wgpu::BindingResource::TextureView(godray_view),
            },
            wgpu::BindGroupEntry {
                binding: 2,
                resource: wgpu::BindingResource::TextureView(output_view),
            },
        ],
    })
}

/// Create bind group for the final blit render pass.
///
/// Bindings (by index):
/// 0: output texture (read-only) - full-screen texture to present.
/// 1: sampler - filtering and addressing for sampling output.
/// 2: overlay uniform buffer - small UI data (FPS/size) consumed by fragment shader.
fn make_blit_bg(
    device: &wgpu::Device,
    layout: &wgpu::BindGroupLayout,
    output_view: &wgpu::TextureView,
    sampler: &wgpu::Sampler,
    overlay_buf: &wgpu::Buffer,
) -> wgpu::BindGroup {
    device.create_bind_group(&wgpu::BindGroupDescriptor {
        label: Some("blit_bg"),
        layout,
        entries: &[
            // Fullscreen texture to sample in the fragment shader.
            wgpu::BindGroupEntry {
                binding: 0,
                resource: wgpu::BindingResource::TextureView(output_view),
            },
            // Sampler used when sampling the output texture.
            wgpu::BindGroupEntry {
                binding: 1,
                resource: wgpu::BindingResource::Sampler(sampler),
            },
            // Overlay uniforms (FPS, screen size, etc.).
            wgpu::BindGroupEntry {
                binding: 2,
                resource: overlay_buf.as_entire_binding(),
            },
        ],
    })
}

/// Top-level helper to (re)create all bind groups.
///
/// Called during initial renderer creation, and again after resize since texture
/// views change (new textures -> new views -> old bind groups become invalid).
pub fn create_bind_groups(
    device: &wgpu::Device,
    layouts: &Layouts,
    buffers: &Buffers,
    textures: &TextureSet,
    sampler: &wgpu::Sampler,
) -> BindGroups {
    // Primary includes storage outputs (color/depth), so it must NOT be used in godray.
    // Otherwise you'd risk binding the same resource in incompatible ways across passes.
    let primary = make_primary_bg(device, &layouts.primary, buffers, textures);

    // Scene excludes storage outputs; safe for passes that only need buffers.
    let scene = make_scene_bg(device, &layouts.scene, buffers);

    // Empty bind group used as placeholder for composite pipeline layout group(0/1).
    // (Composite pass likely only uses group(2), but the pipeline layout includes 0..2.)
    let empty = device.create_bind_group(&wgpu::BindGroupDescriptor {
        label: Some("empty_bg"),
        layout: &layouts.empty,
        entries: &[],
    });

    // Godray ping-pong:
    // - godray[0] reads history=A, writes out=B
    // - godray[1] reads history=B, writes out=A
    //
    // `textures.godray` is a pair of textures used to avoid reading and writing the same
    // texture in a single dispatch.
    let godray = [
        make_godray_bg(
            device,
            &layouts.godray,
            &textures.depth.view,
            &textures.godray[0].view,
            &textures.godray[1].view,
            "godray_bg_a_to_b",
        ),
        make_godray_bg(
            device,
            &layouts.godray,
            &textures.depth.view,
            &textures.godray[1].view,
            &textures.godray[0].view,
            "godray_bg_b_to_a",
        ),
    ];

    // Composite bind groups:
    // Same pipeline/layout, but choose which godray texture to read (A or B).
    let composite = [
        make_composite_bg(
            device,
            &layouts.composite,
            &textures.color.view,
            &textures.godray[0].view,
            &textures.output.view,
            "composite_bg_read_a",
        ),
        make_composite_bg(
            device,
            &layouts.composite,
            &textures.color.view,
            &textures.godray[1].view,
            &textures.output.view,
            "composite_bg_read_b",
        ),
    ];

    // Blit bind group:
    // Samples final output into the swapchain; overlay buffer provides UI data.
    let blit = make_blit_bg(
        device,
        &layouts.blit,
        &textures.output.view,
        sampler,
        &buffers.overlay,
    );

    BindGroups {
        primary,
        scene,
        godray,
        composite,
        empty,
        blit,
    }
}


// src/render/state/buffers.rs
//
// Persistent GPU buffers and capacities.
//
// This module owns the long-lived GPU buffers used across frames.
// It's separated out so the renderer code can focus on passes/encoding, while
// buffer sizing/allocation logic lives here.
//
// Buffer types used here:
// - UNIFORM buffers: small per-frame parameter blocks (camera, overlay).
// - STORAGE buffers: larger arenas updated incrementally (nodes, chunk metadata, chunk grid).

use crate::{
    config,
    render::gpu_types::{ChunkMetaGpu, NodeGpu},
};

/// Collection of GPU buffers that persist for the lifetime of the renderer.
///
/// These buffers are recreated only when the renderer is created (or when you
/// decide to change capacities). They are written to each frame via COPY_DST.
pub struct Buffers {
    // --- Uniforms (small, frequently updated) ---

    /// Camera uniforms (inverse matrices, camera position, grid params, etc.).
    /// Written each frame.
    pub camera: wgpu::Buffer,

    /// Overlay uniforms (FPS and screen size, etc.).
    /// Written each frame.
    pub overlay: wgpu::Buffer,

    // --- Persistent storage buffers (larger arenas) ---

    /// Node arena (likely an SVO / acceleration structure node pool).
    /// Capacity is derived from a fixed byte budget.
    pub node: wgpu::Buffer,

    /// Chunk metadata array for all resident chunks (as determined by streaming).
    pub chunk: wgpu::Buffer,

    /// Chunk grid indirection table (u32 handles/indices mapping 3D grid -> chunk slot).
    pub chunk_grid: wgpu::Buffer,

    // --- Capacities (element counts, not bytes) ---

    /// Number of NodeGpu elements the node arena can hold.
    pub node_capacity: u32,

    /// Number of ChunkMetaGpu elements the chunk metadata buffer can hold.
    pub chunk_capacity: u32,

    /// Number of u32 entries in the chunk grid buffer.
    pub grid_capacity: u32,
}

/// Helper to create a fixed-size uniform buffer for some POD-ish type `T`.
///
/// Usage flags:
/// - UNIFORM: bind as uniform buffer in shaders
/// - COPY_DST: allow updating via queue.write_buffer / copy operations
fn make_uniform_buffer<T: Sized>(device: &wgpu::Device, label: &str) -> wgpu::Buffer {
    device.create_buffer(&wgpu::BufferDescriptor {
        label: Some(label),
        // Uniform holds exactly one T.
        size: std::mem::size_of::<T>() as u64,
        usage: wgpu::BufferUsages::UNIFORM | wgpu::BufferUsages::COPY_DST,
        mapped_at_creation: false,
    })
}

/// Helper to create a storage buffer of a specific byte size.
///
/// Usage flags:
/// - STORAGE: bind as storage buffer (read/write in compute, read in fragment/vertex if allowed)
/// - COPY_DST: allow CPU uploads
fn make_storage_buffer(device: &wgpu::Device, label: &str, size_bytes: u64) -> wgpu::Buffer {
    device.create_buffer(&wgpu::BufferDescriptor {
        label: Some(label),
        size: size_bytes,
        usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
        mapped_at_creation: false,
    })
}

/// Create all persistent buffers with capacities derived from config constants.
///
/// Important: this only allocates GPU memory. The actual contents are expected
/// to be filled/updated by streaming + renderer code at runtime.
pub fn create_persistent_buffers(device: &wgpu::Device) -> Buffers {
    // --- Uniform buffers ---

    // Camera uniform buffer (one CameraGpu struct).
    let camera = make_uniform_buffer::<crate::render::gpu_types::CameraGpu>(device, "camera_buf");

    // Overlay uniform buffer (one OverlayGpu struct).
    let overlay =
        make_uniform_buffer::<crate::render::gpu_types::OverlayGpu>(device, "overlay_buf");

    // --- Storage buffers ---

    // Node arena capacity derived from a fixed byte budget.
    //
    // NOTE: integer division truncates, which is fine: we allocate as many whole NodeGpu
    // elements as fit into NODE_BUDGET_BYTES.
    let node_capacity = (config::NODE_BUDGET_BYTES / std::mem::size_of::<NodeGpu>()) as u32;

    // Allocate node arena buffer sized to hold `node_capacity` NodeGpu elements.
    let node = make_storage_buffer(
        device,
        "svo_nodes_arena",
        (node_capacity as u64) * (std::mem::size_of::<NodeGpu>() as u64),
    );

    // Chunk meta capacity: max resident chunks in the KEEP box (streaming working set).
    //
    // The formula here implies:
    // - X dimension: (2*KEEP_RADIUS + 1)
    // - Z dimension: (2*KEEP_RADIUS + 1)
    // - Y dimension: 4 (hard-coded vertical span; likely number of chunk layers kept)
    //
    // So total = X * Y * Z.
    let chunk_capacity =
        (2 * config::KEEP_RADIUS + 1) as u32 * 4u32 * (2 * config::KEEP_RADIUS + 1) as u32;

    // Allocate chunk metadata buffer sized to hold `chunk_capacity` ChunkMetaGpu elements.
    let chunk = make_storage_buffer(
        device,
        "chunk_meta_persistent",
        (chunk_capacity as u64) * (std::mem::size_of::<ChunkMetaGpu>() as u64),
    );

    // Chunk grid buffer (fixed capacity).
    //
    // Stored as u32 indices/handles, likely mapping each grid cell to a slot in `chunk`
    // or a sentinel for "empty".
    // Here it's set equal to chunk_capacity, meaning "one u32 per chunk slot".
    let grid_capacity = chunk_capacity;

    let chunk_grid = make_storage_buffer(
        device,
        "chunk_grid_buf",
        (grid_capacity as u64) * (std::mem::size_of::<u32>() as u64),
    );

    // Return the assembled buffer set and their capacities.
    Buffers {
        camera,
        overlay,
        node,
        chunk,
        chunk_grid,
        node_capacity,
        chunk_capacity,
        grid_capacity,
    }
}


// src/render/state/layout.rs
//
// Bind group layouts and small "entry constructors" to reduce repetition.
//
// A BindGroupLayout is the *schema* for a bind group: it defines which resources
// exist at which @group/@binding, and how shaders are allowed to access them.
//
// This module encodes the contract between WGSL `@group(n) @binding(m)`
// declarations and the Rust-side wgpu setup. If bindings or types mismatch,
// pipeline creation or bind group creation will fail (or validation will trip).

pub struct Layouts {
    /// group(0): scene inputs + storage outputs (color/depth). Used only by the primary pass.
    pub primary: wgpu::BindGroupLayout,

    /// group(0): scene inputs only (camera + buffers). Used by godray to avoid storage conflicts.
    pub scene: wgpu::BindGroupLayout,

    /// group(1): godray pass resources (depth sample + history sample + out storage).
    pub godray: wgpu::BindGroupLayout,

    /// group(2): composite pass resources (color sample + godray sample + output storage).
    pub composite: wgpu::BindGroupLayout,

    /// Empty layout: used as a placeholder when a pipeline layout expects group(0/1)
    /// but the composite pass only meaningfully uses group(2).
    pub empty: wgpu::BindGroupLayout,

    /// Blit render pass layout: sample final output texture + sampler + overlay uniform.
    pub blit: wgpu::BindGroupLayout,
}

// -----------------------------------------------------------------------------
// BindGroupLayoutEntry helpers
// -----------------------------------------------------------------------------
//
// These tiny constructors keep the layout definitions readable and ensure
// consistent settings (no dynamic offsets, no min_binding_size, etc.).

/// Convenience for a uniform-buffer entry.
///
/// - `binding`: WGSL @binding index
/// - `visibility`: which shader stages can read this binding
fn bgl_uniform(binding: u32, visibility: wgpu::ShaderStages) -> wgpu::BindGroupLayoutEntry {
    wgpu::BindGroupLayoutEntry {
        binding,
        visibility,
        ty: wgpu::BindingType::Buffer {
            ty: wgpu::BufferBindingType::Uniform,
            // No dynamic offsets; the entire buffer is bound as-is.
            has_dynamic_offset: false,
            // None => let wgpu infer/validate size at bind time.
            min_binding_size: None,
        },
        // Not an array binding.
        count: None,
    }
}

/// Convenience for a read-only storage-buffer entry.
fn bgl_storage_ro(binding: u32, visibility: wgpu::ShaderStages) -> wgpu::BindGroupLayoutEntry {
    wgpu::BindGroupLayoutEntry {
        binding,
        visibility,
        ty: wgpu::BindingType::Buffer {
            ty: wgpu::BufferBindingType::Storage { read_only: true },
            has_dynamic_offset: false,
            min_binding_size: None,
        },
        count: None,
    }
}

/// Convenience for a sampled 2D texture entry.
///
/// `sample_type` must match the WGSL texture type (float/int/uint + filterable or not).
fn bgl_tex_sample(
    binding: u32,
    visibility: wgpu::ShaderStages,
    sample_type: wgpu::TextureSampleType,
) -> wgpu::BindGroupLayoutEntry {
    wgpu::BindGroupLayoutEntry {
        binding,
        visibility,
        ty: wgpu::BindingType::Texture {
            sample_type,
            view_dimension: wgpu::TextureViewDimension::D2,
            multisampled: false,
        },
        count: None,
    }
}

/// Convenience for a write-only storage texture entry.
///
/// Storage textures are typically used as compute outputs.
/// `format` must match the actual texture format and the WGSL storage texture declaration.
fn bgl_storage_tex_wo(
    binding: u32,
    visibility: wgpu::ShaderStages,
    format: wgpu::TextureFormat,
) -> wgpu::BindGroupLayoutEntry {
    wgpu::BindGroupLayoutEntry {
        binding,
        visibility,
        ty: wgpu::BindingType::StorageTexture {
            access: wgpu::StorageTextureAccess::WriteOnly,
            format,
            view_dimension: wgpu::TextureViewDimension::D2,
        },
        count: None,
    }
}

// -----------------------------------------------------------------------------
// Layout creation
// -----------------------------------------------------------------------------

/// Create all bind group layouts used by the renderer.
///
/// The specific formats and binding indices here must match the WGSL shader code.
/// Notes:
/// - Most compute textures are declared non-filterable, since they are likely written
///   as storage textures and/or sampled as exact texel values.
/// - The final blit pass uses a filterable sampler + filterable texture sampling.
pub fn create_layouts(device: &wgpu::Device) -> Layouts {
    // Most passes in this file are compute passes; share the stage visibility constant.
    let cs_vis = wgpu::ShaderStages::COMPUTE;

    // Shared scene entries (group(0)) used by both `scene` and `primary`.
    //
    // These bindings must match your WGSL declarations, e.g.:
    //   @group(0) @binding(0) var<uniform> camera : ...
    //   @group(0) @binding(1) var<storage, read> chunks : ...
    // etc.
    let scene_entries: [wgpu::BindGroupLayoutEntry; 4] = [
        bgl_uniform(0, cs_vis),    // camera uniform
        bgl_storage_ro(1, cs_vis), // chunks meta (read-only)
        bgl_storage_ro(2, cs_vis), // nodes arena (read-only)
        bgl_storage_ro(3, cs_vis), // chunk grid (read-only)
    ];

    // group(0) SCENE:
    // Camera + buffers only.
    //
    // Godray pass uses this so it can bind depth/history textures in other groups
    // without also binding storage outputs that could cause read/write conflicts.
    let scene = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
        label: Some("scene_bgl"),
        entries: &scene_entries,
    });

    // group(0) PRIMARY:
    // Scene inputs + storage outputs for color/depth.
    //
    // This is the "main" compute pass that writes color and depth textures.
    let mut primary_entries = Vec::with_capacity(6);
    primary_entries.extend_from_slice(&scene_entries);

    // binding(4): color storage output (rgba16f)
    primary_entries.push(bgl_storage_tex_wo(
        4,
        cs_vis,
        wgpu::TextureFormat::Rgba16Float,
    ));

    // binding(5): depth storage output (r32f)
    primary_entries.push(bgl_storage_tex_wo(5, cs_vis, wgpu::TextureFormat::R32Float));

    let primary = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
        label: Some("primary_bgl"),
        entries: &primary_entries,
    });

    // group(1) GODRAY:
    // - depth sampled (r32f)
    // - history sampled (rgba16f)
    // - out storage (rgba16f)
    //
    // Sampling is marked filterable:false to match typical storage-written float textures.
    let godray = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
        label: Some("godray_bgl"),
        entries: &[
            // binding(0): depth texture (sampled)
            bgl_tex_sample(
                0,
                cs_vis,
                wgpu::TextureSampleType::Float { filterable: false },
            ),
            // binding(1): history texture (sampled)
            bgl_tex_sample(
                1,
                cs_vis,
                wgpu::TextureSampleType::Float { filterable: false },
            ),
            // binding(2): output storage texture (write-only)
            bgl_storage_tex_wo(2, cs_vis, wgpu::TextureFormat::Rgba16Float),
        ],
    });

    // group(2) COMPOSITE:
    // - color sampled
    // - godray sampled
    // - output storage (final offscreen output)
    let composite = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
        label: Some("composite_bgl"),
        entries: &[
            // binding(0): base color texture
            bgl_tex_sample(
                0,
                cs_vis,
                wgpu::TextureSampleType::Float { filterable: false },
            ),
            // binding(1): godray texture
            bgl_tex_sample(
                1,
                cs_vis,
                wgpu::TextureSampleType::Float { filterable: false },
            ),
            // binding(2): final output storage texture
            bgl_storage_tex_wo(2, cs_vis, wgpu::TextureFormat::Rgba16Float),
        ],
    });

    // Empty layout used as a placeholder bind group layout for group(0/1)
    // in the composite pipeline layout.
    let empty = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
        label: Some("empty_bgl"),
        entries: &[],
    });

    // Blit layout (render pass):
    // - sampled output texture (filterable, because we likely scale to the swapchain)
    // - filtering sampler
    // - overlay uniform (fragment stage)
    let blit = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
        label: Some("blit_bgl"),
        entries: &[
            // binding(0): output texture sampled in fragment shader
            bgl_tex_sample(
                0,
                wgpu::ShaderStages::FRAGMENT,
                wgpu::TextureSampleType::Float { filterable: true },
            ),
            // binding(1): sampler for output texture
            wgpu::BindGroupLayoutEntry {
                binding: 1,
                visibility: wgpu::ShaderStages::FRAGMENT,
                ty: wgpu::BindingType::Sampler(wgpu::SamplerBindingType::Filtering),
                count: None,
            },
            // binding(2): overlay uniform (fps, dimensions, etc.)
            bgl_uniform(2, wgpu::ShaderStages::FRAGMENT),
        ],
    });

    Layouts {
        primary,
        scene,
        godray,
        composite,
        empty,
        blit,
    }
}


// src/render/state/mod.rs
// -----------------------
// Multi-pass renderer built around compute shaders plus a final blit render pass.

mod bindgroups;
mod buffers;
mod layout;
mod pipelines;
pub mod textures;

use crate::{
    render::gpu_types::{CameraGpu, OverlayGpu},
    streaming::ChunkUpload,
};

use bindgroups::{create_bind_groups, BindGroups};
use buffers::{create_persistent_buffers, Buffers};
use layout::{create_layouts, Layouts};
use pipelines::{create_pipelines, Pipelines};
use textures::{create_textures, quarter_dim, TextureSet};

pub struct Renderer {
    device: wgpu::Device,
    queue: wgpu::Queue,

    sampler: wgpu::Sampler,

    layouts: Layouts,
    pipelines: Pipelines,
    buffers: Buffers,
    textures: TextureSet,
    bind_groups: BindGroups,

    ping: usize,
}

impl Renderer {
    pub async fn new(
        adapter: &wgpu::Adapter,
        surface_format: wgpu::TextureFormat,
        width: u32,
        height: u32,
    ) -> Self {
        let adapter_limits = adapter.limits();
        let required_limits = wgpu::Limits {
            max_storage_buffer_binding_size: adapter_limits.max_storage_buffer_binding_size,
            max_buffer_size: adapter_limits.max_buffer_size,
            ..wgpu::Limits::default()
        };

        let (device, queue) = adapter
            .request_device(
                &wgpu::DeviceDescriptor {
                    label: Some("device"),
                    required_features: wgpu::Features::empty(),
                    required_limits,
                },
                None,
            )
            .await
            .unwrap();

        let cs_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("ray_cs"),
            source: wgpu::ShaderSource::Wgsl(crate::render::shaders::ray_cs_wgsl().into()),
        });

        let fs_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("blit"),
            source: wgpu::ShaderSource::Wgsl(crate::render::shaders::blit_wgsl().into()),
        });

        let sampler = device.create_sampler(&wgpu::SamplerDescriptor {
            label: Some("nearest_sampler"),
            mag_filter: wgpu::FilterMode::Nearest,
            min_filter: wgpu::FilterMode::Nearest,
            mipmap_filter: wgpu::FilterMode::Nearest,
            ..Default::default()
        });

        let layouts = create_layouts(&device);
        let buffers = create_persistent_buffers(&device);

        let textures = create_textures(&device, width, height);

        let pipelines = create_pipelines(&device, &layouts, &cs_module, &fs_module, surface_format);

        let bind_groups = create_bind_groups(&device, &layouts, &buffers, &textures, &sampler);

        Self {
            device,
            queue,
            sampler,
            layouts,
            pipelines,
            buffers,
            textures,
            bind_groups,
            ping: 0,
        }
    }

    pub fn device(&self) -> &wgpu::Device {
        &self.device
    }

    pub fn queue(&self) -> &wgpu::Queue {
        &self.queue
    }

    pub fn resize_output(&mut self, width: u32, height: u32) {
        self.textures = create_textures(&self.device, width, height);
        self.bind_groups = create_bind_groups(
            &self.device,
            &self.layouts,
            &self.buffers,
            &self.textures,
            &self.sampler,
        );

        self.ping = 0;
    }

    pub fn write_chunk_grid(&self, grid: &[u32]) {
        let n = grid.len().min(self.buffers.grid_capacity as usize);
        self.queue.write_buffer(
            &self.buffers.chunk_grid,
            0,
            bytemuck::cast_slice(&grid[..n]),
        );
    }

    pub fn write_camera(&self, cam: &CameraGpu) {
        self.queue
            .write_buffer(&self.buffers.camera, 0, bytemuck::bytes_of(cam));
    }

    pub fn write_overlay(&self, ov: &OverlayGpu) {
        self.queue
            .write_buffer(&self.buffers.overlay, 0, bytemuck::bytes_of(ov));
    }

    // UPDATED: nodes are Arc<[NodeGpu]> so we can upload without cloning large Vecs.
    pub fn apply_chunk_uploads(&self, uploads: Vec<ChunkUpload>) {
        let node_stride = std::mem::size_of::<crate::render::gpu_types::NodeGpu>() as u64;
        let meta_stride = std::mem::size_of::<crate::render::gpu_types::ChunkMetaGpu>() as u64;

        for u in uploads {
            // Chunk metadata write (always present).
            if u.slot < self.buffers.chunk_capacity {
                let meta_off = (u.slot as u64) * meta_stride;
                self.queue
                    .write_buffer(&self.buffers.chunk, meta_off, bytemuck::bytes_of(&u.meta));
            }

            // Node payload write (only when nodes are included for this chunk).
            if !u.nodes.is_empty() {
                let needed = u.nodes.len() as u32;

                if u.node_base <= self.buffers.node_capacity
                    && u.node_base + needed <= self.buffers.node_capacity
                {
                    let node_off = (u.node_base as u64) * node_stride;
                    self.queue.write_buffer(
                        &self.buffers.node,
                        node_off,
                        bytemuck::cast_slice(u.nodes.as_ref()),
                    );
                }
            }
        }
    }

    pub fn encode_compute(&mut self, encoder: &mut wgpu::CommandEncoder, width: u32, height: u32) {
        {
            let mut cpass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("primary_pass"),
                timestamp_writes: None,
            });

            cpass.set_pipeline(&self.pipelines.primary);
            cpass.set_bind_group(0, &self.bind_groups.primary, &[]);

            let gx = (width + 7) / 8;
            let gy = (height + 7) / 8;
            cpass.dispatch_workgroups(gx, gy, 1);
        }

        let ping = self.ping;
        let pong = 1 - ping;

        {
            let qw = quarter_dim(width);
            let qh = quarter_dim(height);

            let mut cpass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("godray_pass"),
                timestamp_writes: None,
            });

            cpass.set_pipeline(&self.pipelines.godray);
            cpass.set_bind_group(0, &self.bind_groups.scene, &[]);
            cpass.set_bind_group(1, &self.bind_groups.godray[ping], &[]);

            let gx = (qw + 7) / 8;
            let gy = (qh + 7) / 8;
            cpass.dispatch_workgroups(gx, gy, 1);
        }

        {
            let mut cpass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("composite_pass"),
                timestamp_writes: None,
            });

            cpass.set_pipeline(&self.pipelines.composite);

            cpass.set_bind_group(0, &self.bind_groups.empty, &[]);
            cpass.set_bind_group(1, &self.bind_groups.empty, &[]);
            cpass.set_bind_group(2, &self.bind_groups.composite[pong], &[]);

            let gx = (width + 7) / 8;
            let gy = (height + 7) / 8;
            cpass.dispatch_workgroups(gx, gy, 1);
        }

        self.ping = pong;
    }

    pub fn encode_blit(&self, encoder: &mut wgpu::CommandEncoder, frame_view: &wgpu::TextureView) {
        let mut rpass = encoder.begin_render_pass(&wgpu::RenderPassDescriptor {
            label: Some("blit_pass"),
            color_attachments: &[Some(wgpu::RenderPassColorAttachment {
                view: frame_view,
                resolve_target: None,
                ops: wgpu::Operations {
                    load: wgpu::LoadOp::Clear(wgpu::Color::BLACK),
                    store: wgpu::StoreOp::Store,
                },
            })],
            depth_stencil_attachment: None,
            timestamp_writes: None,
            occlusion_query_set: None,
        });

        rpass.set_pipeline(&self.pipelines.blit);
        rpass.set_bind_group(0, &self.bind_groups.blit, &[]);
        rpass.draw(0..3, 0..1);
    }
}


// src/render/state/pipelines.rs
//
// Pipeline creation.
// This is intentionally isolated so the renderer logic (per-frame encoding) isn't
// buried under wgpu setup boilerplate.
//
// Terminology:
// - BindGroupLayout (BGL): describes what resources exist at @group/@binding.
// - PipelineLayout (PL): ordered list of BGLs for group(0), group(1), ...
// - Pipeline: compiled/validated shader entry point + fixed state + pipeline layout.
//
// Rule of thumb:
// The order of BGLs in `bind_group_layouts` must match the group indices used in WGSL.
// If a shader references @group(2), then the pipeline layout must include entries
// for group(0) and group(1) as well (even if they're "empty" placeholders).

use super::layout::Layouts;

pub struct Pipelines {
    /// Compute pipeline for the primary full-resolution pass (writes color/depth).
    pub primary: wgpu::ComputePipeline,

    /// Compute pipeline for the quarter-resolution godray pass (ping-pong temporal).
    pub godray: wgpu::ComputePipeline,

    /// Compute pipeline for the full-resolution composite pass (writes final output).
    pub composite: wgpu::ComputePipeline,

    /// Render pipeline for the final blit to the swapchain (fullscreen triangle).
    pub blit: wgpu::RenderPipeline,
}

/// Helper to build a compute pipeline with a specific entry point and bind group layout list.
///
/// `bgls` order defines the pipeline layout's group indices:
/// - bgls[0] => group(0)
/// - bgls[1] => group(1)
/// - ...
fn make_compute_pipeline(
    device: &wgpu::Device,
    label: &str,
    module: &wgpu::ShaderModule,
    entry: &str,
    bgls: &[&wgpu::BindGroupLayout],
) -> wgpu::ComputePipeline {
    // Create a pipeline layout named "{label}_pl" that fixes the bind group schema.
    let pl = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
        label: Some(&format!("{label}_pl")),
        bind_group_layouts: bgls,
        // No push constants used by these shaders.
        push_constant_ranges: &[],
    });

    // Create the compute pipeline referencing the WGSL entry point.
    device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
        label: Some(label),
        layout: Some(&pl),
        module,
        entry_point: entry,
        compilation_options: Default::default(),
    })
}

/// Create all pipelines (compute + blit).
///
/// Inputs:
/// - `cs_module`: WGSL module containing compute entry points.
/// - `fs_module`: WGSL module containing vertex/fragment entry points for blit.
/// - `surface_format`: swapchain format used for the final render target.
pub fn create_pipelines(
    device: &wgpu::Device,
    layouts: &Layouts,
    cs_module: &wgpu::ShaderModule,
    fs_module: &wgpu::ShaderModule,
    surface_format: wgpu::TextureFormat,
) -> Pipelines {
    // -------------------------------------------------------------------------
    // Compute pipelines
    // -------------------------------------------------------------------------

    // Primary pass:
    // Uses group(0) = layouts.primary, which includes:
    // - camera + scene buffers
    // - storage outputs for color/depth
    let primary = make_compute_pipeline(
        device,
        "primary_pipeline",
        cs_module,
        "main_primary",
        &[&layouts.primary],
    );

    // Godray pass:
    // Uses:
    //   group(0) = layouts.scene  (camera + scene buffers only)
    //   group(1) = layouts.godray (depth sample + history sample + out storage)
    let godray = make_compute_pipeline(
        device,
        "godray_pipeline",
        cs_module,
        "main_godray",
        &[&layouts.scene, &layouts.godray],
    );

    // Composite pass:
    // Shader reads from @group(2) (color + godray + output storage).
    // wgpu requires the pipeline layout to include group(0) and group(1) slots too,
    // so we provide empty placeholder layouts for those indices.
    let composite = make_compute_pipeline(
        device,
        "composite_pipeline",
        cs_module,
        "main_composite",
        &[&layouts.empty, &layouts.empty, &layouts.composite],
    );

    // -------------------------------------------------------------------------
    // Render pipeline: blit
    // -------------------------------------------------------------------------
    //
    // Full-screen triangle approach:
    // - No vertex buffers.
    // - Vertex shader generates positions from vertex_index.
    // - Fragment shader samples the renderer output texture.

    // Pipeline layout for blit uses a single bind group: group(0) = layouts.blit.
    let blit_pl = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
        label: Some("blit_pl"),
        bind_group_layouts: &[&layouts.blit],
        push_constant_ranges: &[],
    });

    // Render pipeline state:
    // - Targets the swapchain format.
    // - Uses REPLACE blending (overwrite framebuffer).
    // - Default primitive/multisample state is fine for a simple fullscreen draw.
    let blit = device.create_render_pipeline(&wgpu::RenderPipelineDescriptor {
        label: Some("blit_pipeline"),
        layout: Some(&blit_pl),
        vertex: wgpu::VertexState {
            module: fs_module,
            entry_point: "vs_main",
            // No vertex buffers; vertices are synthesized in the vertex shader.
            buffers: &[],
            compilation_options: Default::default(),
        },
        fragment: Some(wgpu::FragmentState {
            module: fs_module,
            entry_point: "fs_main",
            targets: &[Some(wgpu::ColorTargetState {
                format: surface_format,
                // Overwrite swapchain pixel with sampled color.
                blend: Some(wgpu::BlendState::REPLACE),
                write_mask: wgpu::ColorWrites::ALL,
            })],
            compilation_options: Default::default(),
        }),
        // Default triangle list, CCW front face, etc. (fullscreen triangle doesn't care much).
        primitive: wgpu::PrimitiveState::default(),
        depth_stencil: None,
        multisample: wgpu::MultisampleState::default(),
        multiview: None,
    });

    Pipelines {
        primary,
        godray,
        composite,
        blit,
    }
}

// src/render/state/textures.rs
//
// Texture creation and sizing policy for the renderer.
//
// Everything in here is "size-dependent": if the window/output resolution changes,
// these textures must be recreated, and any bind groups that reference their views
// must be rebuilt.
//
// Texture roles in the frame graph:
// - color_tex (full-res, RGBA16F): written by primary pass, sampled by composite.
// - depth_tex (full-res, R32F)   : written by primary pass, sampled by godray.
// - godray[A/B] (quarter-res, RGBA16F): temporal accumulation ping-pong.
// - output (full-res)           : written by composite, sampled by blit.
//
// Usage policy:
// - Intermediates are both STORAGE_BINDING and TEXTURE_BINDING so they can be
//   written as storage textures in compute and later sampled as regular textures.

use crate::render::resources::{create_output_texture, OutputTex};

/// Minimal wrapper around a 2D texture view.
///
/// Note: we only store the TextureView, not the Texture handle itself.
/// In wgpu, the view keeps the underlying texture alive as long as the view exists.
pub struct Tex2D {
    /// View used for binding (storage or sampled).
    pub view: wgpu::TextureView,
}

/// Bundle of all size-dependent textures used by the renderer.
pub struct TextureSet {
    // -------------------------------------------------------------------------
    // Final output
    // -------------------------------------------------------------------------

    /// Final output texture:
    /// - composite pass writes here (storage)
    /// - blit pass samples from it (filterable sampling in fragment shader)
    pub output: OutputTex,

    // -------------------------------------------------------------------------
    // Full-resolution intermediates
    // -------------------------------------------------------------------------

    /// Primary color accumulation buffer (RGBA16F).
    /// Written by primary compute pass.
    pub color: Tex2D,

    /// Primary depth buffer (R32F).
    /// Written by primary compute pass and sampled by godray.
    pub depth: Tex2D,

    // -------------------------------------------------------------------------
    // Quarter-resolution intermediates (temporal ping-pong)
    // -------------------------------------------------------------------------

    /// Godray accumulation textures (RGBA16F), quarter resolution.
    ///
    /// Index meaning:
    /// - 0 = A
    /// - 1 = B
    ///
    /// Each frame:
    /// - one is sampled as history
    /// - the other is written as the new result
    pub godray: [Tex2D; 2],
}

/// Compute the quarter-resolution dimension for an axis.
///
/// This uses ceil(x/4) so small sizes don't collapse to zero and so coverage remains
/// conservative (quarter-res still covers the full-res domain when upsampled).
pub fn quarter_dim(x: u32) -> u32 {
    // ceil(x / 4)
    (x + 3) / 4
}

/// Create a 2D texture and return only its view wrapper.
///
/// `usage` is passed through so the same helper can build both sampled-only or
/// read/write compute textures.
///
/// Zero-sized textures are invalid in wgpu, so width/height are clamped to >= 1.
fn make_tex2d(
    device: &wgpu::Device,
    label: &str,
    w: u32,
    h: u32,
    format: wgpu::TextureFormat,
    usage: wgpu::TextureUsages,
) -> Tex2D {
    // Avoid creating zero-sized textures (wgpu disallows it).
    let w = w.max(1);
    let h = h.max(1);

    // Allocate the texture backing storage.
    let tex = device.create_texture(&wgpu::TextureDescriptor {
        label: Some(label),
        size: wgpu::Extent3d {
            width: w,
            height: h,
            depth_or_array_layers: 1,
        },
        mip_level_count: 1,
        sample_count: 1,
        dimension: wgpu::TextureDimension::D2,
        format,
        usage,
        view_formats: &[],
    });

    // Default view (whole texture, base mip, 2D).
    let view = tex.create_view(&Default::default());

    Tex2D { view }
}

/// Create the full size-dependent texture set for the renderer.
///
/// Sizing policy:
/// - color/depth are full resolution (match output size)
/// - godray textures are quarter resolution (ceil(width/4), ceil(height/4))
/// - godray uses two textures for ping-pong temporal accumulation
///
/// Usage policy for intermediates:
/// - STORAGE_BINDING: compute passes write to them as storage textures
/// - TEXTURE_BINDING: later passes sample them as regular textures
pub fn create_textures(device: &wgpu::Device, width: u32, height: u32) -> TextureSet {
    // Clamp to avoid invalid zero-sized allocations during minimize/resizes.
    let width = width.max(1);
    let height = height.max(1);

    // Intermediates are written by compute (storage) and read by later passes (sampled).
    let rw_tex_usage =
        wgpu::TextureUsages::STORAGE_BINDING | wgpu::TextureUsages::TEXTURE_BINDING;

    // Final output texture used by composite -> blit.
    // (Implementation details live in render::resources.)
    let output = create_output_texture(device, width, height);

    // Full-res primary color buffer.
    let color = make_tex2d(
        device,
        "color_tex",
        width,
        height,
        wgpu::TextureFormat::Rgba16Float,
        rw_tex_usage,
    );

    // Full-res primary depth buffer.
    let depth = make_tex2d(
        device,
        "depth_tex",
        width,
        height,
        wgpu::TextureFormat::R32Float,
        rw_tex_usage,
    );

    // Quarter-res sizes for godrays.
    let qw = quarter_dim(width);
    let qh = quarter_dim(height);

    // Godray ping-pong textures (same format/usage; only roles swap per frame).
    let godray = [
        make_tex2d(
            device,
            "godray_a",
            qw,
            qh,
            wgpu::TextureFormat::Rgba16Float,
            rw_tex_usage,
        ),
        make_tex2d(
            device,
            "godray_b",
            qw,
            qh,
            wgpu::TextureFormat::Rgba16Float,
            rw_tex_usage,
        ),
    ];

    TextureSet {
        output,
        color,
        depth,
        godray,
    }
}


// src/shaders/common.wgsl
// -----------------------
// common.wgsl
//
// Shared WGSL across compute passes.
// - Constants / tuning knobs (ALL constants live here)
// - GPU-side struct defs + scene bindings
// - Shared math + grid helpers + sky/cloud/fog utilities

// ------------------------------------------------------------
// IDs / numeric
// ------------------------------------------------------------

const LEAF_U32 : u32 = 0xFFFFFFFFu; // Sentinel for "leaf node" in child_base.
const INVALID_U32 : u32 = 0xFFFFFFFFu; // Sentinel for invalid grid slot (chunk_grid entry).

const BIG_F32  : f32 = 1e30;
const EPS_INV  : f32 = 1e-8;

// Materials (keeps magic numbers out of core code)
const MAT_AIR   : u32 = 0u;
const MAT_GRASS : u32 = 1u;
const MAT_DIRT  : u32 = 2u;
const MAT_STONE : u32 = 3u;
const MAT_WOOD  : u32 = 4u;
const MAT_LEAF  : u32 = 5u;

// ------------------------------------------------------------
// Sun / sky
// ------------------------------------------------------------

const SUN_DIR : vec3<f32> = vec3<f32>(0.61237244, 0.5, 0.61237244);
const SUN_COLOR     : vec3<f32> = vec3<f32>(1.0, 0.98, 0.90);
const SUN_INTENSITY : f32 = 3.5;

const SUN_DISC_ANGULAR_RADIUS : f32 = 0.009;
const SUN_DISC_SOFTNESS       : f32 = 0.004;

const SKY_EXPOSURE : f32 = 0.40;

// ------------------------------------------------------------
// Shadows
// ------------------------------------------------------------

const SHADOW_BIAS : f32 = 2e-4;

// Shadow traversal tuning
const SHADOW_STEPS : u32 = 32u;

// If false: leaves cast shadows using their undisplaced cube (faster).
// If true : shadows match displaced leaf cubes (slower but consistent).
const SHADOW_DISPLACED_LEAVES : bool = false;

// Volumetric sun transmittance tuning (leafy canopy)
const VSM_STEPS : u32 = 24u;
const LEAF_LIGHT_TRANSMIT : f32 = 0.50;
const MIN_TRANS : f32 = 0.03;

// ------------------------------------------------------------
// Fog / volumetrics
// ------------------------------------------------------------

const FOG_HEIGHT_FALLOFF : f32 = 0.18;
const FOG_MAX_DIST       : f32 = 100.0;

const FOG_PRIMARY_SCALE : f32 = 0.02;
const FOG_GODRAY_SCALE  : f32 = 2.0;

const FOG_PRIMARY_VIS   : f32 = 0.08;

const FOG_COLOR_GROUND     : vec3<f32> = vec3<f32>(0.62, 0.64, 0.66);
const FOG_COLOR_SKY_BLEND  : f32 = 0.20;

const GODRAY_MAX_DIST    : f32 = 80.0;
const GODRAY_STRENGTH    : f32 = 4.0;

const GODRAY_OFFAXIS_POW : f32 = 3.0;
const GODRAY_OFFAXIS_W   : f32 = 0.18;

// Godray scattering height behavior (ONLY affects added beam light, not fog density)
const GODRAY_SCATTER_HEIGHT_FALLOFF : f32 = 0.04; // << smaller than FOG_HEIGHT_FALLOFF (0.18)
const GODRAY_SCATTER_MIN_FRAC       : f32 = 0.35; // floor as fraction of sea-level scatter

const GODRAY_SIDE_BOOST : f32 = 0.65; // 0..1
const GODRAY_BLACK_LEVEL : f32 = 0.018; // try 0.010..0.030

const GODRAY_TS_LP_ALPHA   : f32 = 0.50; // 0.2..0.5 (higher = smoother, less noisy)
const GODRAY_EDGE0         : f32 = 0.015;
const GODRAY_EDGE1         : f32 = 0.10;

const GODRAY_BASE_HAZE     : f32 = 0.08; // 0.02..0.10 (tiny DC term)
const GODRAY_HAZE_NEAR_FADE: f32 = 18.0; // meters: haze ramps in with distance


const INV_4PI      : f32 = 0.0795774715;
const PHASE_G      : f32 = 0.10;
const PHASE_MIE_W  : f32 = 0.25;

// ------------------------------------------------------------
// Fractal clouds
// ------------------------------------------------------------

const CLOUD_H : f32 = 200.0;
const CLOUD_UV_SCALE : f32 = 0.002;
const CLOUD_WIND : vec2<f32> = vec2<f32>(0.020, 0.012);

const CLOUD_COVERAGE : f32 = 0.45;
const CLOUD_SOFTNESS : f32 = 0.10;

const CLOUD_HORIZON_Y0 : f32 = 0.02;
const CLOUD_HORIZON_Y1 : f32 = 0.25;

const CLOUD_SKY_DARKEN : f32 = 0.95;
const CLOUD_ABSORB : f32 = 10.0;

const CLOUD_BASE_COL   : vec3<f32> = vec3<f32>(0.72, 0.74, 0.76);
const CLOUD_SILVER_POW : f32 = 8.0;
const CLOUD_SILVER_STR : f32 = 0.6;
const CLOUD_BLEND      : f32 = 0.85;

const CLOUD_DIM_SUN_DISC : bool = true;
const CLOUD_SUN_DISC_ABSORB_SCALE : f32 = 0.8;

// ------------------------------------------------------------
// Leaf wind (displaced cubes)
// ------------------------------------------------------------

const WIND_CELL_FREQ : f32 = 2.5;
const WIND_DIR_XZ : vec2<f32> = vec2<f32>(0.9, 0.4);

const WIND_RAMP_Y0 : f32 = 2.0;
const WIND_RAMP_Y1 : f32 = 14.0;

const WIND_GUST_TIME_FREQ    : f32 = 0.9;
const WIND_FLUTTER_TIME_FREQ : f32 = 4.2;

const WIND_GUST_XZ_FREQ    : vec2<f32> = vec2<f32>(0.35, 0.22);
const WIND_FLUTTER_XZ_FREQ : vec2<f32> = vec2<f32>(1.7,  1.1);

const WIND_GUST_WEIGHT    : f32 = 0.75;
const WIND_FLUTTER_WEIGHT : f32 = 0.25;

const WIND_VERTICAL_SCALE : f32 = 0.25;
const LEAF_VERTICAL_REDUCE : f32 = 0.15;

const LEAF_OFFSET_AMP : f32 = 0.45;
const LEAF_OFFSET_MAX_FRAC : f32 = 0.45;

const WIND_PHASE_OFF_1 : vec3<f32> = vec3<f32>(19.0, 7.0, 11.0);
const TAU : f32 = 6.28318530718;

// ------------------------------------------------------------
// Ray/main pass knobs (moved from ray_main)
// ------------------------------------------------------------

const PRIMARY_NUDGE_VOXEL_FRAC : f32 = 1e-4;

// Godray sampling pattern
const GODRAY_FRAME_FPS : f32 = 60.0;
const GODRAY_BLOCK_SIZE : i32 = 4;
const GODRAY_PATTERN_HASH_SCALE : f32 = 0.73;

const J0_SCALE : f32 = 1.31;
const J1_SCALE : f32 = 2.11;
const J2_SCALE : f32 = 3.01;
const J3_SCALE : f32 = 4.19;

const J0_F : vec2<f32> = vec2<f32>(0.11, 0.17);
const J1_F : vec2<f32> = vec2<f32>(0.23, 0.29);
const J2_F : vec2<f32> = vec2<f32>(0.37, 0.41);
const J3_F : vec2<f32> = vec2<f32>(0.53, 0.59);

const GODRAY_TV_CUTOFF : f32 = 0.02;
const GODRAY_STEPS_FAST : u32 = 8u;

// Composite
const COMPOSITE_SHARPEN : f32 = 0.35;
const COMPOSITE_GOD_SCALE : f32 = 3.0;
const COMPOSITE_BEAM_COMPRESS : bool = true;

// Post
const POST_EXPOSURE : f32 = 1.15;

// ------------------------------------------------------------
// GPU structs (must match Rust layouts)
// ------------------------------------------------------------

struct Node {
  child_base : u32,
  child_mask : u32,
  material   : u32,
  _pad       : u32,
};

struct Camera {
  view_inv    : mat4x4<f32>,
  proj_inv    : mat4x4<f32>,
  cam_pos     : vec4<f32>,

  chunk_size  : u32,
  chunk_count : u32,
  max_steps   : u32,
  _pad0       : u32,

  // x = voxel_size_m, y = time_seconds, z = wind_strength, w = fog_density
  voxel_params : vec4<f32>,

  grid_origin_chunk : vec4<i32>,
  grid_dims         : vec4<u32>,
};

struct ChunkMeta {
  origin     : vec4<i32>,
  node_base  : u32,
  node_count : u32,
  _pad0      : u32,
  _pad1      : u32,
};

// ------------------------------------------------------------
// Scene bindings (group(0))
// ------------------------------------------------------------

@group(0) @binding(0) var<uniform> cam : Camera;
@group(0) @binding(1) var<storage, read> chunks : array<ChunkMeta>;
@group(0) @binding(2) var<storage, read> nodes  : array<Node>;
@group(0) @binding(3) var<storage, read> chunk_grid : array<u32>;

// ------------------------------------------------------------
// Ray reconstruction
// ------------------------------------------------------------

fn ray_dir_from_pixel(px: vec2<f32>, res: vec2<f32>) -> vec3<f32> {
  let ndc = vec4<f32>(
    2.0 * px.x / res.x - 1.0,
    1.0 - 2.0 * px.y / res.y,
    1.0,
    1.0
  );

  let view = cam.proj_inv * ndc;
  let vdir = vec4<f32>(view.xyz / view.w, 0.0);
  let wdir = (cam.view_inv * vdir).xyz;
  return normalize(wdir);
}

// ------------------------------------------------------------
// AABB intersection (slab)
// ------------------------------------------------------------

fn intersect_aabb(ro: vec3<f32>, rd: vec3<f32>, bmin: vec3<f32>, bmax: vec3<f32>) -> vec2<f32> {
  let eps = 1e-8;

  var t_enter = -1e30;
  var t_exit  =  1e30;

  if (abs(rd.x) < eps) {
    if (ro.x < bmin.x || ro.x > bmax.x) { return vec2<f32>(1.0, 0.0); }
  } else {
    let inv = 1.0 / rd.x;
    let t0 = (bmin.x - ro.x) * inv;
    let t1 = (bmax.x - ro.x) * inv;
    t_enter = max(t_enter, min(t0, t1));
    t_exit  = min(t_exit,  max(t0, t1));
  }

  if (abs(rd.y) < eps) {
    if (ro.y < bmin.y || ro.y > bmax.y) { return vec2<f32>(1.0, 0.0); }
  } else {
    let inv = 1.0 / rd.y;
    let t0 = (bmin.y - ro.y) * inv;
    let t1 = (bmax.y - ro.y) * inv;
    t_enter = max(t_enter, min(t0, t1));
    t_exit  = min(t_exit,  max(t0, t1));
  }

  if (abs(rd.z) < eps) {
    if (ro.z < bmin.z || ro.z > bmax.z) { return vec2<f32>(1.0, 0.0); }
  } else {
    let inv = 1.0 / rd.z;
    let t0 = (bmin.z - ro.z) * inv;
    let t1 = (bmax.z - ro.z) * inv;
    t_enter = max(t_enter, min(t0, t1));
    t_exit  = min(t_exit,  max(t0, t1));
  }

  return vec2<f32>(t_enter, t_exit);
}

// ------------------------------------------------------------
// Sparse children addressing (compact child list)
// ------------------------------------------------------------

fn child_rank(mask: u32, ci: u32) -> u32 {
  let bit = 1u << ci;
  let lower = mask & (bit - 1u);
  return countOneBits(lower);
}

// ------------------------------------------------------------
// Chunk-grid helpers (moved from ray_main; used by shadows too)
// ------------------------------------------------------------

fn grid_lookup_slot(cx: i32, cy: i32, cz: i32) -> u32 {
  let ox = cam.grid_origin_chunk.x;
  let oy = cam.grid_origin_chunk.y;
  let oz = cam.grid_origin_chunk.z;

  let ix_i = cx - ox;
  let iy_i = cy - oy;
  let iz_i = cz - oz;

  if (ix_i < 0 || iy_i < 0 || iz_i < 0) { return INVALID_U32; }

  let nx = cam.grid_dims.x;
  let ny = cam.grid_dims.y;
  let nz = cam.grid_dims.z;

  let ix = u32(ix_i);
  let iy = u32(iy_i);
  let iz = u32(iz_i);

  if (ix >= nx || iy >= ny || iz >= nz) { return INVALID_U32; }

  let idx = (iz * ny * nx) + (iy * nx) + ix;
  return chunk_grid[idx];
}

fn chunk_coord_from_pos(p: vec3<f32>, chunk_size_m: f32) -> vec3<i32> {
  return vec3<i32>(
    i32(floor(p.x / chunk_size_m)),
    i32(floor(p.y / chunk_size_m)),
    i32(floor(p.z / chunk_size_m))
  );
}

// ------------------------------------------------------------
// Hash / noise / FBM (clouds)
// ------------------------------------------------------------

fn hash12(p: vec2<f32>) -> f32 {
  let h = dot(p, vec2<f32>(127.1, 311.7));
  return fract(sin(h) * 43758.5453);
}

fn hash21(p: vec2<f32>) -> f32 {
  let h = dot(p, vec2<f32>(127.1, 311.7));
  return fract(sin(h) * 43758.5453);
}

fn value_noise(p: vec2<f32>) -> f32 {
  let i = floor(p);
  let f = fract(p);

  let a = hash21(i);
  let b = hash21(i + vec2<f32>(1.0, 0.0));
  let c = hash21(i + vec2<f32>(0.0, 1.0));
  let d = hash21(i + vec2<f32>(1.0, 1.0));

  let u = f * f * (3.0 - 2.0 * f);

  let x1 = mix(a, b, u.x);
  let x2 = mix(c, d, u.x);
  return mix(x1, x2, u.y);
}

fn fbm(p: vec2<f32>) -> f32 {
  var x = p;
  var sum = 0.0;
  var amp = 0.5;

  let rot = mat2x2<f32>(0.8, -0.6, 0.6, 0.8);

  for (var i: u32 = 0u; i < 5u; i = i + 1u) {
    sum += amp * value_noise(x);
    x = rot * x * 2.0 + vec2<f32>(17.0, 9.0);
    amp *= 0.5;
  }
  return sum;
}

fn cloud_coverage_at_xz(xz: vec2<f32>, time_s: f32) -> f32 {
  var uv = xz * CLOUD_UV_SCALE + CLOUD_WIND * time_s;

  let n  = fbm(uv);
  let n2 = fbm(uv * 2.3 + vec2<f32>(13.2, 7.1));
  let field = 0.65 * n + 0.35 * n2;

  return smoothstep(CLOUD_COVERAGE, CLOUD_COVERAGE + CLOUD_SOFTNESS, field);
}

fn cloud_sun_transmittance(p: vec3<f32>, sun_dir: vec3<f32>) -> f32 {
  if (sun_dir.y <= 0.01) { return 1.0; }

  let t = (CLOUD_H - p.y) / sun_dir.y;
  if (t <= 0.0) { return 1.0; }

  let time_s = cam.voxel_params.y;
  let hit = p + sun_dir * t;
  let cov = cloud_coverage_at_xz(hit.xz, time_s);
  return exp(-CLOUD_ABSORB * cov);
}

// ------------------------------------------------------------
// Phase functions
// ------------------------------------------------------------

fn phase_mie(costh: f32) -> f32 {
  let g = PHASE_G;
  let gg = g * g;
  let denom = pow(1.0 + gg - 2.0 * g * costh, 1.5);
  return (1.0 - gg) / max(denom, 1e-3);
}

fn phase_blended(costh: f32) -> f32 {
  let mie = phase_mie(costh);
  return mix(INV_4PI, mie, PHASE_MIE_W);
}

// ------------------------------------------------------------
// Sky
// ------------------------------------------------------------

fn sky_color(rd: vec3<f32>) -> vec3<f32> {
  let tsky = clamp(0.5 * (rd.y + 1.0), 0.0, 1.0);
  var col = mix(
    vec3<f32>(0.05, 0.08, 0.12),
    vec3<f32>(0.55, 0.75, 0.95),
    tsky
  );

  col *= SKY_EXPOSURE;

  let mu  = dot(rd, SUN_DIR);
  let ang = acos(clamp(mu, -1.0, 1.0));
  let disc = 1.0 - smoothstep(
    SUN_DISC_ANGULAR_RADIUS,
    SUN_DISC_ANGULAR_RADIUS + SUN_DISC_SOFTNESS,
    ang
  );
  let halo = exp(-ang * 30.0) * 0.15;

  var cloud = 0.0;

  if (rd.y > 0.01) {
    let ro = cam.cam_pos.xyz;
    let t = (CLOUD_H - ro.y) / rd.y;

    if (t > 0.0) {
      let hit = ro + rd * t;
      let time_s = cam.voxel_params.y;

      cloud = cloud_coverage_at_xz(hit.xz, time_s);

      let horizon = clamp((rd.y - CLOUD_HORIZON_Y0) / CLOUD_HORIZON_Y1, 0.0, 1.0);
      cloud *= horizon;

      col *= mix(1.0, CLOUD_SKY_DARKEN, cloud);

      let toward_sun = clamp(mu, 0.0, 1.0);
      let silver = pow(toward_sun, CLOUD_SILVER_POW) * CLOUD_SILVER_STR;
      let cloud_col = mix(CLOUD_BASE_COL, vec3<f32>(1.0), silver);

      col = mix(col, cloud_col, cloud * CLOUD_BLEND);
    }
  }

  var sun_term = (disc + halo);
  if (CLOUD_DIM_SUN_DISC) {
    let Tc_view = exp(-CLOUD_ABSORB * cloud * CLOUD_SUN_DISC_ABSORB_SCALE);
    sun_term *= Tc_view;
  }

  col += SUN_COLOR * SUN_INTENSITY * sun_term;
  return col;
}

// Fog color used by primary composition
fn fog_color(rd: vec3<f32>) -> vec3<f32> {
  let up = clamp(rd.y * 0.5 + 0.5, 0.0, 1.0);
  let sky = sky_color(rd);
  return mix(FOG_COLOR_GROUND, sky, FOG_COLOR_SKY_BLEND * up);
}

// ------------------------------------------------------------
// Fog helpers
// ------------------------------------------------------------

fn fog_density_primary() -> f32 {
  return max(cam.voxel_params.w * FOG_PRIMARY_SCALE, 0.0);
}

fn fog_density_godray() -> f32 {
  return max(cam.voxel_params.w * FOG_GODRAY_SCALE, 0.0);
}

fn fog_optical_depth_with_base(base: f32, ro: vec3<f32>, rd: vec3<f32>, t: f32) -> f32 {
  if (base <= 0.0) { return 0.0; }

  let k = FOG_HEIGHT_FALLOFF;
  let y0 = ro.y;
  let dy = rd.y;

  if (abs(dy) < 1e-4) {
    return base * exp(-k * y0) * t;
  }

  let a = exp(-k * y0);
  let b = exp(-k * (y0 + dy * t));
  return base * (a - b) / (k * dy);
}

fn fog_transmittance_primary(ro: vec3<f32>, rd: vec3<f32>, t: f32) -> f32 {
  let od = max(fog_optical_depth_with_base(fog_density_primary(), ro, rd, t), 0.0);
  return exp(-od);
}

fn fog_transmittance_godray(ro: vec3<f32>, rd: vec3<f32>, t: f32) -> f32 {
  let od = max(fog_optical_depth_with_base(fog_density_godray(), ro, rd, t), 0.0);
  return exp(-od);
}

// src/shaders/ray_core.wgsl
// -------------------------
// ray_core.wgsl
//
// Consolidated core:
// - SVO queries + hybrid traversal
// - Leaf wind + displaced hit
// - Shadows + sun transmittance
// - Material palette + shading

// Depends on: common.wgsl (constants, structs, bindings, helpers)

fn safe_inv(x: f32) -> f32 {
  return select(1.0 / x, BIG_F32, abs(x) < EPS_INV);
}

// ------------------------------------------------------------
// Leaf query: point -> leaf cell in SVO
// ------------------------------------------------------------

struct LeafQuery {
  bmin : vec3<f32>,
  size : f32,
  mat  : u32,
};

fn query_leaf_at(
  p_in: vec3<f32>,
  root_bmin: vec3<f32>,
  root_size: f32,
  node_base: u32
) -> LeafQuery {
  var idx: u32 = node_base;
  var bmin: vec3<f32> = root_bmin;
  var size: f32 = root_size;

  let min_leaf: f32 = cam.voxel_params.x;
  var p = p_in;

  for (var d: u32 = 0u; d < 32u; d = d + 1u) {
    let n = nodes[idx];

    if (n.child_base == LEAF_U32) {
      return LeafQuery(bmin, size, n.material);
    }

    if (size <= min_leaf) {
      return LeafQuery(bmin, size, MAT_AIR);
    }

    let half = size * 0.5;
    let mid  = bmin + vec3<f32>(half);

    let e = 1e-6 * size;

    let hx = select(0u, 1u, p.x > mid.x + e);
    let hy = select(0u, 1u, p.y > mid.y + e);
    let hz = select(0u, 1u, p.z > mid.z + e);
    let ci = hx | (hy << 1u) | (hz << 2u);

    let child_bmin = bmin + vec3<f32>(
      select(0.0, half, hx != 0u),
      select(0.0, half, hy != 0u),
      select(0.0, half, hz != 0u)
    );

    let bit = 1u << ci;
    if ((n.child_mask & bit) == 0u) {
      return LeafQuery(child_bmin, half, MAT_AIR);
    }

    let rank = child_rank(n.child_mask, ci);
    idx = node_base + (n.child_base + rank);

    bmin = child_bmin;
    size = half;
  }

  return LeafQuery(bmin, size, MAT_AIR);
}

// ------------------------------------------------------------
// Fast stepping: cube exit time with inv dir
// ------------------------------------------------------------

fn exit_time_from_cube_inv(
  ro: vec3<f32>,
  rd: vec3<f32>,
  inv: vec3<f32>,
  bmin: vec3<f32>,
  size: f32
) -> f32 {
  let bmax = bmin + vec3<f32>(size);

  let tx = (select(bmin.x, bmax.x, rd.x > 0.0) - ro.x) * inv.x;
  let ty = (select(bmin.y, bmax.y, rd.y > 0.0) - ro.y) * inv.y;
  let tz = (select(bmin.z, bmax.z, rd.z > 0.0) - ro.z) * inv.z;

  return min(tx, min(ty, tz));
}

// ------------------------------------------------------------
// AABB hit + stable normal selection
// ------------------------------------------------------------

struct BoxHit {
  hit : bool,
  t   : f32,
  n   : vec3<f32>,
};

fn aabb_hit_normal_inv(
  ro: vec3<f32>,
  rd: vec3<f32>,
  inv: vec3<f32>,
  bmin: vec3<f32>,
  size: f32,
  t_min: f32,
  t_max: f32
) -> BoxHit {
  let bmax = bmin + vec3<f32>(size);

  let tx0 = (bmin.x - ro.x) * inv.x;
  let tx1 = (bmax.x - ro.x) * inv.x;
  let ty0 = (bmin.y - ro.y) * inv.y;
  let ty1 = (bmax.y - ro.y) * inv.y;
  let tz0 = (bmin.z - ro.z) * inv.z;
  let tz1 = (bmax.z - ro.z) * inv.z;

  let tminx = min(tx0, tx1);
  let tmaxx = max(tx0, tx1);
  let tminy = min(ty0, ty1);
  let tmaxy = max(ty0, ty1);
  let tminz = min(tz0, tz1);
  let tmaxz = max(tz0, tz1);

  let t_enter = max(tminx, max(tminy, tminz));
  let t_exit  = min(tmaxx, min(tmaxy, tmaxz));

  let t0 = max(t_enter, t_min);

  if (t_exit < t0 || t0 > t_max) {
    return BoxHit(false, BIG_F32, vec3<f32>(0.0));
  }

  let eps = 1e-6 * size;

  var best_abs = -1.0;
  var pick: u32 = 0u;

  if (abs(t_enter - tminx) <= eps) {
    let a = abs(rd.x);
    if (a > best_abs) { best_abs = a; pick = 0u; }
  }
  if (abs(t_enter - tminy) <= eps) {
    let a = abs(rd.y);
    if (a > best_abs) { best_abs = a; pick = 1u; }
  }
  if (abs(t_enter - tminz) <= eps) {
    let a = abs(rd.z);
    if (a > best_abs) { best_abs = a; pick = 2u; }
  }

  var n = vec3<f32>(0.0);
  if (pick == 0u) { n = vec3<f32>(select( 1.0, -1.0, rd.x > 0.0), 0.0, 0.0); }
  if (pick == 1u) { n = vec3<f32>(0.0, select( 1.0, -1.0, rd.y > 0.0), 0.0); }
  if (pick == 2u) { n = vec3<f32>(0.0, 0.0, select( 1.0, -1.0, rd.z > 0.0)); }

  return BoxHit(true, t0, n);
}

// ------------------------------------------------------------
// Leaf wind field + displaced cube hit
// ------------------------------------------------------------

fn hash1(p: vec3<f32>) -> f32 {
  let h = dot(p, vec3<f32>(127.1, 311.7, 74.7));
  return fract(sin(h) * 43758.5453);
}

fn wind_field(pos_m: vec3<f32>, t: f32) -> vec3<f32> {
  let cell = floor(pos_m * WIND_CELL_FREQ);

  let ph0 = hash1(cell);
  let ph1 = hash1(cell + WIND_PHASE_OFF_1);

  let dir = normalize(WIND_DIR_XZ);

  let h = clamp((pos_m.y - WIND_RAMP_Y0) / max(WIND_RAMP_Y1 - WIND_RAMP_Y0, 1e-3), 0.0, 1.0);

  let gust = sin(
    t * WIND_GUST_TIME_FREQ +
    dot(pos_m.xz, WIND_GUST_XZ_FREQ) +
    ph0 * TAU
  );

  let flutter = sin(
    t * WIND_FLUTTER_TIME_FREQ +
    dot(pos_m.xz, WIND_FLUTTER_XZ_FREQ) +
    ph1 * TAU
  );

  let xz = dir * (WIND_GUST_WEIGHT * gust + WIND_FLUTTER_WEIGHT * flutter) * h;
  let y  = WIND_VERTICAL_SCALE * flutter * h;

  return vec3<f32>(xz.x, y, xz.y);
}

fn clamp_len(v: vec3<f32>, max_len: f32) -> vec3<f32> {
  let l2 = dot(v, v);
  if (l2 <= max_len * max_len) { return v; }
  return v * (max_len / sqrt(l2));
}

fn leaf_cube_offset(bmin: vec3<f32>, size: f32, time_s: f32, strength: f32) -> vec3<f32> {
  let center = bmin + vec3<f32>(0.5 * size);

  var w = wind_field(center, time_s) * strength;
  w = vec3<f32>(w.x, LEAF_VERTICAL_REDUCE * w.y, w.z);

  let amp = LEAF_OFFSET_AMP * size;
  return clamp_len(w * amp, LEAF_OFFSET_MAX_FRAC * size);
}

struct LeafCubeHit {
  hit  : bool,
  t    : f32,
  n    : vec3<f32>,
};

fn leaf_displaced_cube_hit(
  ro: vec3<f32>,
  rd: vec3<f32>,
  bmin: vec3<f32>,
  size: f32,
  time_s: f32,
  strength: f32,
  t_min: f32,
  t_max: f32
) -> LeafCubeHit {
  let off   = leaf_cube_offset(bmin, size, time_s, strength);
  let bmin2 = bmin + off;

  let inv = vec3<f32>(safe_inv(rd.x), safe_inv(rd.y), safe_inv(rd.z));
  let bh  = aabb_hit_normal_inv(ro, rd, inv, bmin2, size, t_min, t_max);

  return LeafCubeHit(bh.hit, bh.t, bh.n);
}

// ------------------------------------------------------------
// Chunk tracing: hybrid point-query + interval stepping
// ------------------------------------------------------------

struct HitGeom {
  hit : bool,
  t   : f32,
  mat : u32,
  n   : vec3<f32>,
};

fn trace_chunk_hybrid_interval(
  ro: vec3<f32>,
  rd: vec3<f32>,
  ch: ChunkMeta,
  t_enter: f32,
  t_exit: f32
) -> HitGeom {
  let voxel_size = cam.voxel_params.x;

  let root_bmin_vox = vec3<f32>(f32(ch.origin.x), f32(ch.origin.y), f32(ch.origin.z));
  let root_bmin = root_bmin_vox * voxel_size;
  let root_size = f32(cam.chunk_size) * voxel_size;

  let eps_step = 1e-4 * voxel_size;

  var tcur = max(t_enter, 0.0) + eps_step;

  let inv = vec3<f32>(safe_inv(rd.x), safe_inv(rd.y), safe_inv(rd.z));

  for (var step_i: u32 = 0u; step_i < cam.max_steps; step_i = step_i + 1u) {
    if (tcur > t_exit) { break; }

    let p  = ro + tcur * rd;
    let pq = p + rd * (1e-4 * voxel_size);

    let q = query_leaf_at(pq, root_bmin, root_size, ch.node_base);

    if (q.mat != MAT_AIR) {
      if (q.mat == MAT_LEAF) {
        let time_s   = cam.voxel_params.y;
        let strength = cam.voxel_params.z;

        let h2 = leaf_displaced_cube_hit(
          ro, rd,
          q.bmin, q.size,
          time_s, strength,
          t_enter,
          t_exit
        );

        if (h2.hit) {
          return HitGeom(true, h2.t, MAT_LEAF, h2.n);
        }

        let t_leave = exit_time_from_cube_inv(ro, rd, inv, q.bmin, q.size);
        tcur = max(t_leave, tcur) + eps_step;
        continue;
      }

      let bh = aabb_hit_normal_inv(
        ro, rd, inv,
        q.bmin, q.size,
        t_enter,
        t_exit
      );

      if (bh.hit) {
        return HitGeom(true, bh.t, q.mat, bh.n);
      }

      let t_leave = exit_time_from_cube_inv(ro, rd, inv, q.bmin, q.size);
      tcur = max(t_leave, tcur) + eps_step;
      continue;
    }

    let t_leave = exit_time_from_cube_inv(ro, rd, inv, q.bmin, q.size);
    tcur = max(t_leave, tcur) + eps_step;
  }

  return HitGeom(false, BIG_F32, MAT_AIR, vec3<f32>(0.0));
}

// ------------------------------------------------------------
// Shadow traversal
// ------------------------------------------------------------

fn trace_chunk_shadow_interval(
  ro: vec3<f32>,
  rd: vec3<f32>,
  ch: ChunkMeta,
  t_enter: f32,
  t_exit: f32
) -> bool {
  let voxel_size = cam.voxel_params.x;
  let nudge_s = 0.18 * voxel_size;

  let root_bmin_vox = vec3<f32>(f32(ch.origin.x), f32(ch.origin.y), f32(ch.origin.z));
  let root_bmin = root_bmin_vox * voxel_size;
  let root_size = f32(cam.chunk_size) * voxel_size;

  var tcur = max(t_enter, 0.0) + nudge_s;
  let inv = vec3<f32>(safe_inv(rd.x), safe_inv(rd.y), safe_inv(rd.z));

  for (var step_i: u32 = 0u; step_i < SHADOW_STEPS; step_i = step_i + 1u) {
    if (tcur > t_exit) { break; }

    let p = ro + tcur * rd;
    let q = query_leaf_at(p, root_bmin, root_size, ch.node_base);

    if (q.mat != MAT_AIR) {
      if (q.mat == MAT_LEAF) {
        if (!SHADOW_DISPLACED_LEAVES) {
          return true;
        }

        let time_s   = cam.voxel_params.y;
        let strength = cam.voxel_params.z;

        let h2 = leaf_displaced_cube_hit(
          ro, rd,
          q.bmin, q.size,
          time_s, strength,
          tcur - nudge_s,
          t_exit
        );

        if (h2.hit) { return true; }

        let t_leave = exit_time_from_cube_inv(ro, rd, inv, q.bmin, q.size);
        tcur = max(t_leave, tcur) + nudge_s;
        continue;
      }

      return true;
    }

    let t_leave = exit_time_from_cube_inv(ro, rd, inv, q.bmin, q.size);
    tcur = max(t_leave, tcur) + nudge_s;
  }

  return false;
}

fn in_shadow(p: vec3<f32>, sun_dir: vec3<f32>) -> bool {
  let voxel_size   = cam.voxel_params.x;
  let nudge_s      = 0.18 * voxel_size;
  let chunk_size_m = f32(cam.chunk_size) * voxel_size;

  let go = cam.grid_origin_chunk;
  let gd = cam.grid_dims;

  let grid_bmin = vec3<f32>(
    f32(go.x) * chunk_size_m,
    f32(go.y) * chunk_size_m,
    f32(go.z) * chunk_size_m
  );

  let grid_bmax = grid_bmin + vec3<f32>(
    f32(gd.x) * chunk_size_m,
    f32(gd.y) * chunk_size_m,
    f32(gd.z) * chunk_size_m
  );

  let bias = max(SHADOW_BIAS, 0.50 * voxel_size);
  let ro   = p + sun_dir * bias;
  let rd   = sun_dir;

  let rtg = intersect_aabb(ro, rd, grid_bmin, grid_bmax);
  let t_enter = max(rtg.x, 0.0);
  let t_exit  = rtg.y;
  if (t_exit < t_enter) { return false; }

  let start_t = t_enter + nudge_s;
  let p0 = ro + start_t * rd;

  var t_local: f32 = 0.0;
  let t_exit_local = max(t_exit - start_t, 0.0);

  var c = chunk_coord_from_pos(p0, chunk_size_m);
  var cx: i32 = c.x;
  var cy: i32 = c.y;
  var cz: i32 = c.z;

  let inv = vec3<f32>(safe_inv(rd.x), safe_inv(rd.y), safe_inv(rd.z));

  let step_x: i32 = select(-1, 1, rd.x > 0.0);
  let step_y: i32 = select(-1, 1, rd.y > 0.0);
  let step_z: i32 = select(-1, 1, rd.z > 0.0);

  let bx = select(f32(cx) * chunk_size_m, f32(cx + 1) * chunk_size_m, rd.x > 0.0);
  let by = select(f32(cy) * chunk_size_m, f32(cy + 1) * chunk_size_m, rd.y > 0.0);
  let bz = select(f32(cz) * chunk_size_m, f32(cz + 1) * chunk_size_m, rd.z > 0.0);

  var tMaxX: f32 = (bx - p0.x) * inv.x;
  var tMaxY: f32 = (by - p0.y) * inv.y;
  var tMaxZ: f32 = (bz - p0.z) * inv.z;

  let tDeltaX: f32 = abs(chunk_size_m * inv.x);
  let tDeltaY: f32 = abs(chunk_size_m * inv.y);
  let tDeltaZ: f32 = abs(chunk_size_m * inv.z);

  if (abs(rd.x) < EPS_INV) { tMaxX = BIG_F32; }
  if (abs(rd.y) < EPS_INV) { tMaxY = BIG_F32; }
  if (abs(rd.z) < EPS_INV) { tMaxZ = BIG_F32; }

  let max_chunk_steps = min((gd.x + gd.y + gd.z) * 6u + 8u, 1024u);

  for (var s: u32 = 0u; s < max_chunk_steps; s = s + 1u) {
    if (t_local > t_exit_local) { break; }

    let tNextLocal = min(tMaxX, min(tMaxY, tMaxZ));

    let slot = grid_lookup_slot(cx, cy, cz);
    if (slot != INVALID_U32 && slot < cam.chunk_count) {
      let ch = chunks[slot];

      let cell_enter = start_t + t_local;
      let cell_exit  = start_t + min(tNextLocal, t_exit_local);

      if (trace_chunk_shadow_interval(ro, rd, ch, cell_enter, cell_exit)) {
        return true;
      }
    }

    if (tMaxX < tMaxY) {
      if (tMaxX < tMaxZ) { cx += step_x; t_local = tMaxX; tMaxX += tDeltaX; }
      else               { cz += step_z; t_local = tMaxZ; tMaxZ += tDeltaZ; }
    } else {
      if (tMaxY < tMaxZ) { cy += step_y; t_local = tMaxY; tMaxY += tDeltaY; }
      else               { cz += step_z; t_local = tMaxZ; tMaxZ += tDeltaZ; }
    }

    let ox = cam.grid_origin_chunk.x;
    let oy = cam.grid_origin_chunk.y;
    let oz = cam.grid_origin_chunk.z;

    let nx = i32(cam.grid_dims.x);
    let ny = i32(cam.grid_dims.y);
    let nz = i32(cam.grid_dims.z);

    if (cx < ox || cy < oy || cz < oz || cx >= ox + nx || cy >= oy + ny || cz >= oz + nz) {
      break;
    }
  }

  return false;
}

fn trace_chunk_shadow_trans_interval(
  ro: vec3<f32>,
  rd: vec3<f32>,
  ch: ChunkMeta,
  t_enter: f32,
  t_exit: f32
) -> f32 {
  let voxel_size = cam.voxel_params.x;
  let nudge_s = 0.18 * voxel_size;

  let root_bmin_vox = vec3<f32>(f32(ch.origin.x), f32(ch.origin.y), f32(ch.origin.z));
  let root_bmin = root_bmin_vox * voxel_size;
  let root_size = f32(cam.chunk_size) * voxel_size;

  var tcur = max(t_enter, 0.0) + nudge_s;
  let inv = vec3<f32>(safe_inv(rd.x), safe_inv(rd.y), safe_inv(rd.z));

  var trans = 1.0;

  for (var step_i: u32 = 0u; step_i < VSM_STEPS; step_i = step_i + 1u) {
    if (tcur > t_exit) { break; }
    if (trans < MIN_TRANS) { break; }

    let p = ro + tcur * rd;
    let qeps = 1e-4 * cam.voxel_params.x;
    let pq   = p + rd * qeps;

    let q = query_leaf_at(pq, root_bmin, root_size, ch.node_base);

    if (q.mat != MAT_AIR) {
      if (q.mat == MAT_LEAF) {
        trans *= LEAF_LIGHT_TRANSMIT;
        let t_leave = exit_time_from_cube_inv(ro, rd, inv, q.bmin, q.size);
        tcur = max(t_leave, tcur) + nudge_s;
        continue;
      }
      return 0.0;
    }

    let t_leave = exit_time_from_cube_inv(ro, rd, inv, q.bmin, q.size);
    tcur = max(t_leave, tcur) + nudge_s;
  }

  return trans;
}

fn sun_transmittance(p: vec3<f32>, sun_dir: vec3<f32>) -> f32 {
  let Tc = cloud_sun_transmittance(p, sun_dir);

  let voxel_size   = cam.voxel_params.x;
  let nudge_s      = 0.18 * voxel_size;
  let chunk_size_m = f32(cam.chunk_size) * voxel_size;

  let go = cam.grid_origin_chunk;
  let gd = cam.grid_dims;

  let grid_bmin = vec3<f32>(
    f32(go.x) * chunk_size_m,
    f32(go.y) * chunk_size_m,
    f32(go.z) * chunk_size_m
  );

  let grid_bmax = grid_bmin + vec3<f32>(
    f32(gd.x) * chunk_size_m,
    f32(gd.y) * chunk_size_m,
    f32(gd.z) * chunk_size_m
  );

  let bias = max(SHADOW_BIAS, 0.50 * voxel_size);
  let ro   = p + sun_dir * bias;
  let rd   = sun_dir;

  let rtg = intersect_aabb(ro, rd, grid_bmin, grid_bmax);
  let t_enter = max(rtg.x, 0.0);
  let t_exit  = rtg.y;
  if (t_exit < t_enter) { return Tc; }

  let start_t = t_enter + nudge_s;
  let p0 = ro + start_t * rd;

  var t_local: f32 = 0.0;
  let t_exit_local = max(t_exit - start_t, 0.0);

  var c = chunk_coord_from_pos(p0, chunk_size_m);
  var cx: i32 = c.x;
  var cy: i32 = c.y;
  var cz: i32 = c.z;

  let inv = vec3<f32>(safe_inv(rd.x), safe_inv(rd.y), safe_inv(rd.z));

  let step_x: i32 = select(-1, 1, rd.x > 0.0);
  let step_y: i32 = select(-1, 1, rd.y > 0.0);
  let step_z: i32 = select(-1, 1, rd.z > 0.0);

  let bx = select(f32(cx) * chunk_size_m, f32(cx + 1) * chunk_size_m, rd.x > 0.0);
  let by = select(f32(cy) * chunk_size_m, f32(cy + 1) * chunk_size_m, rd.y > 0.0);
  let bz = select(f32(cz) * chunk_size_m, f32(cz + 1) * chunk_size_m, rd.z > 0.0);

  var tMaxX: f32 = (bx - p0.x) * inv.x;
  var tMaxY: f32 = (by - p0.y) * inv.y;
  var tMaxZ: f32 = (bz - p0.z) * inv.z;

  let tDeltaX: f32 = abs(chunk_size_m * inv.x);
  let tDeltaY: f32 = abs(chunk_size_m * inv.y);
  let tDeltaZ: f32 = abs(chunk_size_m * inv.z);

  if (abs(rd.x) < EPS_INV) { tMaxX = BIG_F32; }
  if (abs(rd.y) < EPS_INV) { tMaxY = BIG_F32; }
  if (abs(rd.z) < EPS_INV) { tMaxZ = BIG_F32; }

  var trans = 1.0;

  let max_chunk_steps = min((gd.x + gd.y + gd.z) * 6u + 8u, 512u);

  for (var s: u32 = 0u; s < max_chunk_steps; s = s + 1u) {
    if (t_local > t_exit_local) { break; }
    if (trans < MIN_TRANS) { break; }

    let tNextLocal = min(tMaxX, min(tMaxY, tMaxZ));
    let slot = grid_lookup_slot(cx, cy, cz);

    if (slot != INVALID_U32 && slot < cam.chunk_count) {
      let ch = chunks[slot];

      let cell_enter = start_t + t_local;
      let cell_exit  = start_t + min(tNextLocal, t_exit_local);

      trans *= trace_chunk_shadow_trans_interval(ro, rd, ch, cell_enter, cell_exit);
      if (trans < MIN_TRANS) { break; }
    }

    if (tMaxX < tMaxY) {
      if (tMaxX < tMaxZ) { cx += step_x; t_local = tMaxX; tMaxX += tDeltaX; }
      else               { cz += step_z; t_local = tMaxZ; tMaxZ += tDeltaZ; }
    } else {
      if (tMaxY < tMaxZ) { cy += step_y; t_local = tMaxY; tMaxY += tDeltaY; }
      else               { cz += step_z; t_local = tMaxZ; tMaxZ += tDeltaZ; }
    }

    let ox = cam.grid_origin_chunk.x;
    let oy = cam.grid_origin_chunk.y;
    let oz = cam.grid_origin_chunk.z;

    let nx = i32(cam.grid_dims.x);
    let ny = i32(cam.grid_dims.y);
    let nz = i32(cam.grid_dims.z);

    if (cx < ox || cy < oy || cz < oz || cx >= ox + nx || cy >= oy + ny || cz >= oz + nz) {
      break;
    }
  }

  return trans * Tc;
}

// ------------------------------------------------------------
// Shading
// ------------------------------------------------------------

fn color_for_material(m: u32) -> vec3<f32> {
  if (m == MAT_AIR)   { return vec3<f32>(0.0); }

  if (m == MAT_GRASS) { return vec3<f32>(0.18, 0.75, 0.18); }
  if (m == MAT_DIRT)  { return vec3<f32>(0.45, 0.30, 0.15); }
  if (m == MAT_STONE) { return vec3<f32>(0.50, 0.50, 0.55); }
  if (m == MAT_WOOD)  { return vec3<f32>(0.38, 0.26, 0.14); }
  if (m == MAT_LEAF)  { return vec3<f32>(0.10, 0.55, 0.12); }

  return vec3<f32>(1.0, 0.0, 1.0);
}

fn shade_hit(ro: vec3<f32>, rd: vec3<f32>, hg: HitGeom) -> vec3<f32> {
  let hp = ro + hg.t * rd;
  let base = color_for_material(hg.mat);

  let voxel_size = cam.voxel_params.x;
  let hp_shadow  = hp + hg.n * (0.75 * voxel_size);

  let shadow = select(1.0, 0.0, in_shadow(hp_shadow, SUN_DIR));
  let cloud = cloud_sun_transmittance(hp_shadow, SUN_DIR);

  let diff = max(dot(hg.n, SUN_DIR), 0.0);

  let ambient = select(0.22, 0.28, hg.mat == MAT_LEAF);

  var dapple = 1.0;
  if (hg.mat == MAT_LEAF) {
    let time_s = cam.voxel_params.y;
    let d0 = sin(dot(hp.xz, vec2<f32>(3.0, 2.2)) + time_s * 3.5);
    let d1 = sin(dot(hp.xz, vec2<f32>(6.5, 4.1)) - time_s * 6.0);
    dapple = 0.90 + 0.10 * (0.6 * d0 + 0.4 * d1);
  }

  let direct = SUN_COLOR * SUN_INTENSITY * diff * shadow * cloud;
  return base * (ambient + (1.0 - ambient) * direct) * dapple;
}

// src/shaders/ray_main.wgsl
// -------------------------
// ray_main.wgsl
//
// Compute entrypoints only.
// Depends on: common.wgsl + ray_core.wgsl

@group(0) @binding(4) var color_img : texture_storage_2d<rgba16float, write>;
@group(0) @binding(5) var depth_img : texture_storage_2d<r32float, write>;

@group(1) @binding(0) var depth_tex       : texture_2d<f32>;
@group(1) @binding(1) var godray_hist_tex : texture_2d<f32>;
@group(1) @binding(2) var godray_out      : texture_storage_2d<rgba16float, write>;

@group(2) @binding(0) var color_tex  : texture_2d<f32>;
@group(2) @binding(1) var godray_tex : texture_2d<f32>;
@group(2) @binding(2) var out_img    : texture_storage_2d<rgba16float, write>;

fn tonemap_exp(hdr: vec3<f32>) -> vec3<f32> {
  return vec3<f32>(1.0) - exp(-hdr * POST_EXPOSURE);
}

// Quarter-res upsample (manual bilerp)
fn godray_sample_bilerp(px_full: vec2<f32>) -> vec3<f32> {
  let q = px_full * 0.25;
  let q0 = vec2<i32>(i32(floor(q.x)), i32(floor(q.y)));
  let f  = fract(q);

  let qdims = textureDimensions(godray_tex);
  let x0 = clamp(q0.x, 0, i32(qdims.x) - 1);
  let y0 = clamp(q0.y, 0, i32(qdims.y) - 1);
  let x1 = min(x0 + 1, i32(qdims.x) - 1);
  let y1 = min(y0 + 1, i32(qdims.y) - 1);

  let c00 = textureLoad(godray_tex, vec2<i32>(x0, y0), 0).xyz;
  let c10 = textureLoad(godray_tex, vec2<i32>(x1, y0), 0).xyz;
  let c01 = textureLoad(godray_tex, vec2<i32>(x0, y1), 0).xyz;
  let c11 = textureLoad(godray_tex, vec2<i32>(x1, y1), 0).xyz;

  let cx0 = mix(c00, c10, f.x);
  let cx1 = mix(c01, c11, f.x);
  return mix(cx0, cx1, f.y);
}

fn godray_integrate(ro: vec3<f32>, rd: vec3<f32>, t_end: f32, j: f32) -> vec3<f32> {
  let base = fog_density_godray();
  if (base <= 0.0 || t_end <= 0.0) { return vec3<f32>(0.0); }

  let costh = dot(rd, SUN_DIR);
  let phase = phase_blended(costh);

  let dt = t_end / f32(GODRAY_STEPS_FAST);

  var sum = vec3<f32>(0.0);

  // Stronger stabilization to kill shimmer:
  // - LP Ts (sun visibility)
  // - LP shaft weight itself
  var ts_lp: f32    = 1.0;
  var shaft_lp: f32 = 0.0;

  // Make smoothing scale with step length so it behaves consistently as t_end changes.
  let a_ts    = 1.0 - exp(-dt * 3.0);  // Ts smoothing
  let a_shaft = 1.0 - exp(-dt * 5.0);  // shaft smoothing

  for (var i: u32 = 0u; i < GODRAY_STEPS_FAST; i = i + 1u) {
    let ti = (f32(i) + 0.5 + j) * dt;
    if (ti <= 0.0) { continue; }

    let p = ro + rd * ti;

    let Tv = fog_transmittance_godray(ro, rd, ti);
    if (Tv < GODRAY_TV_CUTOFF) { break; }

    // Sun visibility (occluders + clouds).
    let Ts0 = sun_transmittance(p, SUN_DIR);

    // Soften hard leaf cutouts a bit (helps go through leaves look).
    // < 1.0 makes dimmer-but-present transmission survive.
    let Ts_soft = pow(clamp(Ts0, 0.0, 1.0), 0.75);

    // LP Ts heavily to remove per-frame sparkle from undersampling/jitter.
    let ts_prev = ts_lp;
    ts_lp = mix(ts_lp, Ts_soft, a_ts);

    // Edge energy from *stabilized* Ts change (this is where shafts come from).
    // Using ts_prev avoids derivative of already-updated state weirdness.
    let dTs = abs(Ts_soft - ts_prev);

    // Convert edge energy into a soft mask, then LP that too.
    var shaft = smoothstep(GODRAY_EDGE0, GODRAY_EDGE1, dTs);
    shaft = sqrt(shaft); // widen/soften
    shaft_lp = mix(shaft_lp, shaft, a_shaft);
    shaft = shaft_lp;

    // Small baseline haze so it stays volumetric (but doesnt milk out the scene).
    let haze_ramp = 1.0 - exp(-ti / GODRAY_HAZE_NEAR_FADE);
    let haze = GODRAY_BASE_HAZE * haze_ramp;

    // Prevent shafts from looking painted on in fully-dark regions:
    // tie shaft contribution to how much sun is actually present.
    let shaft_sun_gate = smoothstep(0.10, 0.55, ts_lp);

    let w = haze + (1.0 - haze) * (shaft * shaft_sun_gate);

    // Godray scatter density with its own height behavior.
    let hfall = GODRAY_SCATTER_HEIGHT_FALLOFF;
    let hmin  = GODRAY_SCATTER_MIN_FRAC;
    let height_term = max(exp(-hfall * p.y), hmin);

    let dens = base * height_term;

    // Slight strength reduction here (so you don't have to rebalance everything else).
    let strength_scale = 0.70;

    sum += (SUN_COLOR * SUN_INTENSITY) * (dens * dt) * Tv * ts_lp * phase * w * strength_scale;
  }

  return sum * GODRAY_STRENGTH;
}


@compute @workgroup_size(8, 8, 1)
fn main_primary(@builtin(global_invocation_id) gid: vec3<u32>) {
  let dims = textureDimensions(color_img);
  if (gid.x >= dims.x || gid.y >= dims.y) { return; }

  let res = vec2<f32>(f32(dims.x), f32(dims.y));
  let px  = vec2<f32>(f32(gid.x) + 0.5, f32(gid.y) + 0.5);

  let ro = cam.cam_pos.xyz;
  let rd = ray_dir_from_pixel(px, res);

  let sky = sky_color(rd);

  let voxel_size = cam.voxel_params.x;
  let nudge_p = PRIMARY_NUDGE_VOXEL_FRAC * voxel_size;

  if (cam.chunk_count == 0u) {
    let ip = vec2<i32>(i32(gid.x), i32(gid.y));
    textureStore(color_img, ip, vec4<f32>(sky, 1.0));
    textureStore(depth_img, ip, vec4<f32>(FOG_MAX_DIST, 0.0, 0.0, 0.0));
    return;
  }

  let chunk_size_m = f32(cam.chunk_size) * voxel_size;

  let go = cam.grid_origin_chunk;
  let gd = cam.grid_dims;

  let grid_bmin = vec3<f32>(
    f32(go.x) * chunk_size_m,
    f32(go.y) * chunk_size_m,
    f32(go.z) * chunk_size_m
  );

  let grid_bmax = grid_bmin + vec3<f32>(
    f32(gd.x) * chunk_size_m,
    f32(gd.y) * chunk_size_m,
    f32(gd.z) * chunk_size_m
  );

  let rtg = intersect_aabb(ro, rd, grid_bmin, grid_bmax);
  var t_enter = max(rtg.x, 0.0);
  let t_exit  = rtg.y;

  if (t_exit < t_enter) {
    let ip = vec2<i32>(i32(gid.x), i32(gid.y));
    textureStore(color_img, ip, vec4<f32>(sky, 1.0));
    textureStore(depth_img, ip, vec4<f32>(FOG_MAX_DIST, 0.0, 0.0, 0.0));
    return;
  }

  let start_t = t_enter + nudge_p;
  let p0 = ro + start_t * rd;

  var c = chunk_coord_from_pos(p0, chunk_size_m);
  var cx: i32 = c.x;
  var cy: i32 = c.y;
  var cz: i32 = c.z;

  var t_local: f32 = 0.0;

  let inv = vec3<f32>(safe_inv(rd.x), safe_inv(rd.y), safe_inv(rd.z));
  let step_x: i32 = select(-1, 1, rd.x > 0.0);
  let step_y: i32 = select(-1, 1, rd.y > 0.0);
  let step_z: i32 = select(-1, 1, rd.z > 0.0);

  let bx = select(f32(cx) * chunk_size_m, f32(cx + 1) * chunk_size_m, rd.x > 0.0);
  let by = select(f32(cy) * chunk_size_m, f32(cy + 1) * chunk_size_m, rd.y > 0.0);
  let bz = select(f32(cz) * chunk_size_m, f32(cz + 1) * chunk_size_m, rd.z > 0.0);

  var tMaxX: f32 = (bx - p0.x) * inv.x;
  var tMaxY: f32 = (by - p0.y) * inv.y;
  var tMaxZ: f32 = (bz - p0.z) * inv.z;

  let tDeltaX: f32 = abs(chunk_size_m * inv.x);
  let tDeltaY: f32 = abs(chunk_size_m * inv.y);
  let tDeltaZ: f32 = abs(chunk_size_m * inv.z);

  if (abs(rd.x) < EPS_INV) { tMaxX = BIG_F32; }
  if (abs(rd.y) < EPS_INV) { tMaxY = BIG_F32; }
  if (abs(rd.z) < EPS_INV) { tMaxZ = BIG_F32; }

  var best = HitGeom(false, BIG_F32, MAT_AIR, vec3<f32>(0.0));
  let t_exit_local = max(t_exit - start_t, 0.0);

  let max_chunk_steps = min((gd.x + gd.y + gd.z) * 6u + 8u, 1024u);

  for (var s: u32 = 0u; s < max_chunk_steps; s = s + 1u) {
    if (t_local > t_exit_local) { break; }

    let tNextLocal = min(tMaxX, min(tMaxY, tMaxZ));
    if (best.hit && (start_t + tNextLocal) >= best.t) { break; }

    let slot = grid_lookup_slot(cx, cy, cz);
    if (slot != INVALID_U32 && slot < cam.chunk_count) {
      let ch = chunks[slot];

      let cell_enter = start_t + t_local;
      let cell_exit  = start_t + min(tNextLocal, t_exit_local);

      let h = trace_chunk_hybrid_interval(ro, rd, ch, cell_enter, cell_exit);
      if (h.hit && h.t < best.t) { best = h; }
    }

    if (tMaxX < tMaxY) {
      if (tMaxX < tMaxZ) { cx += step_x; t_local = tMaxX; tMaxX += tDeltaX; }
      else               { cz += step_z; t_local = tMaxZ; tMaxZ += tDeltaZ; }
    } else {
      if (tMaxY < tMaxZ) { cy += step_y; t_local = tMaxY; tMaxY += tDeltaY; }
      else               { cz += step_z; t_local = tMaxZ; tMaxZ += tDeltaZ; }
    }

    let ox = cam.grid_origin_chunk.x;
    let oy = cam.grid_origin_chunk.y;
    let oz = cam.grid_origin_chunk.z;
    let nx = i32(cam.grid_dims.x);
    let ny = i32(cam.grid_dims.y);
    let nz = i32(cam.grid_dims.z);
    if (cx < ox || cy < oy || cz < oz || cx >= ox + nx || cy >= oy + ny || cz >= oz + nz) { break; }
  }

  let surface = select(sky, shade_hit(ro, rd, best), best.hit);
  let t_scene = select(min(t_exit, FOG_MAX_DIST), min(best.t, FOG_MAX_DIST), best.hit);

  let T = fog_transmittance_primary(ro, rd, t_scene);
  let fogc = fog_color(rd);

  let fog_amt = (1.0 - T) * FOG_PRIMARY_VIS;
  let col = mix(surface, fogc, fog_amt);

  let ip = vec2<i32>(i32(gid.x), i32(gid.y));
  textureStore(color_img, ip, vec4<f32>(col, 1.0));
  textureStore(depth_img, ip, vec4<f32>(t_scene, 0.0, 0.0, 0.0));
}

@compute @workgroup_size(8, 8, 1)
fn main_godray(@builtin(global_invocation_id) gid: vec3<u32>) {
  let qdims = textureDimensions(godray_out);
  if (gid.x >= qdims.x || gid.y >= qdims.y) { return; }

  let fdims = textureDimensions(depth_tex);
  let ro = cam.cam_pos.xyz;

  // quarter-res pixel
  let hip  = vec2<i32>(i32(gid.x), i32(gid.y));
  let qpx  = vec2<f32>(f32(gid.x), f32(gid.y));

  // frame-stable-ish pattern selector
  let frame = floor(cam.voxel_params.y * GODRAY_FRAME_FPS);
  let flip = select(
    0.0, 1.0,
    hash12(qpx * GODRAY_PATTERN_HASH_SCALE + vec2<f32>(frame, frame * 0.21)) > 0.5
  );

  // map quarter-res pixel -> a 4x4 block in full-res
  let base_x = i32(gid.x) * GODRAY_BLOCK_SIZE;
  let base_y = i32(gid.y) * GODRAY_BLOCK_SIZE;

  // 4 taps in the block (your existing pattern)
  let ax0 = select(1, 2, flip > 0.5);
  let ay0 = 1;
  let ax1 = select(3, 1, flip > 0.5);
  let ay1 = select(1, 2, flip > 0.5);
  let ax2 = select(1, 3, flip > 0.5);
  let ay2 = select(3, 2, flip > 0.5);
  let ax3 = select(3, 2, flip > 0.5);
  let ay3 = 3;

  let fp0 = vec2<i32>(clamp(base_x + ax0, 0, i32(fdims.x) - 1),
                      clamp(base_y + ay0, 0, i32(fdims.y) - 1));
  let fp1 = vec2<i32>(clamp(base_x + ax1, 0, i32(fdims.x) - 1),
                      clamp(base_y + ay1, 0, i32(fdims.y) - 1));
  let fp2 = vec2<i32>(clamp(base_x + ax2, 0, i32(fdims.x) - 1),
                      clamp(base_y + ay2, 0, i32(fdims.y) - 1));
  let fp3 = vec2<i32>(clamp(base_x + ax3, 0, i32(fdims.x) - 1),
                      clamp(base_y + ay3, 0, i32(fdims.y) - 1));

  let res_full = vec2<f32>(f32(fdims.x), f32(fdims.y));

  // per-tap jitter
  let j0 = (hash12(qpx * J0_SCALE + vec2<f32>(frame * J0_F.x, frame * J0_F.y)) - 0.5);
  let j1 = (hash12(qpx * J1_SCALE + vec2<f32>(frame * J1_F.x, frame * J1_F.y)) - 0.5);
  let j2 = (hash12(qpx * J2_SCALE + vec2<f32>(frame * J2_F.x, frame * J2_F.y)) - 0.5);
  let j3 = (hash12(qpx * J3_SCALE + vec2<f32>(frame * J3_F.x, frame * J3_F.y)) - 0.5);

  // read depth for each tap (also used for a cheap "edge/disocclusion" heuristic)
  let t_scene0 = textureLoad(depth_tex, fp0, 0).x;
  let t_scene1 = textureLoad(depth_tex, fp1, 0).x;
  let t_scene2 = textureLoad(depth_tex, fp2, 0).x;
  let t_scene3 = textureLoad(depth_tex, fp3, 0).x;

  // integrate godrays for the taps
  var acc = vec3<f32>(0.0);
  var wsum = 0.0;

  let t_end0 = min(t_scene0, GODRAY_MAX_DIST);
  if (t_end0 > 0.0 && fog_density_godray() > 0.0) {
    let px0 = vec2<f32>(f32(fp0.x) + 0.5, f32(fp0.y) + 0.5);
    acc += godray_integrate(ro, ray_dir_from_pixel(px0, res_full), t_end0, j0);
    wsum += 1.0;
  }

  let t_end1 = min(t_scene1, GODRAY_MAX_DIST);
  if (t_end1 > 0.0 && fog_density_godray() > 0.0) {
    let px1 = vec2<f32>(f32(fp1.x) + 0.5, f32(fp1.y) + 0.5);
    acc += godray_integrate(ro, ray_dir_from_pixel(px1, res_full), t_end1, j1);
    wsum += 1.0;
  }

  let t_end2 = min(t_scene2, GODRAY_MAX_DIST);
  if (t_end2 > 0.0 && fog_density_godray() > 0.0) {
    let px2 = vec2<f32>(f32(fp2.x) + 0.5, f32(fp2.y) + 0.5);
    acc += godray_integrate(ro, ray_dir_from_pixel(px2, res_full), t_end2, j2);
    wsum += 1.0;
  }

  let t_end3 = min(t_scene3, GODRAY_MAX_DIST);
  if (t_end3 > 0.0 && fog_density_godray() > 0.0) {
    let px3 = vec2<f32>(f32(fp3.x) + 0.5, f32(fp3.y) + 0.5);
    acc += godray_integrate(ro, ray_dir_from_pixel(px3, res_full), t_end3, j3);
    wsum += 1.0;
  }

  let cur = max(select(vec3<f32>(0.0), acc / wsum, wsum > 0.0), vec3<f32>(0.0));

  // -------------------------------
  // Temporal resolve (reduced ghosting)
  // -------------------------------

  // history (quarter-res)
  let hist = textureLoad(godray_hist_tex, hip, 0).xyz;

  // (A) depth-edge heuristic inside this quarter-res block:
  // large depth span => likely edge/disocclusion => reduce history
  let dmin = min(min(t_scene0, t_scene1), min(t_scene2, t_scene3));
  let dmax = max(max(t_scene0, t_scene1), max(t_scene2, t_scene3));
  let span = (dmax - dmin) / max(dmin, 1e-3);
  let edge = smoothstep(0.03, 0.15, span); // tune

  // (B) reactive heuristic: large change in godray energy => reduce history
  let delta = length(cur - hist);
  let react = smoothstep(0.03, 0.18, delta); // tune

  // stable = 1 when safe to accumulate, 0 when we should mostly trust current
  let stable = 1.0 - max(edge, react);

  // clamp history near current to prevent trails / overshoot
  let clamp_w = max(cur * 0.75, vec3<f32>(0.02)); // tune (bigger = less ghosting, more flicker)
  let hist_clamped = clamp(hist, cur - clamp_w, cur + clamp_w);

  // history weight (how much of hist_clamped survives)
  // GODRAY_TS_LP_ALPHA is your knob: higher = smoother but more ghost risk.
  let hist_w = clamp(GODRAY_TS_LP_ALPHA * stable, 0.0, 0.90);

  // final
  let blended = mix(cur, hist_clamped, hist_w);

  textureStore(godray_out, hip, vec4<f32>(blended, 1.0));
}


@compute @workgroup_size(8, 8, 1)
fn main_composite(@builtin(global_invocation_id) gid: vec3<u32>) {
  let dims = textureDimensions(out_img);
  if (gid.x >= dims.x || gid.y >= dims.y) { return; }

  let ip = vec2<i32>(i32(gid.x), i32(gid.y));
  let base = textureLoad(color_tex, ip, 0).xyz;

  let px = vec2<f32>(f32(gid.x) + 0.5, f32(gid.y) + 0.5);

  let g = godray_sample_bilerp(px);

  let gx = godray_sample_bilerp(px + vec2<f32>( 1.0, 0.0)) + godray_sample_bilerp(px + vec2<f32>(-1.0, 0.0));
  let gy = godray_sample_bilerp(px + vec2<f32>(0.0,  1.0)) + godray_sample_bilerp(px + vec2<f32>(0.0, -1.0));
  let blur = 0.25 * (gx + gy);

  var god_raw = max(g + COMPOSITE_SHARPEN * (g - blur), vec3<f32>(0.0));

  // remove baseline haze (keeps only excess beam energy)
  god_raw = max(god_raw - vec3<f32>(GODRAY_BLACK_LEVEL), vec3<f32>(0.0));

  let god = (vec3<f32>(1.0) - exp(-god_raw));

  let hdr = max(base + COMPOSITE_GOD_SCALE * god, vec3<f32>(0.0));
  let ldr = tonemap_exp(hdr);

  textureStore(out_img, ip, vec4<f32>(ldr, 1.0));
}


// src/streaming/manager.rs
// ------------------------
// Chunk streaming + background SVO building + CPU cache.
//
// New behavior:
// - Chunks are built once, then stored in a CPU cache (budgeted, LRU-ish).
// - When chunks come back into range, we "promote" from cache: allocate GPU node arena
//   range + upload nodes/meta, without rebuilding on worker threads.

use std::collections::{HashMap, HashSet, VecDeque};
use std::mem::size_of;
use std::sync::{
    atomic::{AtomicBool, Ordering},
    Arc,
};

use crossbeam_channel::{unbounded, Receiver, Sender};
use glam::{Vec2, Vec3};

use crate::{
    config,
    render::gpu_types::{ChunkMetaGpu, NodeGpu},
    svo::{build_chunk_svo_sparse_cancelable_with_scratch, BuildScratch},
    world::WorldGen,
};

use super::NodeArena;

const INVALID_U32: u32 = 0xFFFF_FFFF;

// Vertical band dy in [-1..=2]
const GRID_Y_MIN_DY: i32 = -1;
const GRID_Y_COUNT: u32 = 4;

// How many eviction attempts to make when we can't fit a chunk's nodes contiguously.
const EVICT_ATTEMPTS: usize = 8;

#[derive(Clone, Copy, Hash, PartialEq, Eq, Debug)]
struct ChunkKey {
    x: i32,
    y: i32,
    z: i32,
}

enum ChunkState {
    Missing,
    Queued,
    Building,
    Resident(Resident),
}

#[derive(Clone, Copy, Debug)]
struct Resident {
    slot: u32,      // index into chunk_meta (dense)
    node_base: u32, // base index into global node arena
    node_count: u32,
}

#[derive(Clone, Debug)]
struct BuildJob {
    key: ChunkKey,
    cancel: Arc<AtomicBool>,
}

struct BuildDone {
    key: ChunkKey,
    cancel: Arc<AtomicBool>,
    canceled: bool,
    nodes: Vec<NodeGpu>,
}

pub struct ChunkUpload {
    pub slot: u32,
    pub meta: ChunkMetaGpu,

    pub node_base: u32,
    pub nodes: Arc<[NodeGpu]>,
}

// -----------------------------
// CPU cache (budgeted, LRU-ish)
// -----------------------------

#[derive(Clone)]
struct CachedChunk {
    nodes: Arc<[NodeGpu]>,
    bytes: usize,
    stamp: u64,
}

fn spawn_workers(gen: Arc<WorldGen>, rx_job: Receiver<BuildJob>, tx_done: Sender<BuildDone>) {
    for _ in 0..config::WORKER_THREADS {
        let gen = gen.clone();
        let rx_job = rx_job.clone();
        let tx_done = tx_done.clone();

        std::thread::spawn(move || {
            // One reusable scratch per worker thread: removes most per-chunk allocations.
            let mut scratch = BuildScratch::new();

            while let Ok(job) = rx_job.recv() {
                let k = job.key;

                // If we were already cancelled before starting, still notify the main thread
                // so it can decrement `in_flight`.
                if job.cancel.load(Ordering::Relaxed) {
                    if tx_done
                        .send(BuildDone {
                            key: k,
                            cancel: job.cancel,
                            canceled: true,
                            nodes: Vec::new(),
                        })
                        .is_err()
                    {
                        break;
                    }
                    continue;
                }

                let origin = [
                    k.x * config::CHUNK_SIZE as i32,
                    k.y * config::CHUNK_SIZE as i32,
                    k.z * config::CHUNK_SIZE as i32,
                ];

                let nodes = build_chunk_svo_sparse_cancelable_with_scratch(
                    &gen,
                    origin,
                    config::CHUNK_SIZE,
                    job.cancel.as_ref(),
                    &mut scratch,
                );

                // If we got cancelled mid-build, still notify the main thread,
                // but drop nodes to save main-thread work + upload pressure.
                let canceled = job.cancel.load(Ordering::Relaxed);
                let nodes = if canceled { Vec::new() } else { nodes };

                if tx_done
                    .send(BuildDone {
                        key: k,
                        cancel: job.cancel,
                        canceled,
                        nodes,
                    })
                    .is_err()
                {
                    break;
                }
            }
        });
    }
}

fn sort_queue_near_first(queue: &mut VecDeque<ChunkKey>, center: ChunkKey, cam_fwd: Vec3) {
    let mut v: Vec<ChunkKey> = queue.drain(..).collect();

    // Horizontal forward (XZ) for look direction.
    let mut f = Vec2::new(cam_fwd.x, cam_fwd.z);
    if f.length_squared() > 1e-6 {
        f = f.normalize();
    }

    v.sort_by(|a, b| {
        let sa = chunk_priority_score(*a, center, f);
        let sb = chunk_priority_score(*b, center, f);
        sa.partial_cmp(&sb).unwrap_or(std::cmp::Ordering::Equal)
    });

    queue.extend(v);
}

fn chunk_priority_score(k: ChunkKey, c: ChunkKey, _fwd_xz: Vec2) -> f32 {
    let dx = (k.x - c.x) as f32;
    let dz = (k.z - c.z) as f32;
    let dy = (k.y - c.y) as f32;

    // Lower score = higher priority.
    // Base distance (prefer close). Penalize vertical moves more.
    dx.abs() + dz.abs() + 2.0 * dy.abs()
}

pub struct ChunkManager {
    gen: Arc<WorldGen>,

    chunks: HashMap<ChunkKey, ChunkState>,
    build_queue: VecDeque<ChunkKey>,

    // Deduplicate queued keys (prevents unbounded queue growth).
    queued_set: HashSet<ChunkKey>,

    // Only sort/purge when center chunk changes.
    last_center: Option<ChunkKey>,

    // Per-chunk cancel tokens
    cancels: HashMap<ChunkKey, Arc<AtomicBool>>,

    tx_job: Sender<BuildJob>,
    rx_done: Receiver<BuildDone>,
    in_flight: usize,

    // Dense slots for resident chunks
    slot_to_key: Vec<ChunkKey>,    // slot -> key
    chunk_meta: Vec<ChunkMetaGpu>, // slot -> meta
    uploads: Vec<ChunkUpload>,     // pending GPU writes this frame

    // General something changed flag (kept for other systems).
    changed: bool,

    // Grid dirty flag:
    // - true when the GPU lookup grid needs rebuilding (origin shift or resident slot mapping changed)
    // - update() returns this so the renderer can skip `write_chunk_grid()` on most frames.
    grid_dirty: bool,

    // Node arena (in units of NodeGpu elements)
    arena: NodeArena,

    // Chunk grid for GPU lookup: maps grid cell -> resident slot index (or INVALID_U32).
    grid_origin_chunk: [i32; 3],
    grid_dims: [u32; 3],
    chunk_grid: Vec<u32>,

    // ----------------
    // CPU chunk cache
    // ----------------
    cache: HashMap<ChunkKey, CachedChunk>,
    cache_lru: VecDeque<(ChunkKey, u64)>, // (key, stamp) entries; duplicates are allowed
    cache_stamp: u64,
    cache_bytes: usize,
}

impl ChunkManager {
    pub fn new(gen: Arc<WorldGen>) -> Self {
        let (tx_job, rx_job) = unbounded::<BuildJob>();
        let (tx_done, rx_done) = unbounded::<BuildDone>();
        spawn_workers(gen.clone(), rx_job, tx_done);

        // Arena capacity in NodeGpu elements.
        let node_capacity = (config::NODE_BUDGET_BYTES / size_of::<NodeGpu>()) as u32;

        // Grid size (KEEP box).
        let nx = (2 * config::KEEP_RADIUS + 1) as u32;
        let nz = nx;
        let ny = GRID_Y_COUNT;
        let grid_len = (nx * ny * nz) as usize;

        Self {
            gen,
            chunks: HashMap::new(),
            build_queue: VecDeque::new(),
            queued_set: HashSet::new(),
            last_center: None,

            cancels: HashMap::new(),
            tx_job,
            rx_done,
            in_flight: 0,

            slot_to_key: Vec::new(),
            chunk_meta: Vec::new(),
            uploads: Vec::new(),
            changed: false,

            grid_dirty: true, // first update should build + upload the grid
            arena: NodeArena::new(node_capacity),

            grid_origin_chunk: [0, 0, 0],
            grid_dims: [nx, ny, nz],
            chunk_grid: vec![INVALID_U32; grid_len],

            cache: HashMap::new(),
            cache_lru: VecDeque::new(),
            cache_stamp: 1,
            cache_bytes: 0,
        }
    }

    // -------------------------------------------------------------------------
    // Public API
    // -------------------------------------------------------------------------

    pub fn chunk_count(&self) -> u32 {
        self.slot_to_key.len() as u32
    }

    pub fn grid_origin(&self) -> [i32; 3] {
        self.grid_origin_chunk
    }

    pub fn grid_dims(&self) -> [u32; 3] {
        self.grid_dims
    }

    pub fn chunk_grid(&self) -> &[u32] {
        &self.chunk_grid
    }

    pub fn take_uploads(&mut self) -> Vec<ChunkUpload> {
        std::mem::take(&mut self.uploads)
    }

    // -------------------------------------------------------------------------
    // Streaming update
    // -------------------------------------------------------------------------
    //
    // Returns:
    // - true  if the chunk_grid mapping changed (origin shift and/or resident slot mapping changed)
    // - false if chunk_grid is identical to last frame (safe to skip GPU upload)

    pub fn update(&mut self, world: &Arc<WorldGen>, cam_pos_m: Vec3, cam_fwd: Vec3) -> bool {
        self.uploads.clear();

        // Center chunk (ground-anchored).
        let cam_vx = (cam_pos_m.x / config::VOXEL_SIZE_M_F32).floor() as i32;
        let cam_vz = (cam_pos_m.z / config::VOXEL_SIZE_M_F32).floor() as i32;

        let ccx = cam_vx.div_euclid(config::CHUNK_SIZE as i32);
        let ccz = cam_vz.div_euclid(config::CHUNK_SIZE as i32);

        let ground_y_vox = world.ground_height(cam_vx, cam_vz);
        let ground_cy = ground_y_vox.div_euclid(config::CHUNK_SIZE as i32);

        let center = ChunkKey {
            x: ccx,
            y: ground_cy,
            z: ccz,
        };

        // Desired vs keep sets.
        let desired = Self::desired_chunks(center, config::ACTIVE_RADIUS);
        let keep = Self::desired_chunks(center, config::KEEP_RADIUS);
        let keep_set: HashSet<ChunkKey> = keep.iter().copied().collect();

        // Promote cached desired chunks immediately (no rebuild).
        // For missing desired chunks that are not cached, queue a build.
        for k in &desired {
            match self.chunks.get(k) {
                Some(ChunkState::Resident(_)) | Some(ChunkState::Queued) | Some(ChunkState::Building) => {
                    // Already handled / in progress.
                }
                None | Some(ChunkState::Missing) => {
                    // Cache hit: try promote from cache (alloc arena + upload), DO NOT queue build.
                    if self.cache.contains_key(k) {
                        let _ = self.try_promote_from_cache(center, *k);
                        continue;
                    }

                    // Cache miss: queue build.
                    let c = self.cancel_token(*k);
                    c.store(false, Ordering::Relaxed);

                    self.chunks.insert(*k, ChunkState::Queued);

                    // Dedupe queue entries.
                    if self.queued_set.insert(*k) {
                        self.build_queue.push_back(*k);
                    }
                }
            }
        }

        // Unload outside keep (also cancel queued/building).
        {
            let keys_snapshot: Vec<ChunkKey> = self.chunks.keys().copied().collect();
            for k in keys_snapshot {
                if !keep_set.contains(&k) {
                    self.unload_chunk(k);
                }
            }
        }

        // Only purge + sort when the center chunk changes.
        let center_changed = self.last_center.map_or(true, |c| c != center);
        if center_changed {
            self.last_center = Some(center);

            // Purge stale keys aggressively: keep only keys that are still queued and still in KEEP.
            self.build_queue.retain(|k| {
                keep_set.contains(k) && matches!(self.chunks.get(k), Some(ChunkState::Queued))
            });

            // Rebuild queued_set from the queue so it matches reality.
            self.queued_set.clear();
            self.queued_set.extend(self.build_queue.iter().copied());

            // Sort once per center change.
            sort_queue_near_first(&mut self.build_queue, center, cam_fwd);
        }

        // Dispatch builds (only for cache misses).
        while self.in_flight < config::MAX_IN_FLIGHT {
            let Some(k) = self.build_queue.pop_front() else { break; };

            // Popped => no longer queued.
            self.queued_set.remove(&k);

            if !keep_set.contains(&k) {
                // Cancel if it was pending.
                self.cancel_token(k).store(true, Ordering::Relaxed);
                self.chunks.insert(k, ChunkState::Missing);
                continue;
            }

            // If it became cached since it was queued (rare, but possible if you later add disk caching),
            // don't rebuild it.
            if self.cache.contains_key(&k) {
                self.chunks.insert(k, ChunkState::Missing);
                let _ = self.try_promote_from_cache(center, k);
                continue;
            }

            if matches!(self.chunks.get(&k), Some(ChunkState::Queued)) {
                self.chunks.insert(k, ChunkState::Building);

                let cancel = self.cancel_token(k);
                cancel.store(false, Ordering::Relaxed);

                if self
                    .tx_job
                    .send(BuildJob {
                        key: k,
                        cancel: cancel.clone(),
                    })
                    .is_ok()
                {
                    self.in_flight += 1;
                } else {
                    // Channel closed; keep it queued.
                    self.chunks.insert(k, ChunkState::Queued);

                    // Put it back (dedup-safe).
                    if self.queued_set.insert(k) {
                        self.build_queue.push_back(k);
                    }
                    break;
                }
            }
        }

        // Harvest done builds.
        while let Ok(done) = self.rx_done.try_recv() {
            if self.in_flight > 0 {
                self.in_flight -= 1;
            }

            // If the job was canceled (either before start or mid-build),
            // the chunk MUST NOT stay in Building forever.
            if done.canceled || done.cancel.load(Ordering::Relaxed) {
                if self.chunks.get(&done.key).is_some() {
                    self.chunks.insert(done.key, ChunkState::Missing);
                }
                continue;
            }

            // If it finished but is no longer in KEEP, drop it and mark Missing.
            if !keep_set.contains(&done.key) {
                self.cancel_token(done.key).store(true, Ordering::Relaxed);
                self.chunks.insert(done.key, ChunkState::Missing);
                continue;
            }

            // Still relevant: cache + try resident.
            self.on_build_done(center, done.key, done.nodes);
        }

        // If the keep-grid origin would shift, the lookup mapping changes.
        if self.keep_origin_for(center) != self.grid_origin_chunk {
            self.grid_dirty = true;
        }

        // Rebuild grid mapping for current KEEP region only when needed.
        let grid_changed = self.grid_dirty;
        if self.grid_dirty {
            self.rebuild_grid(center);
            self.grid_dirty = false;
        }

        grid_changed
    }

    // -------------------------------------------------------------------------
    // Internals
    // -------------------------------------------------------------------------

    fn cancel_token(&mut self, key: ChunkKey) -> Arc<AtomicBool> {
        self.cancels
            .entry(key)
            .or_insert_with(|| Arc::new(AtomicBool::new(false)))
            .clone()
    }

    fn desired_chunks(center: ChunkKey, radius: i32) -> Vec<ChunkKey> {
        let mut out = Vec::new();
        for dy in GRID_Y_MIN_DY..=(GRID_Y_MIN_DY + GRID_Y_COUNT as i32 - 1) {
            for dz in -radius..=radius {
                for dx in -radius..=radius {
                    out.push(ChunkKey {
                        x: center.x + dx,
                        y: center.y + dy,
                        z: center.z + dz,
                    });
                }
            }
        }
        out
    }

    fn evict_one_farthest(&mut self, center: ChunkKey, protect: ChunkKey) -> bool {
        if self.slot_to_key.is_empty() {
            return false;
        }

        let mut best: Option<(f32, ChunkKey)> = None;
        for &k in &self.slot_to_key {
            if k == protect {
                continue;
            }
            let dx = (k.x - center.x) as f32;
            let dz = (k.z - center.z) as f32;
            let dy = (k.y - center.y) as f32;

            // Weighted distance (favor keeping vertical neighbors).
            let d = dx * dx + dz * dz + 4.0 * dy * dy;

            if best.map_or(true, |(bd, _)| d > bd) {
                best = Some((d, k));
            }
        }

        if let Some((_, k)) = best {
            self.unload_chunk(k);
            return true;
        }

        false
    }

    // ----------------
    // Cache helpers
    // ----------------

    fn cache_touch(&mut self, key: ChunkKey) {
        if let Some(e) = self.cache.get_mut(&key) {
            self.cache_stamp = self.cache_stamp.wrapping_add(1).max(1);
            e.stamp = self.cache_stamp;
            self.cache_lru.push_back((key, e.stamp));
        }
    }

    fn cache_put(&mut self, key: ChunkKey, nodes: Arc<[NodeGpu]>) {
        let bytes = nodes.len() * size_of::<NodeGpu>();

        // If replacing an existing entry, subtract its bytes first.
        if let Some(old) = self.cache.remove(&key) {
            self.cache_bytes = self.cache_bytes.saturating_sub(old.bytes);
        }

        self.cache_stamp = self.cache_stamp.wrapping_add(1).max(1);
        let stamp = self.cache_stamp;

        self.cache.insert(
            key,
            CachedChunk {
                nodes,
                bytes,
                stamp,
            },
        );

        self.cache_bytes = self.cache_bytes.saturating_add(bytes);
        self.cache_lru.push_back((key, stamp));

        self.evict_cache_as_needed();
    }

    fn evict_cache_as_needed(&mut self) {
        let budget = config::CHUNK_CACHE_BUDGET_BYTES;

        while self.cache_bytes > budget {
            let Some((k, stamp)) = self.cache_lru.pop_front() else { break; };

            // Only evict if this LRU record matches the current entry stamp.
            let should_evict = self
                .cache
                .get(&k)
                .map(|e| e.stamp == stamp)
                .unwrap_or(false);

            if !should_evict {
                continue;
            }

            if let Some(ev) = self.cache.remove(&k) {
                self.cache_bytes = self.cache_bytes.saturating_sub(ev.bytes);
            }
        }
    }

    // -------------------------------
    // Resident creation / promotion
    // -------------------------------

    fn try_make_resident(
        &mut self,
        center: ChunkKey,
        key: ChunkKey,
        nodes: Arc<[NodeGpu]>,
    ) -> bool {
        // If already resident, nothing to do.
        if matches!(self.chunks.get(&key), Some(ChunkState::Resident(_))) {
            return true;
        }

        let need = nodes.len() as u32;

        // If we have nothing (shouldn't happen for non-air chunks, but be safe).
        if need == 0 {
            self.chunks.insert(key, ChunkState::Missing);
            return false;
        }

        // Try allocate; if fails, evict farthest chunks and retry a few times.
        let mut node_base = self.arena.alloc(need);
        if node_base.is_none() {
            for _ in 0..EVICT_ATTEMPTS {
                if !self.evict_one_farthest(center, key) {
                    break;
                }
                node_base = self.arena.alloc(need);
                if node_base.is_some() {
                    break;
                }
            }
        }

        let Some(node_base) = node_base else {
            // Can't fit right now; keep cache so we can promote later.
            self.chunks.insert(key, ChunkState::Missing);
            return false;
        };

        // Allocate slot (dense).
        let slot = self.slot_to_key.len() as u32;
        self.slot_to_key.push(key);

        let origin_vox = [
            key.x * config::CHUNK_SIZE as i32,
            key.y * config::CHUNK_SIZE as i32,
            key.z * config::CHUNK_SIZE as i32,
        ];

        let meta = ChunkMetaGpu {
            origin: [origin_vox[0], origin_vox[1], origin_vox[2], 0],
            node_base,
            node_count: need,
            _pad0: 0,
            _pad1: 0,
        };

        self.chunk_meta.push(meta);

        // Mark resident.
        self.chunks.insert(
            key,
            ChunkState::Resident(Resident {
                slot,
                node_base,
                node_count: need,
            }),
        );

        // Schedule GPU upload (nodes + meta).
        self.uploads.push(ChunkUpload {
            slot,
            meta,
            node_base,
            nodes,
        });

        // Resident set changed => grid mapping may change.
        self.grid_dirty = true;
        self.changed = true;

        true
    }

    fn try_promote_from_cache(&mut self, center: ChunkKey, key: ChunkKey) -> bool {
        let Some(entry) = self.cache.get(&key).cloned() else {
            return false;
        };

        // Touch LRU.
        self.cache_touch(key);

        // Try to make resident from cached nodes.
        self.try_make_resident(center, key, entry.nodes)
    }

    fn on_build_done(&mut self, center: ChunkKey, key: ChunkKey, nodes: Vec<NodeGpu>) {
        // If this chunk got cancelled while the result was in flight, drop it.
        if let Some(c) = self.cancels.get(&key) {
            if c.load(Ordering::Relaxed) {
                self.chunks.insert(key, ChunkState::Missing);
                return;
            }
        }

        // Convert to Arc slice once (cheap clones thereafter).
        let nodes_arc: Arc<[NodeGpu]> = nodes.into();

        // Cache it (so we never rebuild this chunk again unless evicted).
        self.cache_put(key, nodes_arc.clone());

        // If already resident (should be rare), don't allocate/upload again.
        if matches!(self.chunks.get(&key), Some(ChunkState::Resident(_))) {
            return;
        }

        // Try to allocate + upload now; if we can't fit, keep it cached and mark missing.
        let ok = self.try_make_resident(center, key, nodes_arc);
        if !ok {
            self.chunks.insert(key, ChunkState::Missing);
        }
    }

    fn unload_chunk(&mut self, key: ChunkKey) {
        let Some(state) = self.chunks.get(&key) else { return; };

        match *state {
            ChunkState::Resident(res) => {
                // Free node arena range.
                self.arena.free(res.node_base, res.node_count);

                // Remove slot densely by swap-remove.
                let dead_slot = res.slot as usize;
                let last_slot = self.slot_to_key.len().saturating_sub(1);

                if dead_slot != last_slot {
                    let moved_key = self.slot_to_key[last_slot];
                    self.slot_to_key[dead_slot] = moved_key;

                    // Move meta.
                    let moved_meta = self.chunk_meta[last_slot];
                    self.chunk_meta[dead_slot] = moved_meta;

                    // Update moved chunk's Resident.slot.
                    if let Some(state) = self.chunks.get_mut(&moved_key) {
                        if let ChunkState::Resident(mr) = state {
                            mr.slot = dead_slot as u32;
                        }
                    }

                    // Schedule meta rewrite for moved slot (GPU needs updated slot meta).
                    self.uploads.push(ChunkUpload {
                        slot: dead_slot as u32,
                        meta: self.chunk_meta[dead_slot],
                        node_base: 0,
                        nodes: Arc::<[NodeGpu]>::from(Vec::<NodeGpu>::new()),
                    });
                }

                self.slot_to_key.pop();
                self.chunk_meta.pop();

                self.chunks.insert(key, ChunkState::Missing);

                // Slot mapping changed => grid mapping changed.
                self.grid_dirty = true;
                self.changed = true;
            }

            ChunkState::Queued | ChunkState::Building => {
                // Cancel work in-flight.
                self.cancel_token(key).store(true, Ordering::Relaxed);

                // If it was queued, prevent duplicate re-adds.
                self.queued_set.remove(&key);

                self.chunks.insert(key, ChunkState::Missing);

                // Conservative dirty.
                self.grid_dirty = true;
                self.changed = true;
            }

            _ => {}
        }
    }

    /// Compute the KEEP-grid origin for a given center (helper so we can detect origin shifts).
    #[inline]
    fn keep_origin_for(&self, center: ChunkKey) -> [i32; 3] {
        let ox = center.x - config::KEEP_RADIUS;
        let oz = center.z - config::KEEP_RADIUS;
        let oy = center.y + GRID_Y_MIN_DY;
        [ox, oy, oz]
    }

    /// Rebuild the chunk_grid mapping for the current KEEP volume.
    ///
    /// This is intentionally called only when `grid_dirty` is set, because it is O(ncells + nchunks)
    /// and it forces a full GPU upload if you do it every frame.
    fn rebuild_grid(&mut self, center: ChunkKey) {
        let nx = (2 * config::KEEP_RADIUS + 1) as u32;
        let nz = nx;
        let ny = GRID_Y_COUNT;

        self.grid_dims = [nx, ny, nz];
        self.grid_origin_chunk = self.keep_origin_for(center);

        let needed = (nx * ny * nz) as usize;
        if self.chunk_grid.len() != needed {
            self.chunk_grid.resize(needed, INVALID_U32);
        }
        self.chunk_grid.fill(INVALID_U32);

        // Fill grid from resident chunks (slot -> key).
        for (slot, &k) in self.slot_to_key.iter().enumerate() {
            if let Some(idx) = self.grid_index_for_chunk(k) {
                self.chunk_grid[idx] = slot as u32;
            }
        }
    }

    #[inline]
    fn grid_index_for_chunk(&self, k: ChunkKey) -> Option<usize> {
        let [ox, oy, oz] = self.grid_origin_chunk;
        let [nx, ny, nz] = self.grid_dims;

        let ix = k.x - ox;
        let iy = k.y - oy;
        let iz = k.z - oz;

        if ix < 0 || iy < 0 || iz < 0 {
            return None;
        }

        let ix = ix as u32;
        let iy = iy as u32;
        let iz = iz as u32;

        if ix >= nx || iy >= ny || iz >= nz {
            return None;
        }

        let idx = (iz * ny * nx) + (iy * nx) + ix;
        Some(idx as usize)
    }
}


// src/streaming/mod.rs
// --------------------
// Chunk streaming + node arena + uploads.

pub mod manager;
pub mod node_arena;

pub use manager::{ChunkManager, ChunkUpload};
pub use node_arena::NodeArena;


// src/streaming/node_arena.rs
//
// Very simple free-list arena for node ranges (in units of NodeGpu elements).
// Improvements:
// - free() now fully coalesces adjacent ranges (fixes long-run fragmentation).
// - alloc() uses best-fit (smallest range that fits) to reduce fragmentation further.

#[derive(Clone, Copy, Debug)]
struct Range {
    start: u32,
    len: u32,
}

pub struct NodeArena {
    free: Vec<Range>, // kept sorted by start
}

impl NodeArena {
    pub fn new(capacity: u32) -> Self {
        Self {
            free: vec![Range {
                start: 0,
                len: capacity,
            }],
        }
    }

    /// Allocate a contiguous range of `len` elements.
    /// Returns the start index in the arena, or None if no free range fits.
    pub fn alloc(&mut self, len: u32) -> Option<u32> {
        if len == 0 {
            return Some(0);
        }

        // Best-fit: choose the smallest free range that still fits.
        let mut best_i: Option<usize> = None;
        let mut best_len: u32 = u32::MAX;

        for (i, r) in self.free.iter().enumerate() {
            if r.len >= len && r.len < best_len {
                best_len = r.len;
                best_i = Some(i);
                if r.len == len {
                    break; // perfect fit
                }
            }
        }

        let i = best_i?;
        let r = self.free[i];
        let start = r.start;

        if r.len == len {
            self.free.remove(i);
        } else {
            self.free[i] = Range {
                start: r.start + len,
                len: r.len - len,
            };
        }

        Some(start)
    }

    /// Free a previously allocated range.
    pub fn free(&mut self, start: u32, len: u32) {
        if len == 0 {
            return;
        }

        // Insert sorted by start.
        let mut idx = 0usize;
        while idx < self.free.len() && self.free[idx].start < start {
            idx += 1;
        }
        self.free.insert(idx, Range { start, len });

        // Fully coalesce with neighbors (both directions).
        self.coalesce_at(idx);
    }

    fn coalesce_at(&mut self, mut i: usize) {
        // Merge backward as long as possible.
        while i > 0 {
            let a = self.free[i - 1];
            let b = self.free[i];
            if a.start + a.len == b.start {
                self.free[i - 1] = Range {
                    start: a.start,
                    len: a.len + b.len,
                };
                self.free.remove(i);
                i -= 1;
            } else {
                break;
            }
        }

        // Merge forward as long as possible.
        while i + 1 < self.free.len() {
            let a = self.free[i];
            let b = self.free[i + 1];
            if a.start + a.len == b.start {
                self.free[i] = Range {
                    start: a.start,
                    len: a.len + b.len,
                };
                self.free.remove(i + 1);
            } else {
                break;
            }
        }
    }

    // Optional: quick stats for debugging.
    pub fn free_range_count(&self) -> usize {
        self.free.len()
    }

    pub fn largest_free_range(&self) -> u32 {
        self.free.iter().map(|r| r.len).max().unwrap_or(0)
    }

    pub fn total_free(&self) -> u32 {
        self.free.iter().map(|r| r.len).sum()
    }
}


// src/svo/builder.rs

use std::sync::atomic::{AtomicBool, Ordering};

use crate::{
    config,
    render::NodeGpu,
    world::{
        materials::{AIR, DIRT, GRASS, STONE, WOOD},
        WorldGen,
    },
};

use super::mips::{build_max_mip_inplace, build_minmax_mip_inplace, MaxMipView, MinMaxMipView};

const LEAF: u32 = 0xFFFF_FFFF;

#[inline]
fn is_empty_leaf(n: &NodeGpu) -> bool {
    n.child_base == LEAF && n.material == AIR
}

#[inline]
fn should_cancel(cancel: &AtomicBool) -> bool {
    cancel.load(Ordering::Relaxed)
}

/// Reusable scratch buffers for chunk building (reduces allocations & improves locality).
pub struct BuildScratch {
    // 2D (side*side)
    ground: Vec<i32>,
    tree_top: Vec<i32>,

    // --- add this ---
    height_cache: Vec<i32>,
    height_cache_w: usize,
    height_cache_h: usize,

    // 3D (side^3)
    material: Vec<u32>,
    prefix: Vec<u32>,

    // mip storage
    ground_min_levels: Vec<Vec<i32>>,
    ground_max_levels: Vec<Vec<i32>>,
    tree_levels: Vec<Vec<i32>>,
}


impl BuildScratch {
    pub fn new() -> Self {
        Self {
            ground: Vec::new(),
            tree_top: Vec::new(),

            height_cache: Vec::new(),
            height_cache_w: 0,
            height_cache_h: 0,

            material: Vec::new(),
            prefix: Vec::new(),
            ground_min_levels: Vec::new(),
            ground_max_levels: Vec::new(),
            tree_levels: Vec::new(),
        }
    }

    #[inline]
    fn ensure_height_cache(&mut self, w: usize, h: usize) {
        let need = w * h;
        if self.height_cache.len() != need {
            self.height_cache.resize(need, 0);
        } else {
            self.height_cache.fill(0);
        }
        self.height_cache_w = w;
        self.height_cache_h = h;
    }


    #[inline]
    fn ensure_2d(v: &mut Vec<i32>, side: usize, fill: i32) {
        let need = side * side;
        if v.len() != need {
            v.resize(need, fill);
        } else {
            v.fill(fill);
        }
    }

    #[inline]
    fn ensure_3d_u32(v: &mut Vec<u32>, side: usize, fill: u32) {
        let need = side * side * side;
        if v.len() != need {
            v.resize(need, fill);
        } else {
            v.fill(fill);
        }
    }

    #[inline]
    fn ensure_prefix(v: &mut Vec<u32>, side: usize) {
        let dim = side + 1;
        let need = dim * dim * dim;
        if v.len() != need {
            v.resize(need, 0);
        } else {
            v.fill(0);
        }
    }
}

#[inline]
fn idx2(side: usize, x: usize, z: usize) -> usize {
    z * side + x
}

#[inline]
fn idx3(side: usize, x: usize, y: usize, z: usize) -> usize {
    (y * side * side) + (z * side) + x
}

#[inline]
fn pidx(dim: usize, x: usize, y: usize, z: usize) -> usize {
    (z * dim * dim) + (y * dim) + x
}

#[inline]
fn prefix_sum_cube(prefix: &[u32], side: usize, x0: usize, y0: usize, z0: usize, size: usize) -> u32 {
    let dim = side + 1;
    let x1 = x0 + size;
    let y1 = y0 + size;
    let z1 = z0 + size;

    let a = prefix[pidx(dim, x1, y1, z1)] as i64;
    let b = prefix[pidx(dim, x0, y1, z1)] as i64;
    let c = prefix[pidx(dim, x1, y0, z1)] as i64;
    let d = prefix[pidx(dim, x1, y1, z0)] as i64;
    let e = prefix[pidx(dim, x0, y0, z1)] as i64;
    let f = prefix[pidx(dim, x0, y1, z0)] as i64;
    let g = prefix[pidx(dim, x1, y0, z0)] as i64;
    let h = prefix[pidx(dim, x0, y0, z0)] as i64;

    let s = a - b - c - d + e + f + g - h;
    debug_assert!(s >= 0);
    s as u32
}

/// Cancelable build with reusable scratch (fast path).
pub fn build_chunk_svo_sparse_cancelable_with_scratch(
    gen: &WorldGen,
    chunk_origin: [i32; 3],
    chunk_size: u32,
    cancel: &AtomicBool,
    scratch: &mut BuildScratch,
) -> Vec<NodeGpu> {
    if should_cancel(cancel) {
        return Vec::new();
    }

    let chunk_ox = chunk_origin[0];
    let chunk_oy = chunk_origin[1];
    let chunk_oz = chunk_origin[2];

    let cs_u = chunk_size;
    let cs_i = chunk_size as i32;
    debug_assert!(cs_u.is_power_of_two());

    let side = cs_u as usize;
    let vpm: i32 = config::VOXELS_PER_METER as i32;
    debug_assert!(vpm > 0);

    // -------------------------------------------------------------------------
    // Height cache (local array)
    // -------------------------------------------------------------------------
    let margin_m: i32 = 6;
    let margin: i32 = margin_m * vpm + (vpm - 1);

    let cache_x0 = chunk_ox - margin;
    let cache_z0 = chunk_oz - margin;
    let cache_x1 = chunk_ox + cs_i + margin; // inclusive
    let cache_z1 = chunk_oz + cs_i + margin; // inclusive

    let cache_w = (cache_x1 - cache_x0 + 1) as usize;
    let cache_h = (cache_z1 - cache_z0 + 1) as usize;

    scratch.ensure_height_cache(cache_w, cache_h);
    for z in 0..cache_h {
        if (z & 15) == 0 && should_cancel(cancel) {
            return Vec::new();
        }
        let wz = cache_z0 + z as i32;
        let row = z * cache_w;
        for x in 0..cache_w {
            let wx = cache_x0 + x as i32;
            scratch.height_cache[row + x] = gen.ground_height(wx, wz);
        }
    }

    let height_at = |wx: i32, wz: i32| -> i32 {
        if wx < cache_x0 || wx > cache_x1 || wz < cache_z0 || wz > cache_z1 {
            gen.ground_height(wx, wz)
        } else {
            let ix = (wx - cache_x0) as usize;
            let iz = (wz - cache_z0) as usize;
            scratch.height_cache[iz * scratch.height_cache_w + ix]
        }
    };


    // -------------------------------------------------------------------------
    // Tree cache/mask (fast O(1) material lookup)
    // -------------------------------------------------------------------------
    let (_tree_cache_unused, tree_mask) = gen.build_tree_cache_with_mask(
        chunk_ox,
        chunk_oy,
        chunk_oz,
        cs_i,
        &height_at,
        cancel,
    );

    // -------------------------------------------------------------------------
    // 2D maps (ground)
    // -------------------------------------------------------------------------
    BuildScratch::ensure_2d(&mut scratch.ground, side, 0);

    for lz in 0..cs_i {
        if (lz & 15) == 0 && should_cancel(cancel) {
            return Vec::new();
        }
        for lx in 0..cs_i {
            let wx = chunk_ox + lx;
            let wz = chunk_oz + lz;
            let g = height_at(wx, wz);

            let i = idx2(side, lx as usize, lz as usize);
            scratch.ground[i] = g;
        }
    }

    let ground_mip: MinMaxMipView<'_> = build_minmax_mip_inplace(
        &scratch.ground,
        cs_u,
        &mut scratch.ground_min_levels,
        &mut scratch.ground_max_levels,
    );

    // -------------------------------------------------------------------------
    // Tree top stamp (2D) for fast above everything pruning
    // -------------------------------------------------------------------------
    BuildScratch::ensure_2d(&mut scratch.tree_top, side, -1);

    let pad_m = 4;
    let xm0 = (chunk_ox.div_euclid(vpm)) - pad_m;
    let xm1 = ((chunk_ox + cs_i).div_euclid(vpm)) + pad_m;
    let zm0 = (chunk_oz.div_euclid(vpm)) - pad_m;
    let zm1 = ((chunk_oz + cs_i).div_euclid(vpm)) + pad_m;

    for zm in zm0..=zm1 {
        if ((zm - zm0) & 3) == 0 && should_cancel(cancel) {
            return Vec::new();
        }

        for xm in xm0..=xm1 {
            let Some((trunk_h_vox, crown_r_vox)) = gen.tree_instance_at_meter(xm, zm) else {
                continue;
            };

            let tx = xm * vpm;
            let tz = zm * vpm;

            let g = height_at(tx, tz);
            let trunk_base = g + vpm;
            let trunk_top = trunk_base + trunk_h_vox;

            let canopy_h_vox = 5 * vpm;
            let top_y = trunk_top + canopy_h_vox + 2 * vpm;

            let r = crown_r_vox + 2 * vpm;

            for dz in -r..=r {
                for dx in -r..=r {
                    if dx * dx + dz * dz > r * r {
                        continue;
                    }

                    let wx = tx + dx;
                    let wz = tz + dz;

                    let lx = wx - chunk_ox;
                    let lz = wz - chunk_oz;
                    if lx >= 0 && lx < cs_i && lz >= 0 && lz < cs_i {
                        let i = idx2(side, lx as usize, lz as usize);
                        scratch.tree_top[i] = scratch.tree_top[i].max(top_y);
                    }
                }
            }

            // ensure trunk column included
            let lx = tx - chunk_ox;
            let lz = tz - chunk_oz;
            if lx >= 0 && lx < cs_i && lz >= 0 && lz < cs_i {
                let i = idx2(side, lx as usize, lz as usize);
                scratch.tree_top[i] = scratch.tree_top[i].max(trunk_top);
            }
        }
    }

    let tree_mip: MaxMipView<'_> =
        build_max_mip_inplace(&scratch.tree_top, cs_u, &mut scratch.tree_levels);

    // -------------------------------------------------------------------------
    // Precompute per-voxel material + occupancy (terrain + trees only)
    // -------------------------------------------------------------------------
    BuildScratch::ensure_3d_u32(&mut scratch.material, side, AIR);

    let dirt_depth = 3 * vpm;

    for ly in 0..cs_i {
        if (ly & 7) == 0 && should_cancel(cancel) {
            return Vec::new();
        }

        let wy = chunk_oy + ly;

        for lz in 0..cs_i {
            for lx in 0..cs_i {
                let wx = chunk_ox + lx;
                let wz = chunk_oz + lz;

                let col = idx2(side, lx as usize, lz as usize);
                let g = scratch.ground[col];

                // base terrain + trees (FAST)
                let m: u32 = if wy < g {
                    if wy >= g - dirt_depth { DIRT } else { STONE }
                } else if wy == g {
                    let tm = tree_mask.material_fast(wx, wy, wz);
                    if tm == WOOD { WOOD } else { GRASS }
                } else {
                    let tm = tree_mask.material_fast(wx, wy, wz);
                    if tm != AIR { tm } else { AIR }
                };

                let i3 = idx3(side, lx as usize, ly as usize, lz as usize);
                scratch.material[i3] = m;
            }
        }
    }

    // -------------------------------------------------------------------------
    // Prefix sum over solid occupancy
    // -------------------------------------------------------------------------
    BuildScratch::ensure_prefix(&mut scratch.prefix, side);
    let dim = side + 1;

    for z in 1..=side {
        if (z & 7) == 0 && should_cancel(cancel) {
            return Vec::new();
        }
        for y in 1..=side {
            let mut run: u32 = 0;
            for x in 1..=side {
                let v = (scratch.material[idx3(side, x - 1, y - 1, z - 1)] != AIR) as u32;
                run += v;

                let a = scratch.prefix[pidx(dim, x, y, z - 1)] as i64;
                let b = scratch.prefix[pidx(dim, x, y - 1, z)] as i64;
                let c = scratch.prefix[pidx(dim, x, y - 1, z - 1)] as i64;

                let p = a + b - c + (run as i64);
                debug_assert!(p >= 0);
                scratch.prefix[pidx(dim, x, y, z)] = p as u32;
            }
        }
    }

    fn build_node(
        nodes: &mut Vec<NodeGpu>,
        ox: i32,
        oy: i32,
        oz: i32,
        size: i32,
        chunk_oy: i32,
        material: &[u32],
        prefix: &[u32],
        side: usize,
        ground_mip: &MinMaxMipView<'_>,
        tree_mip: &MaxMipView<'_>,
        dirt_depth: i32,
        cancel: &AtomicBool,
    ) -> NodeGpu {
        if should_cancel(cancel) {
            return NodeGpu { child_base: LEAF, child_mask: 0, material: AIR, _pad: 0 };
        }

        let size_u = size as u32;

        let (gmin, gmax) = ground_mip.query(ox, oz, size_u);
        let tmax = tree_mip.query_max(ox, oz, size_u);

        let y0 = chunk_oy + oy;
        let y1 = y0 + size - 1;

        // above everything (ground or tree top)
        let top_solid = gmax.max(tmax);
        if y0 > top_solid {
            return NodeGpu { child_base: LEAF, child_mask: 0, material: AIR, _pad: 0 };
        }

        // deep solid stone (below dirt band everywhere in this node footprint)
        if y1 < gmin - dirt_depth {
            return NodeGpu { child_base: LEAF, child_mask: 0, material: STONE, _pad: 0 };
        }

        // empty check via prefix
        let sx = ox as usize;
        let sy = oy as usize;
        let sz = oz as usize;
        let s = size as usize;

        let sum = prefix_sum_cube(prefix, side, sx, sy, sz, s);
        if sum == 0 {
            return NodeGpu { child_base: LEAF, child_mask: 0, material: AIR, _pad: 0 };
        }

        if size == 1 {
            let m = material[idx3(side, sx, sy, sz)];
            return NodeGpu { child_base: LEAF, child_mask: 0, material: m, _pad: 0 };
        }

        let half = size / 2;
        let mut child_roots: [NodeGpu; 8] =
            [NodeGpu { child_base: LEAF, child_mask: 0, material: AIR, _pad: 0 }; 8];

        for ci in 0..8 {
            if (ci & 3) == 0 && should_cancel(cancel) {
                return NodeGpu { child_base: LEAF, child_mask: 0, material: AIR, _pad: 0 };
            }

            let dx = if (ci & 1) != 0 { half } else { 0 };
            let dy = if (ci & 2) != 0 { half } else { 0 };
            let dz = if (ci & 4) != 0 { half } else { 0 };

            child_roots[ci] = build_node(
                nodes,
                ox + dx,
                oy + dy,
                oz + dz,
                half,
                chunk_oy,
                material,
                prefix,
                side,
                ground_mip,
                tree_mip,
                dirt_depth,
                cancel,
            );
        }

        let base = nodes.len() as u32;
        let mut mask: u32 = 0;

        for ci in 0..8 {
            if !is_empty_leaf(&child_roots[ci]) {
                mask |= 1u32 << ci;
                nodes.push(child_roots[ci]);
            }
        }

        if mask == 0 {
            return NodeGpu { child_base: LEAF, child_mask: 0, material: AIR, _pad: 0 };
        }

        NodeGpu { child_base: base, child_mask: mask, material: 0, _pad: 0 }
    }

    // Root must be at index 0 for GPU.
    let mut nodes = vec![NodeGpu { child_base: LEAF, child_mask: 0, material: AIR, _pad: 0 }];

    let root = build_node(
        &mut nodes,
        0,
        0,
        0,
        cs_i,
        chunk_oy,
        &scratch.material,
        &scratch.prefix,
        side,
        &ground_mip,
        &tree_mip,
        dirt_depth,
        cancel,
    );

    if should_cancel(cancel) {
        return Vec::new();
    }

    nodes[0] = root;
    nodes
}

// src/svo/mips.rs

pub struct MinMaxMipView<'a> {
    pub root_side: u32,
    pub min_levels: &'a [Vec<i32>],
    pub max_levels: &'a [Vec<i32>],
}

pub fn build_minmax_mip_inplace<'a>(
    base: &[i32],
    side: u32,
    min_levels: &'a mut Vec<Vec<i32>>,
    max_levels: &'a mut Vec<Vec<i32>>,
) -> MinMaxMipView<'a> {
    debug_assert!(side.is_power_of_two());
    debug_assert_eq!(base.len(), (side * side) as usize);

    let levels = side.trailing_zeros() as usize + 1;

    if min_levels.len() != levels {
        min_levels.resize_with(levels, Vec::new);
    }
    if max_levels.len() != levels {
        max_levels.resize_with(levels, Vec::new);
    }

    // lvl 0
    min_levels[0].clear();
    min_levels[0].extend_from_slice(base);

    max_levels[0].clear();
    max_levels[0].extend_from_slice(base);

    let mut cur_side = side;

    for lvl in 1..levels {
        let next_side = cur_side / 2;
        let need = (next_side * next_side) as usize;

        // --- min: borrow prev + out without aliasing
        {
            let (prev, rest) = min_levels.split_at_mut(lvl);
            let cur_min: &[i32] = &prev[lvl - 1];
            let mn: &mut Vec<i32> = &mut rest[0];
            mn.resize(need, 0);

            for z in 0..next_side {
                for x in 0..next_side {
                    let i00 = ((2 * z) * cur_side + (2 * x)) as usize;
                    let i10 = ((2 * z) * cur_side + (2 * x + 1)) as usize;
                    let i01 = ((2 * z + 1) * cur_side + (2 * x)) as usize;
                    let i11 = ((2 * z + 1) * cur_side + (2 * x + 1)) as usize;

                    let o = (z * next_side + x) as usize;

                    let a0 = cur_min[i00];
                    let a1 = cur_min[i10];
                    let a2 = cur_min[i01];
                    let a3 = cur_min[i11];

                    mn[o] = a0.min(a1).min(a2).min(a3);
                }
            }
        }

        // --- max: borrow prev + out without aliasing
        {
            let (prev, rest) = max_levels.split_at_mut(lvl);
            let cur_max: &[i32] = &prev[lvl - 1];
            let mx: &mut Vec<i32> = &mut rest[0];
            mx.resize(need, 0);

            for z in 0..next_side {
                for x in 0..next_side {
                    let i00 = ((2 * z) * cur_side + (2 * x)) as usize;
                    let i10 = ((2 * z) * cur_side + (2 * x + 1)) as usize;
                    let i01 = ((2 * z + 1) * cur_side + (2 * x)) as usize;
                    let i11 = ((2 * z + 1) * cur_side + (2 * x + 1)) as usize;

                    let o = (z * next_side + x) as usize;

                    let b0 = cur_max[i00];
                    let b1 = cur_max[i10];
                    let b2 = cur_max[i01];
                    let b3 = cur_max[i11];

                    mx[o] = b0.max(b1).max(b2).max(b3);
                }
            }
        }

        cur_side = next_side;
    }

    MinMaxMipView {
        root_side: side,
        min_levels: &min_levels[..],
        max_levels: &max_levels[..],
    }
}

impl<'a> MinMaxMipView<'a> {
    #[inline]
    pub fn query(&self, x0: i32, z0: i32, size: u32) -> (i32, i32) {
        debug_assert!(size.is_power_of_two());
        debug_assert!(size <= self.root_side);
        debug_assert!(x0 >= 0 && z0 >= 0);

        let level = size.trailing_zeros() as usize;
        debug_assert!(level < self.min_levels.len());

        let side = self.root_side >> level;
        let x = (x0 as u32) / size;
        let z = (z0 as u32) / size;
        let idx = (z * side + x) as usize;

        (self.min_levels[level][idx], self.max_levels[level][idx])
    }
}

pub struct MaxMipView<'a> {
    pub root_side: u32,
    pub levels: &'a [Vec<i32>],
}

pub fn build_max_mip_inplace<'a>(
    base: &[i32],
    side: u32,
    levels: &'a mut Vec<Vec<i32>>,
) -> MaxMipView<'a> {
    debug_assert!(side.is_power_of_two());
    debug_assert_eq!(base.len(), (side * side) as usize);

    let nlevels = side.trailing_zeros() as usize + 1;

    if levels.len() != nlevels {
        levels.resize_with(nlevels, Vec::new);
    }

    // lvl 0
    levels[0].clear();
    levels[0].extend_from_slice(base);

    let mut cur_side = side;

    for lvl in 1..nlevels {
        let next_side = cur_side / 2;
        let need = (next_side * next_side) as usize;

        let (prev, rest) = levels.split_at_mut(lvl);
        let cur: &[i32] = &prev[lvl - 1];
        let out: &mut Vec<i32> = &mut rest[0];
        out.resize(need, 0);

        for z in 0..next_side {
            for x in 0..next_side {
                let i00 = ((2 * z) * cur_side + (2 * x)) as usize;
                let i10 = ((2 * z) * cur_side + (2 * x + 1)) as usize;
                let i01 = ((2 * z + 1) * cur_side + (2 * x)) as usize;
                let i11 = ((2 * z + 1) * cur_side + (2 * x + 1)) as usize;

                let o = (z * next_side + x) as usize;
                out[o] = cur[i00].max(cur[i10]).max(cur[i01]).max(cur[i11]);
            }
        }

        cur_side = next_side;
    }

    MaxMipView {
        root_side: side,
        levels: &levels[..],
    }
}

impl<'a> MaxMipView<'a> {
    #[inline]
    pub fn query_max(&self, x0: i32, z0: i32, size: u32) -> i32 {
        debug_assert!(size.is_power_of_two());
        debug_assert!(size <= self.root_side);
        debug_assert!(x0 >= 0 && z0 >= 0);

        let level = size.trailing_zeros() as usize;
        debug_assert!(level < self.levels.len());

        let side = self.root_side >> level;
        let x = (x0 as u32) / size;
        let z = (z0 as u32) / size;
        let idx = (z * side + x) as usize;

        self.levels[level][idx]
    }
}


// src/svo/mod.rs
pub mod builder;
pub mod mips;

pub use builder::{
    BuildScratch,
    build_chunk_svo_sparse_cancelable_with_scratch,
};


// src/world/generator.rs

use noise::{Fbm, MultiFractal, NoiseFn, Perlin};

use crate::config;

#[derive(Clone)]
pub struct WorldGen {
    pub seed: u32,
    height: Fbm<Perlin>,
    detail: Fbm<Perlin>,
}

impl WorldGen {
    pub fn new(seed: u32) -> Self {
        let height = Fbm::<Perlin>::new(seed).set_octaves(7).set_frequency(0.010);
        let detail = Fbm::<Perlin>::new(seed ^ 0xA5A5_A5A5).set_octaves(3).set_frequency(0.02);

        Self { seed, height, detail }
    }

    pub fn ground_height(&self, x_vox: i32, z_vox: i32) -> i32 {
        let xm = x_vox as f64 * config::VOXEL_SIZE_M_F64;
        let zm = z_vox as f64 * config::VOXEL_SIZE_M_F64;

        let h0 = self.height.get([xm, zm]) as f32;
        let h1 = self.detail.get([xm, zm]) as f32;

        let base_m = 10.0;
        let amp_m = 18.0;
        let hills_m = h0 * amp_m + h1 * 3.0;

        let voxels_per_meter = (1.0 / config::VOXEL_SIZE_M_F64) as f32;
        ((base_m + hills_m) * voxels_per_meter).round() as i32
    }
}



// src/world/materials.rs
// ----------------------
// src/world/materials.rs
pub const AIR: u32 = 0;
pub const GRASS: u32 = 1;
pub const DIRT: u32 = 2;
pub const STONE: u32 = 3;
pub const WOOD: u32 = 4;
pub const LEAF: u32 = 5;


// src/world/mod.rs
pub mod generator;
pub mod materials;
pub mod hash;
pub mod trees;

pub use generator::WorldGen;

