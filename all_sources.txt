// src/streaming/cache/lru.rs
// --------------------------
use std::{collections::VecDeque, mem::size_of, sync::Arc};
use rustc_hash::FxHashMap as HashMap;

use crate::{config, render::gpu_types::{NodeGpu, NodeRopesGpu}};
use crate::streaming::types::ChunkKey;

#[derive(Clone)]
pub struct CachedChunk {
    pub nodes: Arc<[NodeGpu]>,
    pub ropes: Arc<[NodeRopesGpu]>,
    pub macro_words: Arc<[u32]>,
    pub colinfo_words: Arc<[u32]>,
    pub bytes: usize,
    pub stamp: u64,
}

pub struct ChunkCache {
    map: HashMap<ChunkKey, CachedChunk>,
    lru: VecDeque<(ChunkKey, u64)>,
    stamp: u64,
    bytes: usize,
}

impl ChunkCache {
    pub fn new() -> Self {
        Self { map: HashMap::default(), lru: VecDeque::default(), stamp: 1, bytes: 0 }
    }

    pub fn stats(&self) -> (usize, usize, usize) {
        (self.bytes, self.map.len(), self.lru.len())
    }

    pub fn get(&self, key: &ChunkKey) -> Option<&CachedChunk> { self.map.get(key) }

    pub fn touch(&mut self, key: ChunkKey) {
        if let Some(e) = self.map.get_mut(&key) {
            self.stamp = self.stamp.wrapping_add(1).max(1);
            e.stamp = self.stamp;
            self.lru.push_back((key, e.stamp));
            self.maybe_compact_lru();
        }
    }

    pub fn put(
        &mut self,
        key: ChunkKey,
        nodes: Arc<[NodeGpu]>,
        macro_words: Arc<[u32]>,
        ropes: Arc<[NodeRopesGpu]>,
        colinfo_words: Arc<[u32]>,
    ) {
        if let Some(old) = self.map.remove(&key) {
            self.bytes = self.bytes.saturating_sub(old.bytes);
        }

        self.stamp = self.stamp.wrapping_add(1).max(1);
        let stamp = self.stamp;

        let bytes =
            nodes.len() * size_of::<NodeGpu>()
            + ropes.len() * size_of::<NodeRopesGpu>()
            + macro_words.len() * size_of::<u32>()
            + colinfo_words.len() * size_of::<u32>();

        self.map.insert(
            key,
            CachedChunk { nodes, ropes, macro_words, colinfo_words, bytes, stamp },
        );

        self.bytes = self.bytes.saturating_add(bytes);
        self.lru.push_back((key, stamp));
        self.evict_as_needed();
    }

    pub fn remove(&mut self, key: &ChunkKey) {
        if let Some(old) = self.map.remove(key) {
            self.bytes = self.bytes.saturating_sub(old.bytes);
        }
    }

    fn evict_as_needed(&mut self) {
        let budget = config::CHUNK_CACHE_BUDGET_BYTES;
        while self.bytes > budget {
            let Some((k, stamp)) = self.lru.pop_front() else { break; };

            let should_evict = self.map.get(&k).map(|e| e.stamp == stamp).unwrap_or(false);
            if !should_evict { continue; }

            if let Some(ev) = self.map.remove(&k) {
                self.bytes = self.bytes.saturating_sub(ev.bytes);
            }
        }
    }

    fn maybe_compact_lru(&mut self) {
        let max = self.map.len().saturating_mul(8).max(1024);
        if self.lru.len() <= max { return; }

        let mut new = VecDeque::with_capacity(self.map.len());
        for (k, e) in self.map.iter() {
            new.push_back((*k, e.stamp));
        }
        self.lru = new;
    }
}

// src/streaming/cache/mod.rs
// --------------------------
mod lru;

pub use lru::{CachedChunk, ChunkCache};

// src/streaming/manager/build.rs
// ------------------------------
use std::sync::{
    atomic::{AtomicBool, Ordering},
    Arc,
};

use crossbeam_channel::TrySendError;
use glam::Vec3;

use crate::streaming::priority::sort_queue_near_first;
use crate::streaming::types::*;
use crate::{config, render::gpu_types::{NodeGpu, NodeRopesGpu}};

use super::{ground, keep, slots};
use super::ChunkManager;

#[inline(always)]
fn in_keep(mgr: &ChunkManager, center: ChunkKey, k: ChunkKey) -> bool {
    let dx = k.x - center.x;
    let dz = k.z - center.z;

    if dx < -config::KEEP_RADIUS || dx > config::KEEP_RADIUS { return false; }
    if dz < -config::KEEP_RADIUS || dz > config::KEEP_RADIUS { return false; }

    let Some(ground_cy) = ground::ground_cy_for_column(mgr, k.x, k.z) else {
        return false;
    };

    let dy = k.y - ground_cy;
    dy >= y_band_min() && dy <= y_band_max()
}

#[inline]
fn cancel_token(mgr: &mut ChunkManager, key: ChunkKey) -> Arc<AtomicBool> {
    mgr.build
        .cancels
        .entry(key)
        .or_insert_with(|| Arc::new(AtomicBool::new(false)))
        .clone()
}

#[inline]
fn queue_build_front(mgr: &mut ChunkManager, k: ChunkKey) {
    if mgr.build.queued_set.contains(&k) {
        if let Some(pos) = mgr.build.build_queue.iter().position(|x| *x == k) {
            mgr.build.build_queue.remove(pos);
            mgr.build.build_queue.push_front(k);
        }
        return;
    }

    mgr.build.chunks.insert(k, ChunkState::Queued);
    mgr.build.queued_set.insert(k);
    mgr.build.build_queue.push_front(k);
}

pub fn ensure_priority_box(mgr: &mut ChunkManager, center: ChunkKey) {
    let n = mgr.offsets.priority_offsets.len();

    for i in 0..n {
        // copy the tuple out; immutable borrow ends right here
        let (dx, dy, dz) = mgr.offsets.priority_offsets[i];

        let x = center.x + dx;
        let z = center.z + dz;

        let Some(ground_cy_col) = ground::ground_cy_for_column(mgr, x, z) else { continue; };
        let k = ChunkKey { x, y: ground_cy_col + dy, z };

        match mgr.build.chunks.get(&k) {
            Some(ChunkState::Resident(_))
            | Some(ChunkState::Uploading(_))
            | Some(ChunkState::Queued)
            | Some(ChunkState::Building) => {
                if matches!(mgr.build.chunks.get(&k), Some(ChunkState::Queued)) {
                    queue_build_front(mgr, k);
                }
            }
            None => {
                if mgr.cache.get(&k).is_some() {
                    let _ = try_promote_from_cache(mgr, center, k);
                    continue;
                }

                let c = cancel_token(mgr, k);
                c.store(false, Ordering::Relaxed);
                queue_build_front(mgr, k);
            }
        }
    }
}


pub fn enqueue_active_ring(mgr: &mut ChunkManager, center: ChunkKey) {
    let n_active = mgr.offsets.active_offsets.len();
    for i in 0..n_active {
        let (dx, dy, dz) = mgr.offsets.active_offsets[i];
        let x = center.x + dx;
        let z = center.z + dz;

        let Some(ground_cy_col) = ground::ground_cy_for_column(mgr, x, z) else {
            continue;
        };
        let k = ChunkKey { x, y: ground_cy_col + dy, z };

        match mgr.build.chunks.get(&k) {
            Some(ChunkState::Resident(_))
            | Some(ChunkState::Uploading(_))
            | Some(ChunkState::Queued)
            | Some(ChunkState::Building) => {}
            None => {
                if mgr.cache.get(&k).is_some() {
                    let _ = try_promote_from_cache(mgr, center, k);
                    continue;
                }

                let c = cancel_token(mgr, k);
                c.store(false, Ordering::Relaxed);

                mgr.build.chunks.insert(k, ChunkState::Queued);
                if mgr.build.queued_set.insert(k) {
                    mgr.build.build_queue.push_back(k);
                }
            }
        }
    }
}

pub fn unload_outside_keep(mgr: &mut ChunkManager, center: ChunkKey) {
    mgr.build.to_unload.clear();

    for &k in mgr.build.chunks.keys() {
        if slots::in_priority_box(mgr, center, k) { continue; }

        if keep::in_active_xz(center, k) {
            match mgr.build.chunks.get(&k) {
                Some(ChunkState::Resident(_)) | Some(ChunkState::Uploading(_)) => continue,
                _ => {}
            }
        }

        if !in_keep(mgr, center, k) {
            mgr.build.to_unload.push(k);
        }
    }

    let unload = std::mem::take(&mut mgr.build.to_unload);
    for k in unload {
        slots::unload_chunk(mgr, center, k);
    }

}

/// This is safe to run every frame; it just keeps the queue sane + near-sorted.
/// (Your old “center changed” special-case becomes “always maintain invariants”.)
pub fn on_center_change_resort(mgr: &mut ChunkManager, center: ChunkKey, cam_fwd: Vec3) {
    let origin = mgr.grid.grid_origin_chunk;
    let nx = (2 * config::KEEP_RADIUS + 1) as i32;

    let mut drop_keys: Vec<ChunkKey> = Vec::new();

    mgr.build.build_queue.retain(|k| {
        let keep_it =
            matches!(mgr.build.chunks.get(k), Some(ChunkState::Queued)) &&
            {
                let dx = k.x - center.x;
                let dz = k.z - center.z;
                !(dx < -config::KEEP_RADIUS || dx > config::KEEP_RADIUS ||
                  dz < -config::KEEP_RADIUS || dz > config::KEEP_RADIUS)
            } &&
            {
                let ix = k.x - origin[0];
                let iz = k.z - origin[2];
                !(ix < 0 || iz < 0 || ix >= nx || iz >= nx)
            } &&
            {
                let idx = ((k.z - origin[2]) * nx + (k.x - origin[0])) as usize;
                mgr.ground.col_ground_cy.get(idx).is_some()
            } &&
            {
                let idx = ((k.z - origin[2]) * nx + (k.x - origin[0])) as usize;
                let ground_cy = mgr.ground.col_ground_cy[idx];
                let dy = k.y - ground_cy;
                dy >= y_band_min() && dy <= y_band_max()
            };

        if !keep_it {
            drop_keys.push(*k);
        }
        keep_it
    });

    // remove dropped queued entries from state (prevents orphanQ explosion)
    for k in drop_keys {
        // mark cancel + remove state + remove cancel token
        if let Some(c) = mgr.build.cancels.get(&k) {
            c.store(true, std::sync::atomic::Ordering::Relaxed);
        }
        mgr.build.chunks.remove(&k);
        mgr.build.cancels.remove(&k);
        mgr.build.queued_set.remove(&k);
    }

    mgr.build.queued_set.clear();
    mgr.build.queued_set.extend(mgr.build.build_queue.iter().copied());

    // Priority box always first, then sort the rest near-first.
    let mut pri = std::collections::VecDeque::new();
    let mut rest = std::collections::VecDeque::new();

    while let Some(k) = mgr.build.build_queue.pop_front() {
        if slots::in_priority_box(mgr, center, k) {
            pri.push_back(k);
        } else {
            rest.push_back(k);
        }
    }

    sort_queue_near_first(&mut rest, center, cam_fwd);

    mgr.build.build_queue = pri;
    mgr.build.build_queue.extend(rest);
}

pub fn dispatch_builds(mgr: &mut ChunkManager, center: ChunkKey) {
    let mut attempts = 0usize;
    let max_attempts = mgr.build.build_queue.len().max(1);

    while mgr.build.in_flight < config::MAX_IN_FLIGHT && attempts < max_attempts {
        attempts += 1;

        let Some(k) = mgr.build.build_queue.pop_front() else { break; };
        mgr.build.queued_set.remove(&k);

        if !in_keep(mgr, center, k) {
            cancel_token(mgr, k).store(true, Ordering::Relaxed);
            mgr.build.chunks.remove(&k);
            mgr.build.cancels.remove(&k);
            continue;
        }

        if mgr.cache.get(&k).is_some() {
            mgr.build.chunks.remove(&k);
            mgr.build.cancels.remove(&k);
            let _ = try_promote_from_cache(mgr, center, k);
            continue;
        }

        if matches!(mgr.build.chunks.get(&k), Some(ChunkState::Queued)) {
            mgr.build.chunks.insert(k, ChunkState::Building);

            let cancel = cancel_token(mgr, k);
            cancel.store(false, Ordering::Relaxed);

            match mgr.build.tx_job.try_send(BuildJob { key: k, cancel: cancel.clone() }) {
                Ok(()) => mgr.build.in_flight += 1,
                Err(TrySendError::Full(_)) | Err(TrySendError::Disconnected(_)) => {
                    mgr.build.chunks.insert(k, ChunkState::Queued);
                    mgr.build.build_queue.push_front(k);
                    mgr.build.queued_set.insert(k);
                    break;
                }
            }
        }
    }
}

pub fn harvest_done_builds(mgr: &mut ChunkManager, center: ChunkKey) {
    let done_backlog = mgr.build.rx_done.len();
    let max_done = (16 + done_backlog / 2).clamp(16, 64);

    let mut done_count = 0usize;
    while done_count < max_done {
        let Ok(done) = mgr.build.rx_done.try_recv() else { break; };
        done_count += 1;

        if mgr.build.in_flight > 0 {
            mgr.build.in_flight -= 1;
        }

        // Drop stale completions (job from an old cancel token)
        let Some(cur_cancel) = mgr.build.cancels.get(&done.key) else { continue; };
        if !Arc::ptr_eq(cur_cancel, &done.cancel) { continue; }

        if done.canceled || done.cancel.load(Ordering::Relaxed) {
            mgr.build.chunks.remove(&done.key);
            mgr.build.cancels.remove(&done.key);
            continue;
        }

        if !in_keep(mgr, center, done.key) {
            cancel_token(mgr, done.key).store(true, Ordering::Relaxed);
            mgr.build.chunks.remove(&done.key);
            continue;
        }

        on_build_done(
            mgr,
            center,
            done.key,
            done.nodes,
            done.macro_words,
            done.ropes,
            done.colinfo_words,
        );
    }
}

fn try_promote_from_cache(mgr: &mut ChunkManager, center: ChunkKey, key: ChunkKey) -> bool {
    // If priority box isn't ready, defer non-priority promotions.
    if !slots::priority_box_ready(mgr, center) && !slots::in_priority_box(mgr, center, key) {
        mgr.build.chunks.insert(key, ChunkState::Queued);
        if mgr.build.queued_set.insert(key) {
            mgr.build.build_queue.push_back(key);
        }
        return false;
    }

    let Some(e) = mgr.cache.get(&key) else { return false; };

    let nodes = e.nodes.clone();
    let macro_words = e.macro_words.clone();
    let ropes = e.ropes.clone();
    let colinfo_words = e.colinfo_words.clone();

    mgr.cache.touch(key);
    slots::try_make_uploading(mgr, center, key, nodes, macro_words, ropes, colinfo_words)
}

fn on_build_done(
    mgr: &mut ChunkManager,
    center: ChunkKey,
    key: ChunkKey,
    nodes: Vec<NodeGpu>,
    macro_words: Vec<u32>,
    ropes: Vec<NodeRopesGpu>,
    colinfo_words: Vec<u32>,
) {
    if let Some(c) = mgr.build.cancels.get(&key) {
        if c.load(Ordering::Relaxed) {
            mgr.build.chunks.remove(&key);
            return;
        }
    }

    let nodes_arc: Arc<[NodeGpu]> = nodes.into();
    let macro_arc: Arc<[u32]> = macro_words.into();
    let ropes_arc: Arc<[NodeRopesGpu]> = ropes.into();
    let colinfo_arc: Arc<[u32]> = colinfo_words.into();

    mgr.cache.put(key, nodes_arc.clone(), macro_arc.clone(), ropes_arc.clone(), colinfo_arc.clone());

    if matches!(mgr.build.chunks.get(&key), Some(ChunkState::Resident(_))) {
        return;
    }

    // Defer GPU upload for non-priority until priority box is GPU-ready.
    if !slots::priority_box_ready(mgr, center) && !slots::in_priority_box(mgr, center, key) {
        mgr.build.chunks.insert(key, ChunkState::Queued);
        if mgr.build.queued_set.insert(key) {
            mgr.build.build_queue.push_back(key);
        }
        return;
    }

    let ok = slots::try_make_uploading(mgr, center, key, nodes_arc, macro_arc, ropes_arc, colinfo_arc);
    if !ok {
        mgr.build.chunks.remove(&key);
    }
}

// src/streaming/manager/grid.rs
// -----------------------------
use crate::{config, streaming::types::*};

use super::ChunkManager;

pub fn rebuild_if_dirty(mgr: &mut ChunkManager, center: ChunkKey) -> bool {
    let changed = mgr.grid.grid_dirty;
    if mgr.grid.grid_dirty {
        rebuild_grid(mgr, center);
        mgr.grid.grid_dirty = false;
    }
    changed
}

pub fn rebuild_grid(mgr: &mut ChunkManager, center: ChunkKey) {
    let nx = (2 * config::KEEP_RADIUS + 1) as u32;
    let nz = nx;
    let ny = GRID_Y_COUNT;

    mgr.grid.grid_dims = [nx, ny, nz];
    mgr.grid.grid_origin_chunk = super::keep::keep_origin_for(center);

    let needed = (nx * ny * nz) as usize;
    if mgr.grid.chunk_grid.len() != needed {
        mgr.grid.chunk_grid.resize(needed, INVALID_U32);
    }
    mgr.grid.chunk_grid.fill(INVALID_U32);

    // Include any slot that is actually usable by the GPU.
    for slot in 0..mgr.slots.slot_to_key.len() {
        let k = mgr.slots.slot_to_key[slot];

        let ready = match mgr.build.chunks.get(&k) {
            Some(ChunkState::Resident(_)) => true,
            Some(ChunkState::Uploading(up)) => up.uploaded, // uploaded => meta+macro/colinfo+nodes are on GPU
            _ => false,
        };

        if !ready {
            continue;
        }

        if let Some(idx) = grid_index_for_chunk(mgr, k) {
            mgr.grid.chunk_grid[idx] = slot as u32;
        }
    }
}

#[inline]
fn grid_index_for_chunk(mgr: &ChunkManager, k: ChunkKey) -> Option<usize> {
    let [ox, oy, oz] = mgr.grid.grid_origin_chunk;
    let [nx, ny, nz] = mgr.grid.grid_dims;

    let ix = k.x - ox;
    let iy = k.y - oy;
    let iz = k.z - oz;

    if ix < 0 || iy < 0 || iz < 0 {
        return None;
    }

    let ix = ix as u32;
    let iy = iy as u32;
    let iz = iz as u32;

    if ix >= nx || iy >= ny || iz >= nz {
        return None;
    }

    let idx = (iz * ny * nx) + (iy * nx) + ix;
    Some(idx as usize)
}

// src/streaming/manager/ground.rs
// -------------------------------
use crate::streaming::types::*;
use crate::{config, world::WorldGen};
use super::{ChunkManager};
use super::keep;

#[inline]
fn compute_ground_cy_at_column(world: &WorldGen, cx: i32, cz: i32) -> i32 {
    let cs = config::CHUNK_SIZE as i32;
    let half = cs / 2;
    let wx = cx * cs + half;
    let wz = cz * cs + half;
    let ground_y_vox = world.ground_height(wx, wz);
    ground_y_vox.div_euclid(cs)
}

pub fn ensure_column_cache(mgr: &mut ChunkManager, world: &WorldGen, center: ChunkKey) {
    let new_origin = keep::keep_origin_for(center);
    let origin_changed = mgr.ground.col_ground_cy.is_empty() || new_origin != mgr.grid.grid_origin_chunk;

    if !origin_changed {
        return;
    }

    let old_origin = mgr.grid.grid_origin_chunk;

    // publish new origin first (so indexing uses it)
    mgr.grid.grid_origin_chunk = new_origin;

    update_column_ground_cache(mgr, world, old_origin, new_origin);
    mgr.grid.grid_dirty = true;
}

pub fn ground_cy_for_column(mgr: &ChunkManager, cx: i32, cz: i32) -> Option<i32> {
    let [ox, _, oz] = mgr.grid.grid_origin_chunk;

    let nx = (2 * config::KEEP_RADIUS + 1) as i32;
    let nz = nx;

    let ix = cx - ox;
    let iz = cz - oz;
    if ix < 0 || iz < 0 || ix >= nx || iz >= nz { return None; }

    let idx = (iz * nx + ix) as usize;
    mgr.ground.col_ground_cy.get(idx).copied()
}

fn update_column_ground_cache(
    mgr: &mut ChunkManager,
    world: &WorldGen,
    old_origin: [i32; 3],
    new_origin: [i32; 3],
) {
    let nx = (2 * config::KEEP_RADIUS + 1) as i32;
    let nz = nx;
    let len = (nx * nz) as usize;

    // first time / resize => full rebuild
    if mgr.ground.col_ground_cy.len() != len || mgr.ground.col_ground_cy.is_empty() {
        mgr.ground.col_ground_cy.resize(len, 0);
        let ox = new_origin[0];
        let oz = new_origin[2];
        for dz in 0..nz {
            for dx in 0..nx {
                let cx = ox + dx;
                let cz = oz + dz;
                mgr.ground.col_ground_cy[(dz * nx + dx) as usize] =
                    compute_ground_cy_at_column(world, cx, cz);
            }
        }
        return;
    }

    let dx_chunks = new_origin[0] - old_origin[0];
    let dz_chunks = new_origin[2] - old_origin[2];

    if dx_chunks == 0 && dz_chunks == 0 {
        return;
    }

    // teleport => rebuild
    if dx_chunks.abs() >= nx || dz_chunks.abs() >= nz {
        let ox = new_origin[0];
        let oz = new_origin[2];
        for dz in 0..nz {
            for dx in 0..nx {
                let cx = ox + dx;
                let cz = oz + dz;
                mgr.ground.col_ground_cy[(dz * nx + dx) as usize] =
                    compute_ground_cy_at_column(world, cx, cz);
            }
        }
        return;
    }

    let old = std::mem::take(&mut mgr.ground.col_ground_cy);
    let mut newv = vec![0i32; len];

    let ox_new = new_origin[0];
    let oz_new = new_origin[2];

    for iz in 0..nz {
        for ix in 0..nx {
            let sx = ix - dx_chunks;
            let sz = iz - dz_chunks;

            let dst_idx = (iz * nx + ix) as usize;

            if sx >= 0 && sx < nx && sz >= 0 && sz < nz {
                let src_idx = (sz * nx + sx) as usize;
                newv[dst_idx] = old[src_idx];
            } else {
                let cx = ox_new + ix;
                let cz = oz_new + iz;
                newv[dst_idx] = compute_ground_cy_at_column(world, cx, cz);
            }
        }
    }

    mgr.ground.col_ground_cy = newv;
}

// src/streaming/manager/keep.rs
// -----------------------------
use glam::Vec3;
use crate::{config, world::WorldGen};
use crate::streaming::types::*;

use super::ChunkManager;

#[inline]
pub fn build_offsets(radius: i32) -> Vec<(i32, i32, i32)> {
    let mut v = Vec::new();
    v.reserve((GRID_Y_COUNT as usize) * ((2 * radius + 1) as usize) * ((2 * radius + 1) as usize));

    for dy in GRID_Y_MIN_DY..=(GRID_Y_MIN_DY + GRID_Y_COUNT as i32 - 1) {
        for dz in -radius..=radius {
            for dx in -radius..=radius {
                v.push((dx, dy, dz));
            }
        }
    }
    v
}

#[inline]
pub fn compute_center(world: &WorldGen, cam_pos_m: Vec3) -> ChunkKey {
    let cam_vx = (cam_pos_m.x / config::VOXEL_SIZE_M_F32).floor() as i32;
    let cam_vz = (cam_pos_m.z / config::VOXEL_SIZE_M_F32).floor() as i32;

    let cs = config::CHUNK_SIZE as i32;
    let half = cs / 2;

    let ccx = cam_vx.div_euclid(cs);
    let ccz = cam_vz.div_euclid(cs);

    let wx = ccx * cs + half;
    let wz = ccz * cs + half;
    let ground_y_vox = world.ground_height(wx, wz);
    let ground_cy = ground_y_vox.div_euclid(cs);

    ChunkKey { x: ccx, y: ground_cy, z: ccz }
}

#[inline]
pub fn keep_origin_for(center: ChunkKey) -> [i32; 3] {
    let ox = center.x - config::KEEP_RADIUS;
    let oz = center.z - config::KEEP_RADIUS;
    let oy = center.y + GRID_Y_MIN_DY;
    [ox, oy, oz]
}

#[inline(always)]
pub fn in_active_xz(center: ChunkKey, k: ChunkKey) -> bool {
    let dx = (k.x - center.x).abs();
    let dz = (k.z - center.z).abs();
    dx <= config::ACTIVE_RADIUS.max(PRIORITY_RADIUS) && dz <= config::ACTIVE_RADIUS.max(PRIORITY_RADIUS)
}

#[inline(always)]
pub fn in_priority_xz(center: ChunkKey, k: ChunkKey) -> bool {
    let dx = (k.x - center.x).abs();
    let dz = (k.z - center.z).abs();
    dx <= PRIORITY_RADIUS && dz <= PRIORITY_RADIUS
}

/// publish new center; if changed, rebucket uploads.
pub fn publish_center_and_rebucket(mgr: &mut ChunkManager, center: ChunkKey) -> bool {
    let changed = mgr.build.last_center.map_or(true, |c| c != center);
    if changed {
        mgr.build.last_center = Some(center);
        super::uploads::rebucket_for_center(mgr, center);
    }
    changed
}

// src/streaming/manager/mod.rs
// ----------------------------

mod build;
mod grid;
mod slots;
mod stats;

mod ground;
mod keep;
mod uploads;


use std::{
    collections::VecDeque,
    mem::size_of,
    sync::{
        atomic::{AtomicBool, Ordering},
        Arc,
    },
};

use crossbeam_channel::{bounded, Receiver, Sender, TrySendError};
use glam::Vec3;
use rustc_hash::{FxHashMap as HashMap, FxHashSet as HashSet};

use crate::{
    config,
    render::gpu_types::{ChunkMetaGpu, NodeGpu, NodeRopesGpu},
    world::WorldGen,
    streaming::types::StreamStats,
};

use crate::streaming::{
    NodeArena,
    cache::ChunkCache,
    priority::sort_queue_near_first,
    types::*,
    workers::spawn_workers,
};

/// Build-related state bucket.
pub(crate) struct BuildState {
    pub chunks: HashMap<ChunkKey, ChunkState>,

    pub build_queue: VecDeque<ChunkKey>,
    pub queued_set: HashSet<ChunkKey>,
    pub cancels: HashMap<ChunkKey, Arc<AtomicBool>>,

    pub tx_job: Sender<BuildJob>,
    pub rx_done: Receiver<BuildDone>,
    pub in_flight: usize,

    pub last_center: Option<ChunkKey>,
    pub to_unload: Vec<ChunkKey>,
}

/// Slot-residency bucket.
pub(crate) struct SlotState {
    pub slot_to_key: Vec<ChunkKey>,
    pub chunk_meta: Vec<ChunkMetaGpu>,
    pub slot_macro: Vec<Arc<[u32]>>,
    pub slot_colinfo: Vec<Arc<[u32]>>,
    pub resident_slots: usize,
}

/// Upload bucket.
pub(crate) struct UploadState {
    pub uploads_rewrite: VecDeque<ChunkUpload>,
    pub uploads_active:  VecDeque<ChunkUpload>,
    pub uploads_other:   VecDeque<ChunkUpload>,
}

/// Grid bucket.
pub(crate) struct GridState {
    pub grid_dirty: bool,
    pub grid_origin_chunk: [i32; 3],
    pub grid_dims: [u32; 3],
    pub chunk_grid: Vec<u32>,
}

/// Column ground cache bucket.
pub(crate) struct GroundState {
    pub col_ground_cy: Vec<i32>,
}

/// Offset precomputes bucket.
pub(crate) struct Offsets {
    pub active_offsets: Vec<(i32, i32, i32)>,
    pub priority_offsets: Vec<(i32, i32, i32)>,
}

pub struct ChunkManager {
    pub(crate) build: BuildState,
    pub(crate) slots: SlotState,
    pub(crate) uploads: UploadState,
    pub(crate) grid: GridState,
    pub(crate) ground: GroundState,
    pub(crate) offsets: Offsets,

    pub(crate) arena: NodeArena,
    pub(crate) cache: ChunkCache,
}

impl ChunkManager {
    pub fn new(gen: Arc<WorldGen>) -> Self {
        let cap = (config::MAX_IN_FLIGHT * 2).max(8);

        let (tx_job, rx_job) = bounded::<BuildJob>(cap);
        let (tx_done, rx_done) = bounded::<BuildDone>(cap);

        spawn_workers(gen, rx_job, tx_done);

        let node_capacity = (config::NODE_BUDGET_BYTES / size_of::<NodeGpu>()) as u32;

        let nx = (2 * config::KEEP_RADIUS + 1) as u32;
        let nz = nx;
        let ny = GRID_Y_COUNT;

        let grid_len = (nx * ny * nz) as usize;

        let active_offsets = keep::build_offsets(config::ACTIVE_RADIUS);
        let priority_offsets = keep::build_offsets(PRIORITY_RADIUS);

        Self {
            build: BuildState {
                chunks: HashMap::default(),
                build_queue: VecDeque::new(),
                queued_set: HashSet::default(),
                cancels: HashMap::default(),
                tx_job,
                rx_done,
                in_flight: 0,
                last_center: None,
                to_unload: Vec::new(),
            },
            slots: SlotState {
                slot_to_key: Vec::new(),
                chunk_meta: Vec::new(),
                slot_macro: Vec::new(),
                slot_colinfo: Vec::new(),
                resident_slots: 0,
            },
            uploads: UploadState {
                uploads_rewrite: VecDeque::new(),
                uploads_active:  VecDeque::new(),
                uploads_other:   VecDeque::new(),
            },
            grid: GridState {
                grid_dirty: true,
                grid_origin_chunk: [0, 0, 0],
                grid_dims: [nx, ny, nz],
                chunk_grid: vec![INVALID_U32; grid_len],
            },
            ground: GroundState { col_ground_cy: Vec::new() },
            offsets: Offsets { active_offsets, priority_offsets },

            arena: NodeArena::new(node_capacity),
            cache: ChunkCache::new(),
        }
    }

    /// Main frame update (same logic as before; now calls into submodules).
    pub fn update(&mut self, world: &Arc<WorldGen>, cam_pos_m: Vec3, cam_fwd: Vec3) -> bool {
        // 1) compute 
        let center = {
            let cam_vx = (cam_pos_m.x / config::VOXEL_SIZE_M_F32).floor() as i32;
            let cam_vz = (cam_pos_m.z / config::VOXEL_SIZE_M_F32).floor() as i32;

            let cs = config::CHUNK_SIZE as i32;
            let half = cs / 2;

            let ccx = cam_vx.div_euclid(cs);
            let ccz = cam_vz.div_euclid(cs);

            // Fast path: use cached column ground if available
            if let Some(ground_cy) = ground::ground_cy_for_column(self, ccx, ccz) {
                ChunkKey { x: ccx, y: ground_cy, z: ccz }
            } else {
                // Slow fallback (only when outside cache / first frame)
                keep::compute_center( world.as_ref(), cam_pos_m)
            }
        };
        
        // 2) ensure ground cache + publish origin
        ground::ensure_column_cache(self, world.as_ref(), center);

        // 3) publish center (rebucket uploads on center change)
        let center_changed = keep::publish_center_and_rebucket(self, center);

        // 4) ensure priority box builds/promotions
        build::ensure_priority_box(self, center);

        // 5) enqueue active offsets
        build::enqueue_active_ring(self, center);

        // 6) unload outside keep
        build::unload_outside_keep(self, center);

        // 7) handle center-change cleanup/sort
        if center_changed {
            build::on_center_change_resort(self, center, cam_fwd);
        }

        // 8) dispatch builds
        build::dispatch_builds(self, center);

        // 9) harvest completions
        build::harvest_done_builds(self, center);

        // 10) rebuild grid if dirty
        grid::rebuild_if_dirty(self, center)
    }


    // --- Public API (same signatures; implemented in submodules via impl blocks) ---

    pub fn chunk_count(&self) -> u32 { self.slots.resident_slots as u32 }
    pub fn grid_origin(&self) -> [i32; 3] { self.grid.grid_origin_chunk }
    pub fn grid_dims(&self) -> [u32; 3] { self.grid.grid_dims }
    pub fn chunk_grid(&self) -> &[u32] { &self.grid.chunk_grid }

    pub fn take_uploads(&mut self) -> Vec<ChunkUpload> {
        uploads::take_all(self)
    }

    pub fn take_uploads_budgeted(&mut self) -> Vec<ChunkUpload> {
        uploads::take_budgeted(self)
    }

    pub fn commit_uploads_applied(&mut self, applied: &[ChunkUpload]) -> bool {
        slots::commit_uploads_applied(self, applied)
    }

    pub fn stats(&self) -> Option<StreamStats> {
        stats::stats(self)
    }
}

// src/streaming/manager/slots.rs
// ------------------------------
use std::sync::{
    atomic::{AtomicBool, Ordering},
    Arc,
};

use crate::{
    config,
    render::gpu_types::{ChunkMetaGpu, NodeGpu, NodeRopesGpu},
    streaming::types::*,
};

use super::{ground, keep};
use super::ChunkManager;

#[inline(always)]
pub fn in_priority_box(mgr: &ChunkManager, center: ChunkKey, k: ChunkKey) -> bool {
    if !keep::in_priority_xz(center, k) {
        return false;
    }
    let Some(ground_cy) = ground::ground_cy_for_column(mgr, k.x, k.z) else {
        return false;
    };
    let dy = k.y - ground_cy;
    dy >= GRID_Y_MIN_DY && dy <= (GRID_Y_MIN_DY + GRID_Y_COUNT as i32 - 1)
}

#[inline(always)]
fn in_keep(mgr: &ChunkManager, center: ChunkKey, k: ChunkKey) -> bool {
    let dx = k.x - center.x;
    let dz = k.z - center.z;

    if dx < -config::KEEP_RADIUS || dx > config::KEEP_RADIUS {
        return false;
    }
    if dz < -config::KEEP_RADIUS || dz > config::KEEP_RADIUS {
        return false;
    }

    let Some(ground_cy) = ground::ground_cy_for_column(mgr, k.x, k.z) else {
        return false;
    };

    let dy = k.y - ground_cy;
    dy >= y_band_min() && dy <= y_band_max()
}

#[inline(always)]
pub fn is_gpu_ready(mgr: &ChunkManager, k: ChunkKey) -> bool {
    match mgr.build.chunks.get(&k) {
        Some(ChunkState::Resident(_)) => true,
        Some(ChunkState::Uploading(up)) => up.uploaded,
        _ => false,
    }
}

pub fn priority_box_ready(mgr: &ChunkManager, center: ChunkKey) -> bool {
    for &(dx, dy, dz) in &mgr.offsets.priority_offsets {
        let x = center.x + dx;
        let z = center.z + dz;

        let Some(ground_cy) = ground::ground_cy_for_column(mgr, x, z) else {
            continue;
        };
        let k = ChunkKey { x, y: ground_cy + dy, z };

        if !is_gpu_ready(mgr, k) {
            return false;
        }
    }
    true
}

#[inline]
fn cancel_token(mgr: &mut ChunkManager, key: ChunkKey) -> Arc<AtomicBool> {
    mgr.build
        .cancels
        .entry(key)
        .or_insert_with(|| Arc::new(AtomicBool::new(false)))
        .clone()
}

pub fn commit_uploads_applied(mgr: &mut ChunkManager, applied: &[ChunkUpload]) -> bool {
    let mut any_promoted = false;
    let mut any_uploaded_flip = false;

    // Mark uploads as applied; detect first-time flip.
    for u in applied {
        if !u.completes_residency {
            continue;
        }
        if let Some(ChunkState::Uploading(up)) = mgr.build.chunks.get_mut(&u.key) {
            if !up.uploaded {
                up.uploaded = true;
                any_uploaded_flip = true;
            }
        }
    }

    let center_opt = mgr.build.last_center;
    let priority_gate = center_opt
        .map(|c| !priority_box_ready(mgr, c))
        .unwrap_or(false);

    loop {
        if mgr.slots.resident_slots >= mgr.slots.slot_to_key.len() {
            break;
        }

        // If the next slot is ready, promote it directly.
        let next_key = mgr.slots.slot_to_key[mgr.slots.resident_slots];
        let next_ready = matches!(
            mgr.build.chunks.get(&next_key),
            Some(ChunkState::Uploading(Uploading { uploaded: true, .. }))
        );

        if !next_ready {
            // Find the best READY uploading chunk in [resident_slots..].
            let mut best: Option<(f32, usize)> = None;

            for s in mgr.slots.resident_slots..mgr.slots.slot_to_key.len() {
                let k = mgr.slots.slot_to_key[s];

                if priority_gate {
                    let c = center_opt.unwrap();
                    if !in_priority_box(mgr, c, k) {
                        continue;
                    }
                }

                let Some(ChunkState::Uploading(up)) = mgr.build.chunks.get(&k) else { continue; };
                if !up.uploaded {
                    continue;
                }

                let mut score = if let Some(c) = center_opt {
                    super::uploads::upload_dist_score(k, c)
                } else {
                    s as f32
                };

                if let Some(c) = center_opt {
                    if keep::in_active_xz(c, k) {
                        score -= 10_000.0;
                    }
                    if in_priority_box(mgr, c, k) {
                        score -= 20_000.0;
                    }
                }

                if best.map_or(true, |(bs, _)| score < bs) {
                    best = Some((score, s));
                }
            }

            let Some((_, best_slot)) = best else {
                break;
            };

            if best_slot != mgr.slots.resident_slots {
                swap_slots(mgr, best_slot, mgr.slots.resident_slots);

                // Slot changed => rewrite that slot ASAP.
                enqueue_slot_rewrite(mgr, mgr.slots.resident_slots);

                // If swapped-out chunk was uploaded too, rewrite it as well.
                let swapped_out_key = mgr.slots.slot_to_key[best_slot];
                let swapped_out_uploaded = matches!(
                    mgr.build.chunks.get(&swapped_out_key),
                    Some(ChunkState::Uploading(Uploading { uploaded: true, .. }))
                );
                if swapped_out_uploaded {
                    enqueue_slot_rewrite(mgr, best_slot);
                }
            }
        }

        // Now the frontier slot should be ready; promote it.
        let slot = mgr.slots.resident_slots;
        let key = mgr.slots.slot_to_key[slot];

        if priority_gate {
            let c = center_opt.unwrap();
            if !in_priority_box(mgr, c, key) {
                break;
            }
        }

        let ready = matches!(
            mgr.build.chunks.get(&key),
            Some(ChunkState::Uploading(Uploading { uploaded: true, .. }))
        );
        if !ready {
            break;
        }

        if let Some(st) = mgr.build.chunks.get_mut(&key) {
            if let ChunkState::Uploading(up) = st {
                let slot_u32 = up.slot;
                let node_base = up.node_base;
                let node_count = up.node_count;
                *st = ChunkState::Resident(Resident { slot: slot_u32, node_base, node_count });
                mgr.slots.resident_slots += 1;
                any_promoted = true;
                continue;
            }
        }

        break;
    }

    if any_promoted || any_uploaded_flip {
        mgr.grid.grid_dirty = true;
        if let Some(center) = mgr.build.last_center {
            super::grid::rebuild_grid(mgr, center);
            mgr.grid.grid_dirty = false;
        }
        return true;
    }

    false
}

pub fn try_make_uploading(
    mgr: &mut ChunkManager,
    center: ChunkKey,
    key: ChunkKey,
    nodes: Arc<[NodeGpu]>,
    macro_words: Arc<[u32]>,
    ropes: Arc<[NodeRopesGpu]>,
    colinfo_words: Arc<[u32]>,
) -> bool {
    if matches!(mgr.build.chunks.get(&key), Some(ChunkState::Resident(_))) {
        return true;
    }

    let need = nodes.len() as u32;
    if need == 0 {
        mgr.build.chunks.remove(&key);
        return false;
    }
    if macro_words.len() != MACRO_WORDS_PER_CHUNK_USIZE {
        mgr.build.chunks.remove(&key);
        return false;
    }
    if ropes.len() != nodes.len() {
        mgr.build.chunks.remove(&key);
        return false;
    }
    if colinfo_words.len() != COLINFO_WORDS_PER_CHUNK_USIZE {
        mgr.build.chunks.remove(&key);
        return false;
    }

    let mut node_base = mgr.arena.alloc(need);
    if node_base.is_none() {
        for _ in 0..EVICT_ATTEMPTS {
            if !evict_one_farthest(mgr, center, key) {
                break;
            }
            node_base = mgr.arena.alloc(need);
            if node_base.is_some() {
                break;
            }
        }
    }

    let Some(node_base) = node_base else {
        mgr.build.chunks.remove(&key);
        return false;
    };

    let slot = mgr.slots.slot_to_key.len() as u32;
    mgr.slots.slot_to_key.push(key);

    mgr.slots.slot_macro.push(macro_words.clone());
    mgr.slots.slot_colinfo.push(colinfo_words.clone());

    let macro_base = slot * MACRO_WORDS_PER_CHUNK;
    let colinfo_base = slot * COLINFO_WORDS_PER_CHUNK;

    let origin_vox = [
        key.x * config::CHUNK_SIZE as i32,
        key.y * config::CHUNK_SIZE as i32,
        key.z * config::CHUNK_SIZE as i32,
    ];

    let meta = ChunkMetaGpu {
        origin: [origin_vox[0], origin_vox[1], origin_vox[2], 0],
        node_base,
        node_count: need,
        macro_base,
        colinfo_base,
    };

    mgr.slots.chunk_meta.push(meta);

    mgr.build.chunks.insert(
        key,
        ChunkState::Uploading(Uploading { slot, node_base, node_count: need, uploaded: false }),
    );

    super::uploads::enqueue(mgr, ChunkUpload {
        key,
        slot,
        meta,
        node_base,
        nodes,
        macro_words,
        ropes,
        colinfo_words,
        completes_residency: true,
    });

    true
}

pub fn unload_chunk(mgr: &mut ChunkManager, center: ChunkKey, key: ChunkKey) {
    let Some(state) = mgr.build.chunks.remove(&key) else {
        return;
    };

    match state {
        ChunkState::Resident(res) => {
            mgr.arena.free(res.node_base, res.node_count);
            let dead = res.slot as usize;

            let last_res = mgr.slots.resident_slots.saturating_sub(1);
            debug_assert!(dead < mgr.slots.resident_slots, "resident slot out of prefix");

            if dead != last_res {
                swap_slots(mgr, dead, last_res);
                enqueue_slot_rewrite(mgr, dead);
            }

            mgr.slots.resident_slots = last_res;

            let remove_idx = last_res;
            let last_slot = mgr.slots.slot_to_key.len().saturating_sub(1);

            if remove_idx != last_slot {
                swap_slots(mgr, remove_idx, last_slot);
                enqueue_slot_rewrite(mgr, remove_idx);
            }

            mgr.slots.slot_to_key.pop();
            mgr.slots.chunk_meta.pop();
            mgr.slots.slot_macro.pop();
            mgr.slots.slot_colinfo.pop();

            mgr.grid.grid_dirty = true;
        }

        ChunkState::Uploading(up) => {
            mgr.arena.free(up.node_base, up.node_count);
            let dead = up.slot as usize;

            debug_assert!(
                dead >= mgr.slots.resident_slots,
                "uploading slot inside resident prefix"
            );

            let last_slot = mgr.slots.slot_to_key.len().saturating_sub(1);
            if dead != last_slot {
                swap_slots(mgr, dead, last_slot);
                enqueue_slot_rewrite(mgr, dead);
            }

            mgr.slots.slot_to_key.pop();
            mgr.slots.chunk_meta.pop();
            mgr.slots.slot_macro.pop();
            mgr.slots.slot_colinfo.pop();
        }

        ChunkState::Queued | ChunkState::Building => {
            cancel_token(mgr, key).store(true, Ordering::Relaxed);
            mgr.build.queued_set.remove(&key);
            mgr.grid.grid_dirty = true;
        }
    }

    mgr.build.cancels.remove(&key);

    // If we removed something that was outside keep anyway, fine.
    // If you want to be aggressive about cleaning cache too:
    // mgr.cache.remove(&key);
    let _ = center; // keep signature aligned with callers; can remove if unused.
}

fn enqueue_slot_rewrite(mgr: &mut ChunkManager, slot: usize) {
    let key = mgr.slots.slot_to_key[slot];
    let slot_u32 = slot as u32;

    super::uploads::enqueue(mgr, ChunkUpload {
        key,
        slot: slot_u32,
        meta: mgr.slots.chunk_meta[slot],
        node_base: 0,
        nodes: Arc::<[NodeGpu]>::from(Vec::<NodeGpu>::new()),
        macro_words: mgr.slots.slot_macro[slot].clone(),
        ropes: Arc::<[NodeRopesGpu]>::from(Vec::<NodeRopesGpu>::new()),
        colinfo_words: mgr.slots.slot_colinfo[slot].clone(),
        completes_residency: false,
    });
}

fn swap_slots(mgr: &mut ChunkManager, a: usize, b: usize) {
    if a == b {
        return;
    }

    let ka = mgr.slots.slot_to_key[a];
    let kb = mgr.slots.slot_to_key[b];

    mgr.slots.slot_to_key.swap(a, b);
    mgr.slots.chunk_meta.swap(a, b);
    mgr.slots.slot_macro.swap(a, b);
    mgr.slots.slot_colinfo.swap(a, b);

    mgr.slots.chunk_meta[a].macro_base = (a as u32) * MACRO_WORDS_PER_CHUNK;
    mgr.slots.chunk_meta[a].colinfo_base = (a as u32) * COLINFO_WORDS_PER_CHUNK;
    mgr.slots.chunk_meta[b].macro_base = (b as u32) * MACRO_WORDS_PER_CHUNK;
    mgr.slots.chunk_meta[b].colinfo_base = (b as u32) * COLINFO_WORDS_PER_CHUNK;

    if let Some(st) = mgr.build.chunks.get_mut(&ka) {
        match st {
            ChunkState::Resident(r) => r.slot = b as u32,
            ChunkState::Uploading(u) => u.slot = b as u32,
            _ => {}
        }
    }
    if let Some(st) = mgr.build.chunks.get_mut(&kb) {
        match st {
            ChunkState::Resident(r) => r.slot = a as u32,
            ChunkState::Uploading(u) => u.slot = a as u32,
            _ => {}
        }
    }
}

fn evict_one_farthest(mgr: &mut ChunkManager, center: ChunkKey, protect: ChunkKey) -> bool {
    if mgr.slots.slot_to_key.is_empty() {
        return false;
    }

    // Pass 1: evict farthest chunk that is OUTSIDE active xz.
    let mut best_outside: Option<(f32, ChunkKey)> = None;

    for &k in &mgr.slots.slot_to_key {
        if k == protect {
            continue;
        }
        if in_priority_box(mgr, center, k) {
            continue;
        }
        if keep::in_active_xz(center, k) {
            continue;
        }

        let dx = (k.x - center.x) as f32;
        let dz = (k.z - center.z) as f32;
        let dy = (k.y - center.y) as f32;
        let d = dx * dx + dz * dz + 4.0 * dy * dy;

        if best_outside.map_or(true, |(bd, _)| d > bd) {
            best_outside = Some((d, k));
        }
    }

    if let Some((_, k)) = best_outside {
        unload_chunk(mgr, center, k);
        return true;
    }

    // Pass 2: if everything is inside ACTIVE, evict farthest overall.
    let mut best_any: Option<(f32, ChunkKey)> = None;

    for &k in &mgr.slots.slot_to_key {
        if k == protect {
            continue;
        }
        if in_priority_box(mgr, center, k) {
            continue;
        }

        let dx = (k.x - center.x) as f32;
        let dz = (k.z - center.z) as f32;
        let dy = (k.y - center.y) as f32;
        let d = dx * dx + dz * dz + 4.0 * dy * dy;

        if best_any.map_or(true, |(bd, _)| d > bd) {
            best_any = Some((d, k));
        }
    }

    if let Some((_, k)) = best_any {
        unload_chunk(mgr, center, k);
        return true;
    }

    false
}

// src/streaming/manager/stats.rs
// ------------------------------
use crate::streaming::types::*;
use super::ChunkManager;

pub fn stats(mgr: &ChunkManager) -> Option<StreamStats> {
    let mut s = StreamStats::default();

    if let Some(c) = mgr.build.last_center {
        s.center = (c.x, c.y, c.z);
    }

    s.resident_slots = mgr.slots.resident_slots as u32;
    s.total_slots    = mgr.slots.slot_to_key.len() as u32;
    s.chunks_map     = mgr.build.chunks.len() as u32;

    for st in mgr.build.chunks.values() {
        match st {
            ChunkState::Queued        => s.st_queued += 1,
            ChunkState::Building      => s.st_building += 1,
            ChunkState::Uploading(_)  => s.st_uploading += 1,
            ChunkState::Resident(_)   => s.st_resident += 1,
        }
    }

    s.in_flight = mgr.build.in_flight as u32;
    s.done_backlog = mgr.build.rx_done.len() as u32;

    s.up_rewrite = mgr.uploads.uploads_rewrite.len() as u32;
    s.up_active  = mgr.uploads.uploads_active.len() as u32;
    s.up_other   = mgr.uploads.uploads_other.len() as u32;

    let (cb, ce, cl) = mgr.cache.stats();
    s.cache_bytes   = cb as u64;
    s.cache_entries = ce as u32;
    s.cache_lru     = cl as u32;

    s.build_queue_len = mgr.build.build_queue.len() as u32;
    s.queued_set_len  = mgr.build.queued_set.len() as u32;
    s.cancels_len     = mgr.build.cancels.len() as u32;

    let mut orphan = 0u32;
    for (k, st) in mgr.build.chunks.iter() {
        if matches!(st, ChunkState::Queued) && !mgr.build.queued_set.contains(k) {
            orphan += 1;
        }
    }
    s.orphan_queued = orphan;

    Some(s)
}

// src/streaming/manager/uploads.rs
// --------------------------------
use std::{collections::VecDeque, mem::size_of, sync::Arc};
use crate::{config, render::gpu_types::{ChunkMetaGpu, NodeGpu, NodeRopesGpu}};
use crate::streaming::types::*;
use super::{ChunkManager};
use super::keep;

#[inline]
fn upload_bytes(u: &ChunkUpload) -> usize {
    let mut b = size_of::<ChunkMetaGpu>();
    b += u.nodes.len() * size_of::<NodeGpu>();
    b += u.macro_words.len() * size_of::<u32>();
    b += u.ropes.len() * size_of::<NodeRopesGpu>();
    b += u.colinfo_words.len() * size_of::<u32>();
    b
}

#[inline(always)]
pub fn upload_dist_score(k: ChunkKey, c: ChunkKey) -> f32 {
    let dx = (k.x - c.x) as f32;
    let dz = (k.z - c.z) as f32;
    let dy = (k.y - c.y) as f32;
    dx.abs() + dz.abs() + 2.0 * dy.abs()
}

pub fn enqueue(mgr: &mut ChunkManager, u: ChunkUpload) {
    if !u.completes_residency {
        mgr.uploads.uploads_rewrite.push_front(u);
        return;
    }

    let Some(center) = mgr.build.last_center else {
        mgr.uploads.uploads_other.push_back(u);
        return;
    };

    if keep::in_active_xz(center, u.key) {
        insert_sorted_by_center(&mut mgr.uploads.uploads_active, u, center);
    } else {
        mgr.uploads.uploads_other.push_back(u);
    }
}

pub fn rebucket_for_center(mgr: &mut ChunkManager, center: ChunkKey) {
    let mut new_active = VecDeque::with_capacity(mgr.uploads.uploads_active.len());
    let mut new_other  = VecDeque::with_capacity(mgr.uploads.uploads_other.len());

    let ar = config::ACTIVE_RADIUS.max(PRIORITY_RADIUS);

    let is_active = |k: ChunkKey| {
        let dx = (k.x - center.x).abs();
        let dz = (k.z - center.z).abs();
        dx <= ar && dz <= ar
    };

    for u in mgr.uploads.uploads_active.drain(..) {
        if is_active(u.key) { new_active.push_back(u); }
        else { new_other.push_back(u); }
    }

    for u in mgr.uploads.uploads_other.drain(..) {
        if is_active(u.key) { new_active.push_back(u); }
        else { new_other.push_back(u); }
    }

    mgr.uploads.uploads_active = new_active;
    mgr.uploads.uploads_other  = new_other;
}

#[inline]
fn insert_sorted_by_center(q: &mut VecDeque<ChunkUpload>, u: ChunkUpload, center: ChunkKey) {
    let us = upload_dist_score(u.key, center);
    let pos = q.iter()
        .position(|e| upload_dist_score(e.key, center) > us)
        .unwrap_or(q.len());
    q.insert(pos, u);
}

#[inline]
fn uploads_len_total(mgr: &ChunkManager) -> usize {
    mgr.uploads.uploads_rewrite.len() + mgr.uploads.uploads_active.len() + mgr.uploads.uploads_other.len()
}

pub fn take_all(mgr: &mut ChunkManager) -> Vec<ChunkUpload> {
    let mut out = Vec::new();
    out.extend(mgr.uploads.uploads_rewrite.drain(..));
    out.extend(mgr.uploads.uploads_active.drain(..));
    out.extend(mgr.uploads.uploads_other.drain(..));
    out
}

pub fn take_budgeted(mgr: &mut ChunkManager) -> Vec<ChunkUpload> {
    let backlog = uploads_len_total(mgr);

    let max_uploads = (MAX_UPLOADS_PER_FRAME + backlog / 4).clamp(MAX_UPLOADS_PER_FRAME, 32);
    let max_bytes   = (MAX_UPLOAD_BYTES_PER_FRAME + backlog * (256 << 10)).clamp(MAX_UPLOAD_BYTES_PER_FRAME, 16 << 20);

    let mut out = Vec::new();
    let mut bytes = 0usize;

    let mut rewrites_taken = 0usize;
    let rewrite_cap = 4;

    let mut pop_next = |mgr: &mut ChunkManager| -> Option<(u8, ChunkUpload)> {
        if rewrites_taken < rewrite_cap {
            if let Some(u) = mgr.uploads.uploads_rewrite.pop_front() {
                rewrites_taken += 1;
                return Some((0, u));
            }
        }
        if let Some(u) = mgr.uploads.uploads_active.pop_front() { return Some((1, u)); }
        if let Some(u) = mgr.uploads.uploads_other.pop_front()  { return Some((2, u)); }
        None
    };

    let mut push_front_same = |mgr: &mut ChunkManager, which: u8, u: ChunkUpload| {
        match which {
            0 => mgr.uploads.uploads_rewrite.push_front(u),
            1 => mgr.uploads.uploads_active.push_front(u),
            _ => mgr.uploads.uploads_other.push_front(u),
        }
    };

    while let Some((which, mut u)) = pop_next(mgr) {
        if out.len() >= max_uploads {
            push_front_same(mgr, which, u);
            break;
        }

        // priority gate (same as before)
        if u.completes_residency {
            if let Some(center) = mgr.build.last_center {
                if !super::slots::priority_box_ready(mgr, center) && !super::slots::in_priority_box(mgr, center, u.key) {
                    push_front_same(mgr, which, u);
                    break;
                }
            }
        }

        // validate slot + update bases
        let slot = match mgr.build.chunks.get(&u.key) {
            Some(ChunkState::Resident(r)) => r.slot,
            Some(ChunkState::Uploading(up)) => up.slot,
            _ => continue,
        };

        u.slot = slot;
        u.meta.macro_base = slot * MACRO_WORDS_PER_CHUNK;
        u.meta.colinfo_base = slot * COLINFO_WORDS_PER_CHUNK;

        let ub = upload_bytes(&u);

        if bytes + ub > max_bytes && !out.is_empty() {
            push_front_same(mgr, which, u);
            break;
        }

        bytes += ub;
        out.push(u);
    }

    out
}

// src/streaming/mod.rs
// --------------------
pub mod types;
pub mod node_arena;
pub mod priority;
pub mod workers;

pub mod cache;
pub mod manager;

pub use manager::ChunkManager;
pub use types::{StreamStats, ChunkUpload};
pub use node_arena::NodeArena;

// src/streaming/node_arena.rs
// ---------------------------
// src/streaming/node_arena.rs
//
// Very simple free-list arena for node ranges (in units of NodeGpu elements).
// Improvements:
// - free() now fully coalesces adjacent ranges (fixes long-run fragmentation).
// - alloc() uses best-fit (smallest range that fits) to reduce fragmentation further.

#[derive(Clone, Copy, Debug)]
struct Range {
    start: u32,
    len: u32,
}

pub struct NodeArena {
    free: Vec<Range>, // kept sorted by start
}

impl NodeArena {
    pub fn new(capacity: u32) -> Self {
        Self {
            free: vec![Range {
                start: 0,
                len: capacity,
            }],
        }
    }

    /// Allocate a contiguous range of `len` elements.
    /// Returns the start index in the arena, or None if no free range fits.
    pub fn alloc(&mut self, len: u32) -> Option<u32> {
        if len == 0 {
            return Some(0);
        }

        // Best-fit: choose the smallest free range that still fits.
        let mut best_i: Option<usize> = None;
        let mut best_len: u32 = u32::MAX;

        for (i, r) in self.free.iter().enumerate() {
            if r.len >= len && r.len < best_len {
                best_len = r.len;
                best_i = Some(i);
                if r.len == len {
                    break; // perfect fit
                }
            }
        }

        let i = best_i?;
        let r = self.free[i];
        let start = r.start;

        if r.len == len {
            self.free.remove(i);
        } else {
            self.free[i] = Range {
                start: r.start + len,
                len: r.len - len,
            };
        }

        Some(start)
    }

    /// Free a previously allocated range.
    pub fn free(&mut self, start: u32, len: u32) {
        if len == 0 {
            return;
        }

        // Insert sorted by start (log n search).
        let idx = self
            .free
            .binary_search_by_key(&start, |r| r.start)
            .unwrap_or_else(|i| i);

        self.free.insert(idx, Range { start, len });

        // Fully coalesce with neighbors (both directions).
        self.coalesce_at(idx);
    }

    fn coalesce_at(&mut self, mut i: usize) {
        // Merge backward as long as possible.
        while i > 0 {
            let a = self.free[i - 1];
            let b = self.free[i];
            if a.start + a.len == b.start {
                self.free[i - 1] = Range {
                    start: a.start,
                    len: a.len + b.len,
                };
                self.free.remove(i);
                i -= 1;
            } else {
                break;
            }
        }

        // Merge forward as long as possible.
        while i + 1 < self.free.len() {
            let a = self.free[i];
            let b = self.free[i + 1];
            if a.start + a.len == b.start {
                self.free[i] = Range {
                    start: a.start,
                    len: a.len + b.len,
                };
                self.free.remove(i + 1);
            } else {
                break;
            }
        }
    }
}

// src/streaming/priority.rs
// -------------------------
// src/streaming/priority.rs
use std::collections::VecDeque;
use glam::{Vec2, Vec3};

use super::types::ChunkKey;

pub fn sort_queue_near_first(queue: &mut VecDeque<ChunkKey>, center: ChunkKey, cam_fwd: Vec3) {
    let mut v: Vec<ChunkKey> = queue.drain(..).collect();

    let mut f = Vec2::new(cam_fwd.x, cam_fwd.z);
    if f.length_squared() > 1e-6 {
        f = f.normalize();
    } else {
        f = Vec2::ZERO;
    }

    v.sort_by(|a, b| {
        let sa = chunk_priority_score(*a, center, f);
        let sb = chunk_priority_score(*b, center, f);
        sa.partial_cmp(&sb).unwrap_or(std::cmp::Ordering::Equal)
    });

    queue.extend(v);
}

fn chunk_priority_score(k: ChunkKey, c: ChunkKey, fwd_xz: Vec2) -> f32 {
    let dx = (k.x - c.x) as f32;
    let dz = (k.z - c.z) as f32;
    let dy = (k.y - c.y) as f32;

    let base = dx.abs() + dz.abs() + 2.0 * dy.abs();
    let dir = dx * fwd_xz.x + dz * fwd_xz.y;

    let front_bonus = 0.75;
    let behind_penalty = 0.25;

    let bias = if dir >= 0.0 {
        -front_bonus * dir
    } else {
        -behind_penalty * dir
    };

    base + bias
}

// src/streaming/types.rs
// ----------------------
// src/streaming/types.rs
use std::sync::{Arc, atomic::AtomicBool};

use crate::render::gpu_types::{ChunkMetaGpu, NodeGpu, NodeRopesGpu};

pub const INVALID_U32: u32 = 0xFFFF_FFFF;

// Vertical band dy in [-1..=2]
pub const GRID_Y_MIN_DY: i32 = -2;
pub const GRID_Y_COUNT: u32 = 5;

pub const EVICT_ATTEMPTS: usize = 8;

// 8^3 bits = 512 bits = 16 u32
pub const MACRO_WORDS_PER_CHUNK: u32 = 16;
pub const MACRO_WORDS_PER_CHUNK_USIZE: usize = 16;

// 64x64 columns, packed 2x u16 per u32 => 2048 u32 per chunk
pub const COLINFO_WORDS_PER_CHUNK: u32 = 2048;
pub const COLINFO_WORDS_PER_CHUNK_USIZE: usize = 2048;

pub const MAX_UPLOADS_PER_FRAME: usize = 24;            // start 6–12
pub const MAX_UPLOAD_BYTES_PER_FRAME: usize = 16 << 20; // start 2–8 MB

pub const PRIORITY_RADIUS: i32 = 2; // => 5x5 in XZ, and with GRID_Y_* => 5 in Y


#[derive(Clone, Copy, Hash, PartialEq, Eq, Debug)]
pub struct ChunkKey {
    pub x: i32,
    pub y: i32,
    pub z: i32,
}

pub enum ChunkState {
    Queued,
    Building,
    Uploading(Uploading),
    Resident(Resident),
}

#[derive(Clone, Debug)]
pub struct Uploading {
    pub slot: u32,
    pub node_base: u32,
    pub node_count: u32,
    pub uploaded: bool,
}

#[derive(Clone, Debug)]
pub struct Resident {
    pub slot: u32,
    pub node_base: u32,
    pub node_count: u32,
}

#[derive(Clone, Debug)]
pub struct BuildJob {
    pub key: ChunkKey,
    pub cancel: Arc<AtomicBool>,
}

pub struct BuildDone {
    pub key: ChunkKey,
    pub cancel: Arc<AtomicBool>,
    pub canceled: bool,
    pub nodes: Vec<NodeGpu>,
    pub macro_words: Vec<u32>,
    pub ropes: Vec<NodeRopesGpu>,
    pub colinfo_words: Vec<u32>,
}

pub struct ChunkUpload {
    pub key: ChunkKey,
    pub slot: u32,
    pub meta: ChunkMetaGpu,

    pub node_base: u32,
    pub nodes: Arc<[NodeGpu]>,

    pub macro_words: Arc<[u32]>,

    pub ropes: Arc<[NodeRopesGpu]>,

    pub colinfo_words: Arc<[u32]>,

    pub completes_residency: bool,
}

#[inline(always)]
pub fn y_band_min() -> i32 {
    GRID_Y_MIN_DY
}

#[inline(always)]
pub fn y_band_max() -> i32 {
    GRID_Y_MIN_DY + GRID_Y_COUNT as i32 - 1
}

#[derive(Clone, Copy, Debug, Default)]
pub struct StreamStats {
    pub center: (i32, i32, i32),

    pub resident_slots: u32,
    pub total_slots: u32,
    pub chunks_map: u32,

    pub st_queued: u32,
    pub st_building: u32,
    pub st_uploading: u32,
    pub st_resident: u32,

    pub in_flight: u32,
    pub done_backlog: u32,

    pub up_rewrite: u32,
    pub up_active: u32,
    pub up_other: u32,

    pub cache_bytes: u64,
    pub cache_entries: u32,
    pub cache_lru: u32,

    // NEW
    pub build_queue_len: u32,
    pub queued_set_len: u32,
    pub cancels_len: u32,

    pub orphan_queued: u32,
}

// src/streaming/workers.rs
// ------------------------
// src/streaming/workers.rs
use std::sync::{Arc, atomic::Ordering};

use crossbeam_channel::{Receiver, Sender};

use crate::{
    config,
    render::gpu_types::{NodeGpu, NodeRopesGpu},
    svo::{build_chunk_svo_sparse_cancelable_with_scratch, BuildScratch},
    world::WorldGen,
};

use super::types::{BuildDone, BuildJob};

pub fn spawn_workers(gen: Arc<WorldGen>, rx_job: Receiver<BuildJob>, tx_done: Sender<BuildDone>) {
    for _ in 0..config::WORKER_THREADS {
        let gen = gen.clone();
        let rx_job = rx_job.clone();
        let tx_done = tx_done.clone();

        std::thread::spawn(move || {
            let mut scratch = BuildScratch::new();

            while let Ok(job) = rx_job.recv() {
                let k = job.key;

                if job.cancel.load(Ordering::Relaxed) {
                    let _ = tx_done.send(BuildDone {
                        key: k,
                        cancel: job.cancel,
                        canceled: true,
                        nodes: Vec::new(),
                        macro_words: Vec::new(),
                        ropes: Vec::new(),
                        colinfo_words: Vec::new(),
                    });
                    continue;
                }

                let origin = [
                    k.x * config::CHUNK_SIZE as i32,
                    k.y * config::CHUNK_SIZE as i32,
                    k.z * config::CHUNK_SIZE as i32,
                    0,
                ];

                let (nodes, macro_words, ropes, colinfo_words): (Vec<NodeGpu>, Vec<u32>, Vec<NodeRopesGpu>, Vec<u32>) =
                    build_chunk_svo_sparse_cancelable_with_scratch(
                        &gen,
                        [origin[0], origin[1], origin[2]],
                        config::CHUNK_SIZE,
                        job.cancel.as_ref(),
                        &mut scratch,
                    );

                let canceled = job.cancel.load(Ordering::Relaxed);
                let (nodes, macro_words, ropes) = if canceled {
                    (Vec::new(), Vec::new(), Vec::new())
                } else {
                    (nodes, macro_words, ropes)
                };

                let _ = tx_done.send(BuildDone {
                    key: k,
                    cancel: job.cancel,
                    canceled,
                    nodes,
                    macro_words,
                    ropes,
                    colinfo_words,
                });
            }
        });
    }
}

